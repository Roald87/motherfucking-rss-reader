<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/"><channel><title>IEEE Spectrum</title><link>https://spectrum.ieee.org/</link><description>IEEE Spectrum</description><atom:link href="https://spectrum.ieee.org/feeds/feed.rss" rel="self"></atom:link><language>en-us</language><lastBuildDate>Thu, 08 Aug 2024 14:00:02 -0000</lastBuildDate><image><url>https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy8yNjg4NDUyMC9vcmlnaW4ucG5nIiwiZXhwaXJlc19hdCI6MTc2MzA3MTQzOX0.SxRBIud_XE2YWQFaIJD9BPB1w-3JsFhiRkJIIe9Yq-g/image.png?width=210</url><link>https://spectrum.ieee.org/</link><title>IEEE Spectrum</title></image><item><title>Quantum Cryptography Has Everyone Scrambling</title><link>https://spectrum.ieee.org/quantum-key-distribution</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/conceptual-illustration-of-a-key-and-quantum-imagery-with-tones-of-orange-green-and-yellow.jpg?id=52968828&width=1200&height=800&coordinates=62%2C0%2C63%2C0"/><br/><br/><p>While the technology world <a href="https://www.iotworldtoday.com/quantum/businesses-brace-for-nist-post-quantum-cryptography-algorithms" target="_blank">awaits</a> NIST’s latest <a href="https://spectrum.ieee.org/post-quantum-cryptography-nist" target="_blank">“post-quantum” cryptography standards</a> this summer, a parallel effort is underway to also develop <a href="https://www.merriam-webster.com/dictionary/cryptosystem" target="_blank">cryptosystems</a> that are grounded in quantum technology—what are called <a href="https://spectrum.ieee.org/tag/qkd" target="_blank">quantum-key distribution</a> or QKD systems. </p><p>As a result, India, China, and a range of technology organizations in the European Union and United States are researching and developing QKD and weighing standards for the nascent cryptography alternative. And the biggest question of all is how or if QKD fits into a robust, reliable, and fully future-proof cryptography system that will ultimately become the global standard for secure digital communications into the 2030s. As in any emerging technology standard, different players are staking claims on different technologies and implementations of those technologies. And many of the big players are pursuing such divergent options because no technology is a clear winner at the moment. <strong></strong></p><p>According to <a href="https://rhg.com/team/ciel-qi/" rel="noopener noreferrer" target="_blank">Ciel Qi</a>, a research analyst at the New York-based <a href="https://rhg.com/" rel="noopener noreferrer" target="_blank">Rhodium Group</a>, there’s one clear leader in QKD research and development—at least for now. “While China likely holds an advantage in QKD-based cryptography due to its early investment and development, others are catching up,” says Qi.</p><h3>Two different kinds of “quantum secure” tech</h3><p>At the center of these varied cryptography efforts is the distinction between QKD and post-quantum cryptography (PQC) systems. QKD is based on quantum physics, which holds that <a href="https://spectrum.ieee.org/what-is-quantum-entanglement" target="_blank">entangled qubits</a> can store their shared information so securely that any effort to uncover it is unavoidably detectable. Sending pairs of entangled-photon qubits to both ends of a network provides the basis for physically secure cryptographic keys that can lock down data packets sent across that network. </p><p>Typically, quantum cryptography systems are built around photon sources that chirp out <a data-linked-post="2658354274" href="https://spectrum.ieee.org/metasurface-entangled-photons" target="_blank">entangled photon pairs</a>—where photon A heading down one length of fiber has a polarization that’s perpendicular to the polarization of photon B heading in the other direction. The recipients of these two photons perform separate measurements that enable both recipients to know that they and only they have the shared information transmitted by these photon pairs. (Otherwise, if a third party had intervened and measured one or both photons first, the delicate photon states would have been irreparably altered before reaching the recipients.)</p><p class="pull-quote">“People can’t predict theoretically that these PQC algorithms won’t be broken one day.” <strong>—Doug Finke, Global Quantum Intelligence</strong></p><p>This shared bit the two people on opposite ends of the line have in common then becomes a 0 or 1 in a budding secret key that the two recipients build up by sharing more and more entangled photons. Build up enough shared secret 0s and 1s between sender and receiver, and that secret key can be used for a type of strong cryptography, called a <a href="https://en.wikipedia.org/wiki/One-time_pad" target="_blank">one-time pad</a>, that guarantees a message’s safe transmission and <a href="https://crypto.stackexchange.com/questions/48193/are-one-time-pads-crackable-in-theory" target="_blank">faithful receipt by only the intended recipient</a>.  <span></span></p><p>By contrast, post-quantum cryptography (PQC) is based not around quantum physics but pure math, in which next-generation cryptographic algorithms are designed to run on conventional computers. And it’s the algorithms’ vast complexity that makes PQC security systems practically uncrackable, even by a quantum computer. So NIST—the U.S. <a href="https://www.nist.gov/" target="_blank">National Institute of Standards and Technology</a>—is developing <a href="https://csrc.nist.gov/Projects/post-quantum-cryptography/news" target="_blank">gold-standard PQC systems</a> that will undergird tomorrow’s post-quantum networks and communications.  </p><p>The big problem with the latter approach, says Doug Finke, chief content officer of the New York-based <a href="https://www.global-qi.com/" target="_blank">Global Quantum Intelligence</a>, is PQC is only <em>believed </em>(on <a href="https://www.cisa.gov/quantum" target="_blank">very, very good but not infallible evidence</a>) to be uncrackable by a fully-grown quantum computer. PQC, in other words, cannot necessarily offer the ironclad “quantum security” that’s promised.</p><p>“People can’t predict theoretically that these PQC algorithms won’t be broken one day,” Finke says. “On the other hand, QKD—there are theoretical arguments based on quantum physics that you can’t break a QKD network.”</p><p>That said, real-world QKD implementations might still be <a href="https://www.technologyreview.com/2019/03/06/136765/theres-a-new-way-to-break-quantum-cryptography/" target="_blank">hackable</a> via <a data-linked-post="2650272128" href="https://spectrum.ieee.org/rooting-out-malware-with-a-sidechannel-chip-defense-system" target="_blank">side-channel</a>, device-based, and <a href="https://opg.optica.org/oe/fulltext.cfm?uri=oe-25-10-11124&id=363706" target="_blank">other clever attacks</a>. Plus, QKD also requires direct access to a quantum-grade fiber optics network and sensitive quantum communications tech, neither of which is exactly commonplace today. “For day-to-day stuff, for me to send my credit card information to Amazon on my cellphone,” Finke says, “I’m not going to use QKD.”</p><h3>China’s early QKD lead dwindling </h3><p>According to Qi, China may have originally picked QKD as a focal point of their quantum technology development in part because the U.S. was <em>not</em> directing its efforts that way. “[The] strategic focus on QKD may be driven by China’s desire to secure a unique technological advantage, particularly as the U.S. leads in PQC efforts globally,” she says.<br/> </p><p>In particular, she points to ramped up efforts to use satellite uplinks and downlinks as the basis for free-space Chinese <a href="https://spectrum.ieee.org/tag/quantum-cryptography" target="_self">QKD systems</a>. Citing as a source China’s “father of quantum,” <a href="https://en.wikipedia.org/wiki/Pan_Jianwei" target="_blank">Pan Jianwei</a>, Qi says, “To achieve global quantum network coverage, China is currently developing a medium-high orbit quantum satellite, which is expected to be launched around 2026.”</p><p>That said, the limiting factor in all QKD systems to date is their ultimate reliance on a single photon to represent each qubit. Not even the most exquisitely-refined lasers and fiber optic lines can’t escape the vulnerability of individual photons.<br/></p><p>QKD repeaters, which would blindly replicate a single photon’s quantum state but not leak any distinguishing information about the individual photons passing through—meaning the repeater would not be hackable by eavesdroppers—do not exist today. But, Finke says, such tech is achievable, though at least 5 to 10 years away. “It definitely is early days,” he says.</p><p class="pull-quote">“While China likely holds an advantage in QKD-based cryptography due to its early investment and development, others are catching up.” <strong>—Ciel Qi, Rhodium Group</strong></p><p><span></span>“In China they do have a 2,000-kilometer network,” Finke says. “But it uses this thing called trusted nodes. I think they have over 30 in the Beijing to Shanghai network. So maybe every 100 km, they have this unit which basically measures the signal... and then regenerates it. But the trusted node you have to locate on an army base or someplace like that. If someone breaks in there, they can hack into the communications.”</p><p>Meanwhile, India has been playing catch-up, according to <a href="https://ige.nmims.edu/satyam-priyadarshi.php" target="_blank">Satyam Priyadarshy</a>, a senior advisor to Global Quantum Intelligence. Priyadarshy says India’s <a href="https://dst.gov.in/national-quantum-mission-nqm" target="_blank">National Quantum Mission</a> includes plans for QKD communications research—aiming ultimately for QKD networks connecting cities over 2,000-km distances, as well as across similarly long-ranging satellite communications networks.</p><p>Priyadarshy points both to government QKD research efforts—including at the Indian Space Research Organization—and private enterprise-based R&D, including by the Bengaluru-based cybersecurity firm <a href="https://www.qnulabs.com/" target="_blank">QuNu Labs</a>. Priyadarshy says that QuNu, for example, has been working on a hub-and-spoke framework named ChaQra for QKD. (<em>Spectrum</em> also sent requests for comment to officials at India’s Department of Telecommunications, which were unanswered as of press time.)</p><p class="pull-quote">“A hybrid of QKD and PQC is the most likely solution for a quantum safe network.” <strong>—Satyam Priyadarshy, Global Quantum Intelligence</strong></p><p>In the U.S. and European Union, similar early-stage efforts are also afoot. Contacted by <em>IEEE Spectrum</em>, officials from the <a href="https://www.etsi.org/about" target="_blank">European Telecommunications Standards Institute</a> (ETSI); the <a href="https://www.iso.org/home.html">International Standards Organization</a> (ISO); the <a href="https://en.wikipedia.org/wiki/International_Electrotechnical_Commission" target="_blank">International Electrotechnical Commission</a> (IEC); and the <a href="https://www.comsoc.org/" target="_blank">IEEE Communications Society</a> confirmed initiatives and working groups that are now working to both promote QKD technologies and emergent standards now taking shape. </p><p>“While ETSI is fortunate to have experts in a broad range of relevant topics, there is a lot to do,” says <a href="https://iqtevent.com/speaker/martin-ward/" target="_blank">Martin Ward</a>, senior research scientist based at Toshiba’s <a href="https://www.toshiba.eu/pages/eu/Cambridge-Research-Laboratory/" target="_blank">Cambridge Research Laboratory</a> in England, and chair of <a href="https://www.etsi.org/committee/qkd#" target="_blank">a QKD industry standards group</a> at ETSI. </p><p>Multiple sources contacted for this article envisioned a probable future in which PQC will likely be the default standard for most secure communications in a world of pervasive quantum computing. Yet, PQC also cannot avoid its potential Achilles’ heel against increasingly powerful quantum algorithms and machines either. This is where, the sources suggest, QKD could offer the prospect of hybrid secure communications that PQC alone could never provide.<br/></p><p>“QKD provides [theoretical] information security, while PQC enables scalab[ility],” Priyadarshy says. “A hybrid of QKD and PQC is the most likely solution for a quantum safe network.” But he added that efforts at investigating hybrid QKD-PQC technologies and standards today are “very limited.”<br/></p><p>Then, says Finke, QKD could still have the final say, even in a world where PQC remains preeminent. Developing QKD technology just happens, he points out, to also provide the basis for a future quantum Internet. </p><p>“It’s very important to understand that QKD is actually just one use case for a full quantum network,” Finke says. <br/></p><p>“There’s a lot of applications, like distributed quantum computing and quantum data centers and quantum sensor networks,” Finke adds. “So even the research that people are doing now in QKD is still very, very helpful because a lot of that same technology can be leveraged for some of these other use cases.” </p>]]></description><pubDate>Thu, 08 Aug 2024 14:00:02 +0000</pubDate><guid>https://spectrum.ieee.org/quantum-key-distribution</guid><category>Quantum repeaters</category><category>Quantum cryptography</category><category>Cryptography</category><category>Post-quantum cryptography</category><category>Quantum key distribution</category><dc:creator>Margo Anderson</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/conceptual-illustration-of-a-key-and-quantum-imagery-with-tones-of-orange-green-and-yellow.jpg?id=52968828&amp;width=980"></media:content></item><item><title>A Non-Engineer’s Journey to IEEE Leadership</title><link>https://spectrum.ieee.org/non-engineers-journey-to-ieee</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/portrait-of-a-woman-smiling-in-a-blue-blazer-with-a-blue-lanyard-around-her-neck.jpg?id=53131314&width=1200&height=800&coordinates=0%2C0%2C0%2C106"/><br/><br/><p>
<a href="https://www.linkedin.com/in/sharlene-brown-53b43091/" rel="noopener noreferrer" target="_blank">Sharlene Brown</a> often accompanied her husband, IEEE Senior Member <a href="https://www.linkedin.com/in/damith-wickramanayake-078b126/" rel="noopener noreferrer" target="_blank">Damith Wickramanayake</a>, to organization meetings. He has held leadership positions in the <a href="https://site.ieee.org/jamaica/about-ieee/history-of-ieee-jamaican-chapter/" rel="noopener noreferrer" target="_blank">IEEE Jamaica Section</a>, in <a href="https://r3.ieee.org/" rel="noopener noreferrer" target="_blank">IEEE Region 3</a>, and on the <a href="https://mga.ieee.org/" rel="noopener noreferrer" target="_blank">IEEE Member and Geographic Activities</a> board. Both are from Jamaica.
</p><p>
	She either waited outside the conference room or helped with tasks such as serving refreshments. Even though her husband encouraged her to sit in on the meetings, she says, she felt uncomfortable doing so because she wasn’t an engineer. Brown is an accountant and human resources professional. Her husband is a computer science professor at the <a href="https://www.utech.edu.jm/" rel="noopener noreferrer" target="_blank">University of Technology, Jamaica</a>, in Kingston. He is currently Region 3’s education activities coordinator and a member of the section’s education and outreach committee for the <a href="https://www.ieee.org/education/eab.html" rel="noopener noreferrer" target="_blank">IEEE Educational Activities Board</a>.
</p><h3>Sharlene Brown</h3><br/><p>
<strong>Employer</strong>
</p><p>
	Maritime Authority of Jamaica, in Kingston
</p><p>
<strong>Title </strong>
</p><p>
<strong></strong>Assistant accountant
</p><p>
<strong>Member grade</strong>
</p><p>
	Senior member
</p><p>
<strong>Alma mater</strong></p><p><a href="https://www.utech.edu.jm/" rel="noopener noreferrer" target="_blank">University of Technology, Jamaica</a>, in Kingston; <a href="https://www.tsinghua.edu.cn/en/" rel="noopener noreferrer" target="_blank">Tsinghua University</a>, in Beijing
</p><p>
	After earning her master’s degree in public administration in 2017, Brown says, she felt she finally was qualified to join IEEE, so she applied. Membership is open to individuals who, by education or experience, are competent in different fields including management. She was approved the same year.
</p><p>
	“When I joined IEEE, I would spend long hours at night reading various operations manuals and policies because I wanted to know what I was getting into,” she says. “I was always learning. That’s how I got to know a lot of things about the organization.”
</p><p>
	Brown is now a senior member and an active IEEE volunteer. She founded the Jamaica Section’s <a href="https://wie.ieee.org/" rel="noopener noreferrer" target="_blank">Women in Engineering</a> group; established a student branch; sits on several high-level IEEE boards; and ran several successful recruitment campaigns to increase the number of senior members in Jamaica and throughout Region 3.
</p><p>
	Brown was also a member of the subcommittee of the global Women in Engineering committee; she served as membership coordinator and ran several successful senior member campaigns, elevating women on the committee and across IEEE.
</p><p>
	Brown also was integral in the promotion and follow-up activities for the <a href="https://spectrum.ieee.org/a-novel-ieee-workshop-showcases-jamaicas-engineering-community" target="_self">One IEEE</a> event held in January at the University of Technology, Jamaica. The first-of-its-kind workshop connected more than 200 participants to each other and to the organization by showcasing Jamaica’s active engineering community. The Jamaica Section has 135 IEEE members.
</p><h2>From factory worker to accountant</h2><p>
	Brown grew up in <a href="https://en.wikipedia.org/wiki/Bog_Walk" rel="noopener noreferrer" target="_blank">Bog Walk</a>, a rural town in the parish of St. Catherine. Because she had low grades in high school, the only job she was able to get after graduating was as a temporary factory worker at the nearby <a href="https://www.nestle-caribbean.com/" rel="noopener noreferrer" target="_blank">Nestlé</a> plant. She worked as many shifts as she could to help support her family.
</p><p>
	“I didn’t mind working,” she says, “because I was making my mark. Anything I do, I am going to be excellent at, whether it’s cleaning the floor or doing office work.” But she had bigger plans than being a factory worker, she says.
</p><p>
	A friend told her about a temporary job overseeing exams at the Jamaican Institute of Management, now part of the University of Technology. Brown worked both jobs for a time until the school hired her full time to do administrative work in its accounting department.
</p><p>
	One of the perks of working there was free tuition for employees, and Brown took full advantage. She studied information management and computer applications, Jamaican securities, fraud detection, forensic auditing, and supervisory management, earning an associate degree in business administration in 2007. The school hired her in 2002 as an accountant, and she worked there for five years.
</p><p>
	In 2007 she joined the <a href="https://opm.gov.jm/" rel="noopener noreferrer" target="_blank">Office of the Prime Minister</a>, in Kingston, initially as an officer handling payments to suppliers. Her hard work and positive attitude got her noticed by other managers, she says. After a month she was tapped by the budget department to become a commitment control officer, responsible for allocating and overseeing funding for four of the country’s ministries.
</p><p class="pull-quote">“What I realized through my volunteer work in IEEE is that you’re never alone. There is always somebody to guide you.”</p><p>
	As a young accountant, she didn’t have hands-on experience with budgeting, but she was a quick learner who produced quality work, she says. She learned the budgeting process by helping her colleagues when her work slowed down and during her lunch breaks.
</p><p>
	That knowledge gave her the skills she needed to land her current job as an assistant accountant with the budget and management accounts group in the <a href="https://maritimejamaica.com/" rel="noopener noreferrer" target="_blank">Maritime Authority of Jamaica</a> accounts department, a position she has held since 2013.
</p><p>
	While she was working for the Office of the Prime Minister, Brown continued to further her education. She took night courses at the University of Technology and, in 2012, earned a bachelor’s degree in business administration. She majored in accounting and minored in human resources management.
</p><p>
	She secured a full scholarship in 2016 from the Chinese government to study public administration in Beijing at <a href="https://www.tsinghua.edu.cn/en/" rel="noopener noreferrer" target="_blank">Tsinghua University</a>, earning a master’s degree with distinction in 2017.
</p><p>
	Brown says she is now ready to shift to a human resources career. Even though she has been supervising people for more than 17 years, though, she is having a hard time finding an HR position, she says.
</p><p>
	Still willing to take on challenges, she is increasing her experience by volunteering with an HR consulting firm in Jamaica. To get more formal training, she is currently working on an HR certification from the <a href="https://www.shrm.org/" rel="noopener noreferrer" target="_blank">Society for Human Resource Management</a>.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="class setting with children sitting at desks wearing masks and shields on their desks" class="rm-shortcode" data-rm-shortcode-id="7758f24fa4716b846006ec6d65dc12e6" data-rm-shortcode-name="rebelmouse-image" id="a22c9" loading="lazy" src="https://spectrum.ieee.org/media-library/class-setting-with-children-sitting-at-desks-wearing-masks-and-shields-on-their-desks.jpg?id=53131889&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Sharlene Brown arranged for the purchase of 350 desk shields for Jamaican schools during the COVID-19 pandemic.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Sharlene Brown</small></p><h2>Building a vibrant community </h2><p>
	After graduating from Tsinghua University, Brown began volunteering for the IEEE Jamaica Section and Region 3.
</p><p>
	In 2019 she founded the section’s IEEE Women in Engineering affinity group, which she chaired for three years. She advocated for more women in leadership roles and has run successful campaigns to increase the number of female senior members locally, regionally, and globally across IEEE. She herself was elevated to senior member in 2019.
</p><p>
	Brown also got the WIE group more involved in helping the community. One project she is particularly proud of is the purchase of 350 desk shields for Jamaican schools so students could more safely attend classes and examination sessions in person during the COVID-19 pandemic.
</p><p>
	Brown was inspired to undertake the project when a student explained on a local news program that his family couldn’t afford Internet for their home, so he was unable to attend classes remotely.
</p><p>
	“Every time I watched the video clip, I would cry,” she says. “This young man might be the next engineer, the country’s next minister, or the next professional.
</p><p>
	“I’m so happy we were able to get funding from Region 3 and a local organization to provide those shields.”
</p><p>
	She established an IEEE student branch at the <a href="https://cmu.edu.jm/" rel="noopener noreferrer" target="_blank">Caribbean Maritime University</a>, in Kingston. The branch had almost 40 students at the time of formation.
</p><p>
	Brown is working to form student branches at other Jamaican universities, and she is attempting to establish an <a href="https://ieee-pes.org/" rel="noopener noreferrer" target="_blank">IEEE Power & Energy Society</a> chapter in the section.
</p><p>
	She is a member of several IEEE committees including the <a href="https://www.ieee.org/content/dam/ieee-org/ieee/web/org/about/nominations/ieee-election-oversight-committee.pdf" rel="noopener noreferrer" target="_blank">Election Oversight</a><a href="https://www.ieee.org/content/dam/ieee-org/ieee/web/org/about/nominations/nac-pd-tellers.pdf" rel="noopener noreferrer" target="_blank"> and Tellers</a>. She serves as chair for the region’s Professional Activities Committee.
</p><p>
	“What I realized through my volunteer work in IEEE is that you’re never alone,” she says. “There is always somebody to help guide you. If they don’t know something, they will point you to the person who does.
</p><p>
	“Also, you’re allowed to make mistakes,” she says. “In some organizations, if you make a mistake, you might lose your job or have to pay for your error. But IEEE is your professional home, where you learn, grow, and make mistakes.”
</p><p>
	On some of the IEEE committees where she serves, she is the only woman of color, but she says she has not faced any discrimination—only respect.
</p><p>
	“I feel comfortable and appreciated by the people and the communities I work with,” she says. “That motivates me to continue to do well and to touch lives positively. That’s what makes me so active in serving in IEEE: You’re appreciated and rewarded for your hard work.”
</p>]]></description><pubDate>Wed, 07 Aug 2024 18:00:02 +0000</pubDate><guid>https://spectrum.ieee.org/non-engineers-journey-to-ieee</guid><category>Careers</category><category>Ieee jamaica section</category><category>Ieee member news</category><category>Ieee region 3</category><category>Ieee women in engineering</category><category>Type:ti</category><category>Volunteering</category><dc:creator>Kathy Pretz</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/portrait-of-a-woman-smiling-in-a-blue-blazer-with-a-blue-lanyard-around-her-neck.jpg?id=53131314&amp;width=980"></media:content></item><item><title>Engineering the First Fitbit: The Inside Story</title><link>https://spectrum.ieee.org/fitbit</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-closeup-of-a-hand-holding-a-black-fitbit-clipping-it-onto-a-pocket-the-word-step-is-visible-in-blue-text-on-the-device.png?id=53100495&width=1200&height=800&coordinates=0%2C750%2C0%2C750"/><br/><br/><p>
<strong>It was December 2006.</strong> Twenty-nine-year-old entrepreneur James Park had just purchased a Wii game system. It included the Wii Nunchuk, a US $29 handheld controller with motion sensors that let game players interact by moving their bodies—swinging at a baseball, say, or boxing with a virtual partner.
</p><p>
	Park became obsessed with his Wii.
</p><p>
	“I was a tech-gadget geek,” he says. “Anyone holding that nunchuk was fascinated by how it worked. It was the first time that I had seen a compelling consumer use for accelerometers.”
</p><p>
	After a while, though, Park spotted a flaw in the Wii: It got you moving, sure, but it trapped you in your living room. What if, he thought, you could take what was cool about the Wii and use it in a gadget that got you out of the house?
</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="A clear plastic package contains a first-generation black Fitbit. Text reads \u201cFitbit,\u201d \u201cWireless Personal Tracker\u201d, and \u201cTracks your fitness & sleep\u201d " class="rm-shortcode rm-resized-image" data-rm-shortcode-id="3b04720ae23f26c8b70f79a8e56ae656" data-rm-shortcode-name="rebelmouse-image" id="34efc" loading="lazy" src="https://spectrum.ieee.org/media-library/a-clear-plastic-package-contains-a-first-generation-black-fitbit-text-reads-u201cfitbit-u201d-u201cwireless-personal-tracker.png?id=53100561&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">The first generation of Fitbit trackers shipped in this package in 2009.  </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">NewDealDesign </small>
</p><p>
	“That,” says Park, “was the aha moment.” His idea became Fitbit, an activity tracker that has racked up sales of more than 136 million units since its first iteration hit the market in late 2009.
</p><p>
	But back to that “aha moment.” Park quickly called his friend and colleague Eric Friedman. In 2002, the two, both computer scientists by training, had started a photo-sharing company called HeyPix, which they sold to CNET in 2005. They were still working for CNET in 2006, but it wasn’t a bad time to think about doing something different.
</p><p>
	Friedman loved Park’s idea.
</p><p>
	“My mother was an active walker,” Friedman says. “She had a walking group and always had a pedometer with her. And my father worked with augmentative engineering [assistive technology] for the elderly and handicapped. We’d played with accelerometer tech before. So it immediately made sense. We just had to refine it.”
</p><p>
	The two left CNET, and in April 2007 they incorporated the startup with Park as CEO and Friedman as chief technology officer. Park and Friedman weren’t trying to build the first step counter—mechanical pedometers date back to the 1960s. They weren’t inventing the first smart activity tracker—
	<a href="https://bodymedia.com/" target="_blank">BodyMedia</a>, a medical device manufacturer, had in 1999 included accelerometers with other sensors in an armband designed to measure calories burned. And Park and Friedman didn’t get a smart consumer tracker to market first. In 2006, <a href="https://www.nike.com/" target="_blank">Nike</a> had worked with <a href="https://www.apple.com/" target="_blank">Apple</a> to launch the Nike+ for runners, a motion-tracking system that required a special shoe and a receiver that plugged into an iPod
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="Two people stand on a busy sidewalk, one wearing a dark sweater and jeans with arms crossed, the other in a brown checkered shirt and light-colored pants with hands on hips. " class="rm-shortcode" data-rm-shortcode-id="8b8b6c83327af6e83abcb2dc8526748c" data-rm-shortcode-name="rebelmouse-image" id="33cdc" loading="lazy" src="https://spectrum.ieee.org/media-library/two-people-stand-on-a-busy-sidewalk-one-wearing-a-dark-sweater-and-jeans-with-arms-crossed-the-other-in-a-brown-checkered-shir.png?id=53100707&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Fitbit’s founders James Park [left] and Eric Friedman released their first product in 2009, when this photo was taken. </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Peter DaSilva/The New York Times/Redux </small>
</p><p>
	Park wasn’t aware of any of this when he thought about getting fitness out of the living room, but the two quickly did their research and figured out what they did and didn’t want to do.
</p><p>
	“We didn’t want to create something expensive, targeted at athletes,” he says. “Or something that was dumb and not connected to software. And we wanted something that could provide social connection, like photo sharing did.”
</p><p>
	That something had to be comfortable to wear all day, be easy to use, upload its data seamlessly so the data could be tracked and shared with friends, and rarely need charging. Not an easy combination of requirements.
</p><p>
	“It’s one of those things where the simpler you get, the harder it becomes to design something well,” Park says.
</p><h2>The first Fitbit was designed for women
</h2><p>
	The first design decision was the biggest one. Where on the body did they expect people to put this wearable? They weren’t going to ask people to buy special shoes, like the Nike+, or wear a thick band on their upper arms, like BodyMedia’s tracker.
</p><p>
	They hired
	<a href="https://www.newdealdesign.com/" target="_blank">NewDealDesign</a> to figure out some of these details.
</p><p>
	“In our first two weeks, after multiple discussions with Eric and James, we decided that the project was going to be geared to women,” says Gadi Amit, president and principal designer of NewDealDesign. “That decision was the driver of the form factor.”
</p><p>
	“We wanted to start with something familiar to people,” Park says, “and people tended to clip pedometers to their belts.” So a clip-on device made sense. But women generally don’t wear belts.
</p><p>
	To do what it needed to do, the clip-on gadget would have to contain a roughly 2.5-by-2.5-centimeter (1-by-1-inch) printed circuit board, Amit recalls. The big breakthrough came when the team decided to separate the electronics and the battery, which in most devices are stacked. “By doing that, and elongating it a bit, we found that women could put it anywhere,” Amit says. “Many would put it in their bras, so we targeted the design to fit a bra in the center front, purchasing dozens of bras for testing.”
</p><p>
	The decision to design for women also drove the overall look, to “subdue the user interface,” as Amit puts it. They hid a low-resolution monochrome OLED display behind a continuous plastic cover, with the display lighting up only when you asked it to. This choice helped give the device an impressive battery life.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A black rectangular object displaying a small blue flower and clipped onto light blue fabric " class="rm-shortcode" data-rm-shortcode-id="7d78cf08703d1f978a6ea9cb975302c6" data-rm-shortcode-name="rebelmouse-image" id="15926" loading="lazy" src="https://spectrum.ieee.org/media-library/a-black-rectangular-object-displaying-a-small-blue-flower-and-clipped-onto-light-blue-fabric.gif?id=53100713&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">The earliest Fitbit devices used an animated flower as a progress indicator. </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">NewDealDesign </small>
</p><p>
	They also came up with the idea of a flower as a progress indicator—inspired, Park says, by the
	<a href="https://www.bandai.com/brands/tamagotchi" target="_blank">Tamagotchi</a>, one of the biggest toy fads of the late 1990s. “So we had a little animated flower that would shrink or grow based on how active you were,” Park explains.
</p><p>
	And after much discussion over controls, the group gave the original Fitbit just one button.
</p><h2>Hiring an EE—from Dad—to design Fitbit’s circuitry
</h2><p>
	Park and Friedman knew enough about electronics to build a crude prototype, “stuffing electronics into a box made of cut-up balsam wood,” Park says. But they also knew that they needed to bring in a real electrical engineer to develop the hardware.
</p><p>
	Fortunately, they knew just whom to call. Friedman’s father, Mark, had for years been working to develop a device for use in nursing homes, to remotely monitor the position of bed-bound patients. Mark’s partner in this effort was Randy Casciola, an electronics engineer and currently president of Morewood Design Labs.
</p><p>
	Eric called his dad, told him about the gadget he and Park envisioned, and asked if he and Casciola could build a prototype.
</p><p>
	“Mark and I thought we’d build a quick-and-dirty prototype, something they could get sensor data from and use for developing software. And then they’d go off to Asia and get it miniaturized there,” Casciola recalls. “But one revision led to another.” Casciola ended up working on circuit designs for Fitbits virtually full time until the sale of the company to Google, announced in 2019 and completed in early 2021.
</p><p class="pull-quote">
	“We saw some pretty scary manufacturers. Dirty facilities, flash marks on their injection-molded plastics, very low precision.”  <br/>
<strong>—</strong><strong>James Park</strong>
</p><p>
	“We were just two little guys in a little office in Pittsburgh,” Casciola says. “Before Fitbit came along, we had realized that our nursing-home thing wasn’t likely to ever be a product and had started taking on some consulting work. I had no idea Fitbit would become a household name. I just like working on anything, whether I think it’s a good idea or not, or even whether someone is paying me or not.”
</p><p>
	The earliest prototypes were pretty large, about 10 by 15 cm, Casciola says. They were big enough to easily hook up to test equipment, yet small enough to strap on to a willing test subject.
</p><p>
	After that, Park and Eric Friedman—along with Casciola, two contracted software engineers, and a mechanical design firm—struggled with turning the bulky prototype into a small and sleek device that counted steps, stored data until it could be uploaded and then transmitted it seamlessly, had a simple user interface, and didn’t need daily charging.
</p><p>
	“Figuring out the right balance of battery life, size, and capability kept us occupied for about a year,” Park says.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A black Fitbit sits vertically in a square stand with a wire coming out. The screen on the device reads \u201cBATT 6%\u201d " class="rm-shortcode" data-rm-shortcode-id="75f42ce29c77a6e358d72e078e1dcfbb" data-rm-shortcode-name="rebelmouse-image" id="64d75" loading="lazy" src="https://spectrum.ieee.org/media-library/a-black-fitbit-sits-vertically-in-a-square-stand-with-a-wire-coming-out-the-screen-on-the-device-reads-u201cbatt-6-u201d.png?id=53100764&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">The Fitbit prototype, sitting on its charger, booted up for the first time in December 2008. </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">James Park </small>
</p><p>
	After deciding to include a radio transmitter, they made a big move: They turned away from the Bluetooth standard for wireless communications in favor of
	<a href="https://developer.garmin.com/ant-program/overview/" target="_blank">the ANT protocol</a>, a technology developed by Garmin that used far less power. That meant the Fitbit wouldn’t be able to upload to computers directly. Instead, the team designed their own base station, which could be left plugged into a computer and would grab data anytime the Fitbit wearer passed within range.
</p><p>
	Casciola didn’t have expertise in radio-frequency engineering, so he relied on the supplier of the ANT radio chips:
	<a href="https://www.nordicsemi.com/About-us" target="_blank">Nordic Semiconductor</a>, in Trondheim, Norway.
</p><p>
	“They would do a design review of the circuit board layout,” he explains. “Then we would send our hardware to Norway. They would do RF measurements on it and tell me how to tweak the values of the capacitors and conductors in the RF chain, and I would update the schematic. It’s half engineering and half black magic to get this RF stuff working.”
</p><p>
	Another standard they didn’t use was the ubiquitous USB charging connection.
</p><p>
	“We couldn’t use USB,” Park says. “It just took up too much volume. Somebody actually said to us, ‘Whatever you do, don’t design a custom charging system because it’ll be a pain, it’ll be super expensive.’ But we went ahead and built one. And it was a pain and super expensive, but I think it added a level of magic. You just plopped your device on [the charger]. It looked beautiful, and it worked consistently.”
</p><p>
	Most of the electronics they used were off the shelf, including a 16-bit Texas Instruments MSP430 microprocessor, and 92 kilobytes of flash memory and 4 kb of RAM to hold the operating system, the rest of the code, all the graphics, and at least seven days’ worth of collected data.
</p><p>
	The Fitbit was designed to resist sweat, and they generally survived showers and quick dips, says Friedman. “But hot tubs were the bane of our existence. People clipped it to their swimsuits and forgot they had it on when they jumped into the hot tub.”
</p><h2>Fitbit’s demo or die moment
</h2><p>
	Up to this point, the company was surviving on $400,000 invested by Park, Friedman, and a few people who had backed their previous company. But more money would be needed to ramp up manufacturing. And so a critical next step would be a live public demo, which they scheduled for the TechCrunch conference in San Francisco in September 2008.
</p><p>
	Live demonstrations of new technologies are always risky, and this one walked right up to the edge of disaster. The plan was to ask an audience member to call out a number, and then Park, wearing the prototype in its balsa-wood box, would walk that number of steps. The count would sync wirelessly to a laptop projecting to a screen on stage. When Friedman hit refresh on the browser, the step count would appear on the screen. What could go wrong?
</p><p>
	A lot. Friedman explains: “You think counting steps is easy, but let’s say you do three steps. One, two, three. When you bring your feet together, is that a step or is that the end? It’s much easier to count 1,000 steps than it is to do 10 steps. If I walk 10 steps and am off by one, that’s a glaring error. With 1,000, that variance becomes noise.”
</p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="af05758379bd0f2f684a286f3af7f24c" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/-cjWarDhyTM?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
<small class="image-media media-caption" placeholder="Add Photo Caption...">The first semi-assembled Fitbit records its inaugural step count. </small>
<small class="image-media media-photo-credit" placeholder="Add Photo Credit...">James Park</small>
</p><p>
	After a lot of practice, the two thought they could pull it off. Then came the demo. “While I was walking, the laptop crashed,” Park says. “I wasn’t aware of that. I was just walking happily. Eric had to reboot everything while I was still walking. But the numbers showed up; I don’t think anyone except Eric realized what had happened.”
</p><p>
	That day, some 2,000 preorders poured in. And Fitbit closed a $2 million round of venture investment the next month.
</p><p>
	Though Park and Friedman had hoped to get Fitbits into users’ hands—or clipped onto their bras—by Christmas of 2008, they missed that deadline by a year.
</p><h2>The algorithms that determine Fitbit’s count
</h2><p>
	Part of Fitbit’s challenge of getting from prototype to shippable product was software development. They couldn’t expect users to walk as precisely as Park did for the demo. Instead, the device’s algorithms needed to determine what a step was and what was a different kind of motion—say, someone scratching their nose.
</p><p>
	“Data collection was difficult,” Park says. “Initially, it was a lot of us wearing prototype devices doing a variety of different activities. Our head of research, Shelten Yuen, would follow, videotaping so we could go back and count the exact number of steps taken. We would wear multiple devices simultaneously, to compare the data collects against each other.”
</p><p>
	Friedman remembers one such outing. “James was tethered to the computer, and he was pretending to walk his dog around the Haight [in San Francisco], narrating this little play that he’s putting on: ‘OK, I’m going to stop. The dog is going to pee on this tree. And now he’s going over there.’ The great thing about San Francisco is that nobody looks strangely at two guys tethered together walking around talking to themselves.”
</p><p class="pull-quote">
	“Older people tend to have an irregular cadence—to the device, older people look a lot like buses going over potholes.”<strong>–James Park</strong>
</p><p>
	“Pushing baby strollers was an issue,” because the wearer’s arms aren’t swinging, Park says. “So one of our guys put an ET doll in a baby stroller and walked all over the city with it.”
</p><p>
	Road noise was another big issue. “Yuen, who was working on the algorithms, was based in Cambridge, Mass.,” Park says. “They have more potholes than we do. When he took the bus, the bus would hit the potholes and [the device would] be bouncing along, registering steps.” They couldn’t just fix the issue by looking for a regular cadence to count steps, he adds, because not everyone has a regular cadence. “Older people tend to have an irregular cadence—to the device, older people look a lot like buses going over potholes.”
</p><h2>Fitbit’s founders enter the world of manufacturing
</h2><p>
	A consumer gadget means mass manufacturing, potentially in huge quantities. They talked to a lot of contract-manufacturing firms, Park recalls. They realized that as a startup with an unclear future market, they wouldn’t be of interest to the top tier of manufacturers. But they couldn’t go with the lowest-budget operations, because they needed a reasonable level of quality.
</p><p>
	“We saw some pretty scary manufacturers,” Park said. “Dirty facilities, flash marks on their injection-molded plastics [a sign of a bad seal or other errors], very low precision.” They eventually found a small manufacturer that was “pretty good but still hungry for business.” The manufacturer was headquartered in Singapore, while their surface-mount supplier, which put components directly onto printed circuit boards, was in Batam, Indonesia.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="Two rows of women wearing light blue shirts stand at long tables assembling devices. " class="rm-shortcode" data-rm-shortcode-id="2785ce787cabfb0e2889539875ae3ae8" data-rm-shortcode-name="rebelmouse-image" id="6df6d" loading="lazy" src="https://spectrum.ieee.org/media-library/two-rows-of-women-wearing-light-blue-shirts-stand-at-long-tables-assembling-devices.png?id=53100824&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Workers assemble Fitbits by hand in October of 2008.  </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">James Park </small>
</p><p>
	Working with that manufacturer, Park and Friedman made some tweaks in the design of the circuitry and the shape of the case. They struggled over how to keep water—and sweat—out of the device, settling on ultrasonic welding for the case and adding a spray-on coating for the circuitry after some devices were returned with corrosion on the electronics. That required tweaking the layout to make sure the coating would get between the chips. The coating on each circuit board had to be checked and touched up by hand. When they realized that the coating increased the height of the chips, they had to tweak the layout some more.
</p><p>
	In December 2009, just a week before the ship date, Fitbits began rolling off the production line.
</p><p>
	“I was in a hotel room in Singapore testing one of the first fully integrated devices,” Park says. “And it wasn’t syncing to my computer. Then I put the device right next to the base station, and it started to sync. Okay, that’s good, but what was the maximum distance it could sync? And that turned out to be literally just a few inches. In every other test we had done, it was fine. It could sync from 15 or 20 feet [5 or 6 meters] away.”
</p><p>
	The problem, Park eventually figured out, occurred when the two halves of the Fitbit case were ultrasonically welded together. In previous syncing tests, the cases had been left unsealed. The sealing process pushed the halves closer together, so that the cable for the display touched or nearly touched the antenna printed on the circuit board, which affected the radio signal. Park tried squeezing the halves together on an unsealed unit and reproduced the problem.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="Two photos. One photo shows 3 men working in a lab wearing cleanroom suits. One man is seated and handling electronic components, and the others stand observing. The other photo shows a row of six black rectangular devices with green circuit boards hanging out of them " class="rm-shortcode" data-rm-shortcode-id="1cec4719c1a8b043ccb380065b8ee453" data-rm-shortcode-name="rebelmouse-image" id="1f2c0" loading="lazy" src="https://spectrum.ieee.org/media-library/two-photos-one-photo-shows-3-men-working-in-a-lab-wearing-cleanroom-suits-one-man-is-seated-and-handling-electronic-components.png?id=53100989&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Getting the first generation of Fitbits into mass production required some last-minute troubleshooting. Fitbit cofounder James Park [top, standing in center] helps debug a device at the manufacturer shortly before the product’s 2009 launch. Early units from the production line are shown partially assembled [bottom]. </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">James Park </small>
</p><p>
	“I thought, if we could just push that cable away from the antenna, we’d be okay,” Park said. “The only thing I could find in my hotel room to do that was toilet paper. So I rolled up some toilet paper really tight and shoved it in between the cable and the antenna. That seemed to work, though I wasn’t really confident.”
</p><p>
	Park went to the factory the next day to discuss the problem—and his solution—with the manufacturing team. They refined his fix—replacing the toilet paper with a tiny slice of foam—and that’s how the first generation of Fitbits shipped.
</p><h2>Fitbit’s fast evolution
</h2><p>
	The company sold about 5,000 of those $99 first-generation units in 2009, and more than 10 times that number in 2010. The rollout wasn’t entirely smooth. Casciola recalls that Fitbit’s logistics center was sending him a surprising number of corroded devices that had been returned by customers. Casciola’s task was to tear them down and diagnose the problem.
</p><p>
	“One of the contacts on the device, over time, was growing a green corrosion,” Casciola says. “But the other two contacts were not.” It turned out the problem came from Casciola’s design of the system-reset trigger, which allowed users to reset the device without a reset button or a removable battery. “Inevitably,” Casciola says, “firmware is going to crash. When you can’t take the battery out, you have to have another way of forcing a reset; you don’t want to have someone waiting six days for the battery to run out before restarting.”
</p><p>
	The reset that Casciola designed was “a button on the charging station that you could poke with a paper clip. If you did this with the tracker sitting on the charger, it would reset. Of course, we had to have a way for the tracker to see that signal. When I designed the circuit to allow for that, I ended up with a nominal voltage on one pin.” This low voltage was causing the corrosion.
</p><p>
	“If you clipped the tracker onto sweaty clothing—remember, sweat has a high salt content—a very tiny current would flow,” says Casciola. “It was just fractions of a microamp, not enough to cause a reset, but enough, over time, to cause greenish corrosion.”
</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="Two men in white cleanroom suits with hoods stand in front of a door. " class="rm-shortcode rm-resized-image" data-rm-shortcode-id="1f0a065917e4a26e594027c95609c34f" data-rm-shortcode-name="rebelmouse-image" id="d380f" loading="lazy" src="https://spectrum.ieee.org/media-library/two-men-in-white-cleanroom-suits-with-hoods-stand-in-front-of-a-door.png?id=53101472&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Cofounders Eric Friedman [left] and James Park visit Fitbit’s manufacturer in December of 2008. </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">James Park </small>
</p><p>
	On the 2012 generation of the Fitbit, called the Fitbit One, Casciola added a new type of chip, one that hadn’t been available when he was working on the original design. It allowed the single button to trigger a reset when it was held down for some seconds while the device was sitting on the charger. That eliminated the need for the active pin.
</p><p>
	The charging interface was the source of another early problem. In the initial design, the trim of the Fitbit’s plastic casing was painted with chrome. “We originally wanted an actual metal trim,” Friedman says, “but that interfered with the radio signal.”
</p><p>
	Chrome wasn’t a great choice either. “It caused problems with the charger interface,” Park adds. “We had to do a lot of work to prevent shorting there.”
</p><p>
	They dropped the chrome after some tens of thousands of units were shipped—and then got compliments from purchasers about the new, chrome-less look.
</p><p>
	Evolution happened quickly, particularly in the way the device transmitted data. In 2012, when Bluetooth LE became widely available as a new low-power communications standard, the base station was replaced by a small Bluetooth communications dongle. And eventually the dongles disappeared altogether.
</p><p>
	“We had a huge debate about whether or not to keep shipping that dongle,” Park says. “Its cost was significant, and if you had a recent iPhone, you didn’t need it. But we didn’t want someone buying the device and then returning it because their cellphone couldn’t connect.” The team closely tracked the penetration rate of Bluetooth LE in cellphones; when they felt that number was high enough, they killed off the dongle.
</p><h2>Fitbit’s wrist-ward migration
</h2><p>
	After several iterations of the original Fitbit design, sometimes called the “clip” for its shape, the fitness tracker moved to the wrist. This wasn’t a matter of simply redesigning the way the device attached to the body but a rethinking of algorithms.
</p><p>
	The impetus came from some users’ desire to better track their sleep. The Fitbit’s algorithms allowed it to identify sleep patterns, a design choice that, Park says, “was pivotal, because it changed the device from being just an activity tracker to an all-day wellness tracker.” But nightclothes didn’t offer obvious spots for attachment. So the Fitbit shipped with a thin fabric wristband intended for use just at night. Users began asking customer support if they could keep the wristband on around the clock. The answer was no; Fitbit’s step-counting algorithms at the time didn’t support that.
</p><p class="pull-quote">
	“My father, who turned 80 on July 5, is fixated on his step count. From 11 at night until midnight, he’s in the parking garage, going up flights of stairs. And he is in better shape than I ever remember him.” <strong>—Eric Friedman</strong>
</p><p>
	Meanwhile, a cultural phenomenon was underway. In the mid-2000s, yellow
	<a href="https://livestrong.org/" target="_blank">Livestrong</a> bracelets, made out of silicone and sold to support cancer research, were suddenly everywhere. Other causes and movements jumped on the trend with their own brightly colored wristbands. By early 2013, Fitbit and its competitors Nike and Jawbone had launched wrist-worn fitness trackers in roughly the same style as those trendy bracelets. Fitbit’s version was called the Flex, once again designed by NewDealDesign.</p><h2>A no-button user interface for the Fitbit Flex
</h2><p>
	The Flex’s interface was even simpler than the original Fitbit’s one button and OLED screen: It had no buttons and no screen, just five LEDs arranged in a row and a vibrating motor. To change modes, you tapped on the surface.
</p><p>
	“We didn’t want to replace people’s watches,” Park says. The technology wasn’t yet ready to “build a compelling device—one that had a big screen and the compute power to drive really amazing interactions on the wrist that would be worthy of that screen. The technology trends didn’t converge to make that possible until 2014 or 2015.”
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A photo shows a hand wearing a light blue Fitbit Flex reaching toward a tablet displaying the Fitbit app. Another photo shows a black Fitbit Flex. " class="rm-shortcode" data-rm-shortcode-id="5ef67fcb86462f99244275fd1b54c541" data-rm-shortcode-name="rebelmouse-image" id="213ac" loading="lazy" src="https://spectrum.ieee.org/media-library/a-photo-shows-a-hand-wearing-a-light-blue-fitbit-flex-reaching-toward-a-tablet-displaying-the-fitbit-app-another-photo-shows-a.png?id=53101485&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">The Fitbit Flex [right], the first Fitbit designed to be worn on the wrist, was released in 2013. It had no buttons and no screen. Users controlled it by tapping; five LEDs indicated progress toward a step count selected via an app [left]. </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">iStock </small>
</p><p>
	“The amount of stuff the team was able to convey with just the LEDs was amazing,” Friedman recalls. “The status of where you are towards reaching your [step] goal, that’s obvious. But [also] the lights cycling to show that it’s searching for something, the vibrating when you hit your step goal, things like that.”
</p><p>
	The tap part of the interface, though, was “possibly something we didn’t get entirely right,” Park concedes. It took much fine-tuning of algorithms after the launch to better sort out what was
	<em><em>not</em></em> tapping—like applauding. Even more important, some users couldn’t quite intuit the right way to tap.
</p><p>
	“If it works for 98 percent of your users, but you’re growing to millions of users, 2 percent really starts adding up,” Park says. They brought the button back for the next generation of Fitbit devices.
</p><h2>And the rest is history
</h2><p>
	In 2010, its first full year on the market, the Fitbit sold some 50,000 units. Fitbit sales peaked in 2015, with almost 23 million devices sold that year, according to
	<a href="https://www.statista.com/statistics/472591/fitbit-devices-sold/" target="_blank">Statista</a>. Since then, there’s been a bit of a drop-off, as multifunctional smart watches have come down in price and grown in popularity and Fitbit knockoffs entered the market. In 2021, Fitbit still boasted more than 31 million active users, according to <a href="https://media.market.us/fitbit-usage-statistics/" target="_blank">Market.us.Media</a>. And Fitbit may now be riding the trend back to simplicity, as people find themselves wanting to get rid of distractions and <a href="https://www.iqmetrix.com/blog/dumb-phones-a-surprising-tech-trend-for-2024" target="_blank">move back to simpler devices</a>. I see this happening in my own family: My smartwatch-wearing daughter traded in that wearable for a Fitbit Charge 6 earlier this year.
</p><h3>Related Articles</h3><br/><p><a href="https://spectrum.ieee.org/my-first-fitbit" target="_blank">My First Fitbit</a></p><p><a href="https://spectrum.ieee.org/the-consumer-electronics-hall-of-fame-fitbit" target="_blank">The Consumer Electronics Hall of Fame: Fitbit</a></p><p>
	Fitbit went public in 2015 at a valuation of $4.1 billion. In 2021 Google completed its $2.1 billion purchase of the company and absorbed it into its hardware division. In April of this year, Park and Friedman left Google. Early retirement? Hardly. The two, now age 47, have started a new company that’s currently in stealth mode.
</p><p>
	The idea of encouraging people to be active by electronically tracking steps has had staying power.
</p><p>
	“My father, who turned 80 on July 5, is fixated on his step count,” Friedman says. “From 11 at night until midnight, he’s in the parking garage, going up flights of stairs. And he is in better shape than I ever remember him.”
</p><p>
	What could be a better reward than that?
	<span class="ieee-end-mark"></span>
</p>]]></description><pubDate>Wed, 07 Aug 2024 13:00:02 +0000</pubDate><guid>https://spectrum.ieee.org/fitbit</guid><category>Fitbit</category><category>Fitbit flex</category><category>Fitness tracker</category><category>Wearable</category><dc:creator>Tekla S. Perry</dc:creator><media:content medium="image" type="image/png" url="https://spectrum.ieee.org/media-library/a-closeup-of-a-hand-holding-a-black-fitbit-clipping-it-onto-a-pocket-the-word-step-is-visible-in-blue-text-on-the-device.png?id=53100495&amp;width=980"></media:content></item><item><title>Fitting It All In: Keys to Mastering Work-Life Balance</title><link>https://spectrum.ieee.org/work-life-balance</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/illustration-of-a-bunch-of-different-colored-shapes-haphazardly-piled-a-man-holding-triangle-on-the-left-and-a-woman-on-a-ladd.jpg?id=53119169&width=1200&height=800&coordinates=0%2C104%2C0%2C105"/><br/><br/><p><em><em>This article is part of our exclusive career advice series in partnership with the </em></em><a href="https://www.ieee-tems.org/" rel="noopener noreferrer" target="_blank"><u><em><em>I</em></em></u><u><em><em>EEE Technology and Engineering Management Society</em></em></u></a><em><em>. </em></em></p><p>With technological advancement and changing societal expectations, the concept of work-life balance has become an elusive goal for many, particularly within the engineering community. The drive to remain continuously engaged with work, the pressure to achieve perfection, and the challenge of juggling work and personal responsibilities have created a landscape where professional and personal spheres are in constant negotiation. </p><p>This article covers several factors that can disrupt <a href="https://spectrum.ieee.org/learn-who-we-have-to-thank-for-the-term-worklife-balance" target="_blank">work-life balance</a>, with recommendations on how to address them. </p><h2>The myth of urgency</h2><p>In an era dominated by instant communication via email and text messages, the expectation to respond quickly has led to an illusion of urgency. The perpetual state of constant alertness blurs the distinction between what’s urgent and what isn’t.</p><p> Recognizing that not every email message warrants an immediate response is the first step in deciding what’s important. By prioritizing responses based on actual importance, individuals can reclaim control over their time, reduce stress, and foster a more manageable workload.</p><p>Throughout my career, I have found that setting specific times to check and respond to email helps avoid distractions throughout the day. There are programs that prioritize email and classify tasks based on its urgency and importance.</p><p>Another suggestion is to unsubscribe from unnecessary newsletters and set up filters that move unwanted email to a specific folder or the trash before it reaches your inbox.</p><h2>Cutting back the endless workday</h2><p>Today’s work environment, characterized by remote access and flexible hours, has extended the workday beyond a set schedule and has encroached on personal time. The situation is particularly prevalent among engineers committed to solving complex problems, leading to a scenario where work is a constant companion—which leaves little room for personal pursuits or time with family.</p><p class="pull-quote">A balanced life is healthier and more sustainable, and it enriches the quality of our work and our relationships with those we love.</p><p>Establishing clear boundaries between work and personal time is essential. One way to do so is to communicate clear working hours to your manager, coworkers, and clients. You can use tools such as email autoresponders and do-not-disturb modes to reinforce your boundaries. </p><p>It’s important to recognize that work, while integral, is only one aspect of life.</p><h2>The quest for perfectionism</h2><p>The pursuit of perfection is a common trap for many professionals, leading to endless revisions and dissatisfaction with one’s work. The quest not only wastes an inordinate amount of time. It also detracts from the quality of life. </p><p>Embracing the philosophy that “it doesn’t have to be perfect” can liberate individuals from the trap. By aiming for excellence rather than perfection, one can achieve high standards of work while also making time for personal growth and happiness.</p><p>To help adopt such a mindset, practice setting realistic standards for different tasks by asking yourself what level of quality is truly necessary for each. Allocating a fixed amount of time to specific tasks can help prevent endless tweaking.</p><h2>The necessity of exercise</h2><p>Physical activity often takes a back seat to busy schedules and is often viewed as negotiable or secondary to work and family responsibilities. Exercise, however, is a critical component for maintaining mental and physical health. Integrating regular physical activity into one’s routine is not just beneficial; it’s essential for maintaining balance and enhancing your quality of life.</p><p>One way to ensure you are taking care of your health is to schedule exercise as a nonnegotiable activity in your calendar, similar to important meetings or activities. Also consider integrating physical activity into your daily routine, such as riding a bicycle to work, walking to meetings, and taking short strolls around your office building. If you work from home, take a walk around your neighborhood.</p><h2>Sleep boosts productivity</h2><p>Contrary to the glorification of overwork and sleep deprivation in some professional circles, sleep is a paramount factor in maintaining high levels of productivity and creativity. Numerous studies have shown that adequate sleep—seven to nine hours for most adults—enhances cognitive functions, problem-solving skills, and memory retention. </p><p>For engineers and others in professions where innovation and precision are paramount, neglecting sleep can diminish the quality of work and the capacity for critical thinking.</p><p>Sleep deprivation has been linked to a variety of health issues including increased risk of cardiovascular disease, diabetes, and stress-related conditions.</p><p>Prioritizing sleep is not a luxury but a necessity for those aiming to excel in their career while also enjoying a fulfilling personal life.</p><p>Begin your bedtime routine at the same time each night to cue your body that it’s time to wind down. For a smooth transition to sleep, try adjusting lighting, reducing noise, and engaging in relaxing activities such as reading or listening to calm music.</p><h2>Relaxation is the counterbalance to stress</h2><p>Relaxation is crucial for counteracting the effects of stress and preventing burnout. Techniques such as meditation, deep-breathing exercises, yoga, and engaging in leisure activities that bring joy can significantly reduce stress levels, thereby enhancing emotional equilibrium and resilience. </p><p>Spending time with friends and family is another effective relaxation strategy. Social interactions with loved ones can provide emotional support, happiness, and a sense of belonging, all of which are essential for limiting stress and promoting mental health. The social connections help build a support network that can serve as a buffer against life’s challenges, providing a sense of stability and comfort.</p><p>Allow yourself to recharge and foster a sense of fulfillment by allocating time each week to pursue interests that enrich your life. Also consider incorporating relaxation techniques in your daily routine, such as mindfulness meditation or short walks outdoors.</p><h2>Guarding time and energy</h2><p>In the quest for balance, learning to say no and ruthlessly eliminating activities that do not add value are invaluable skills. Make conscious choices about how to spend your time and energy, focusing on activities that align with personal and professional priorities. By doing so, individuals can protect their time, reduce stress, and dedicate themselves more fully to meaningful pursuits.</p><p>Practice assertiveness in communicating your capacity and boundaries to others. When asked to take on an additional task, it’s important to consider the impact on your current priorities. Don’t hesitate to decline politely if the new task doesn’t align.</p><h2>Challenges for women </h2><p>When discussing work-life balance, it’s essential to acknowledge the specific challenges faced by women, particularly in engineering. They are often expected to manage household duties, childcare, and their professional responsibilities while also supporting their partner’s career goals.</p><p>It can be especially <a href="https://spectrum.ieee.org/women-leave-tech-jobs-because-they-cant-climb-the-ladder" target="_blank">challenging for women</a> who strive to meet high standards at work and home. Recognizing and addressing their challenges is crucial in fostering an environment that supports balance for everyone.</p><p> One way to do that is to have open discussions with employers about the challenges and the support needed in the workplace and at home. Advocating for company policies that support work-life balance, such as a flexible work schedule and parental leave, is important.</p><p>Achieving a healthy work-life balance in the engineering profession—and indeed in any high-pressure field—is an ongoing process that requires self-awareness, clear priorities, and the courage to set boundaries. </p><p>It involves a collective effort by employers and workers to recognize the value of balance and to create a culture that supports it. </p><p>By acknowledging the illusion of constant urgency, understanding our limitations, and addressing the particular challenges faced by women, we can move toward a future where professional success and personal fulfillment are mutually reinforcing. </p><p>A balanced life is healthier and more sustainable, and it enriches the quality of our work and our relationships with those we love.</p>]]></description><pubDate>Tue, 06 Aug 2024 18:00:03 +0000</pubDate><guid>https://spectrum.ieee.org/work-life-balance</guid><category>Career advice</category><category>Careers</category><category>Ieee member news</category><category>Ieee technology and engineering management society</category><category>Type:ti</category><dc:creator>Mark Wehde</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/illustration-of-a-bunch-of-different-colored-shapes-haphazardly-piled-a-man-holding-triangle-on-the-left-and-a-woman-on-a-ladd.jpg?id=53119169&amp;width=980"></media:content></item><item><title>Figure 02 Robot Is a Sleeker, Smarter Humanoid</title><link>https://spectrum.ieee.org/figure-new-humanoid-robot</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-photo-of-the-torso-of-a-grey-and-black-humanoid-robot-with-a-shiny-black-face-plate-looking-at-its-hand.jpg?id=53107004&width=1200&height=800&coordinates=450%2C0%2C450%2C0"/><br/><br/><p>Today, <a href="https://spectrum.ieee.org/figure-humanoid-robot" target="_blank">Figure</a> is introducing the newest, slimmest, shiniest, and least creatively named next generation of its <a href="https://spectrum.ieee.org/humanoid-robots" target="_blank">humanoid robot</a>: Figure 02. According to the press release, Figure 02 is the result of “a ground-up hardware and software redesign” and is “the highest performing humanoid robot,” which may even be true for some arbitrary value of “performing.” Also notable is that Figure has been actively testing robots with BMW at a manufacturing plant in Spartanburg, S.C., where the new humanoid has been performing “data collection and use case training.”</p><p>The rest of the press release is pretty much, “Hey, check out our new robot!” And you’ll get all of the content in the release by watching the videos. What you <em><em>won’t</em></em> get from the videos is any additional info about the robot. But we sent along some questions to Figure about these videos, and have a few answers from <a href="https://www.linkedin.com/in/mr-michael-rose/" target="_blank">Michael Rose</a>, director of controls, and <a href="https://www.linkedin.com/in/vadimchernyak/" target="_blank">Vadim Chernyak</a>, director of hardware.</p><hr/><p>First, the trailer:</p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="c48692efbc65bd12448fc94b8d8b65b3" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/FZbY9sReu1k?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p><strong>How many parts does Figure 02 have, and is this all of them?</strong></p><p><strong></strong><strong>Figure: </strong>A couple hundred unique parts and a couple thousand parts total. No, this is not all of them.</p><p><strong>Does Figure 02 make little Figure logos with every step?</strong></p><p><strong>Figure: </strong>If the surface is soft enough, yes.</p><p><strong>Swappable legs! Was that hard to do, or easier to do because you only have to make one leg? Figure: </strong>We chose to make swappable legs to help with manufacturing.</p><p><strong>Is the battery pack swappable too? </strong></p><p><strong>Figure: </strong>Our battery is swappable, but it is not a quick swap procedure.</p><p><strong>What’s that squishy-looking stuff on the back of Figure 02’s knees and in its elbow joints?</strong></p><p><strong>Figure: </strong>These are soft stops which limit the range of motion in a controlled way and prevent robot pinch points</p><p><strong>Where’d you hide that thumb motor?</strong></p><p><strong>Figure: </strong>The thumb is now fully contained in the hand.</p><p><strong>Tell me about the “skin” on the neck!</strong></p><p><strong>Figure: </strong>The skin is a soft fabric which is able to keep a clean seamless look even as the robot moves its head.</p><div class="horizontal-rule"></div><p>And here’s the reveal video:</p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="846c1da5e4338257891a1dd6697c081c" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/0SRVJaOg9Co?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p><strong>When Figure 02’s head turns, its body turns too, and its arms move. Is that necessary, or aesthetic?</strong></p><p><strong>Figure: </strong>Aesthetic.</p><p><strong>The upper torso and shoulders seem very narrow compared to other humanoids. Why is that?</strong> </p><p><strong>Figure: </strong>We find it essential to package the robot to be of similar proportions to a human. This allows us to complete our target use cases and fit into our environment more easily.</p><p><strong>What can you tell me about Figure 02’s walking gait? </strong></p><p><strong>Figure: </strong>The robot is using a model predictive controller to determine footstep locations and forces required to maintain balance and follow the desired robot trajectory.</p><p><strong>How much runtime do you get from 2.25 kilowatt-hours doing the kinds of tasks that we see in the video?</strong></p><p><strong>Figure: </strong>We are targeting a 5-hour run time for our product.</p><div class="horizontal-rule"><br/></div><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A photo a grey and black humanoid robot with a shiny black face plate standing in front of a white wall." class="rm-shortcode" data-rm-shortcode-id="40f638c685f23bc7a4ea515a422ce502" data-rm-shortcode-name="rebelmouse-image" id="7fc98" loading="lazy" src="https://spectrum.ieee.org/media-library/a-photo-a-grey-and-black-humanoid-robot-with-a-shiny-black-face-plate-standing-in-front-of-a-white-wall.jpg?id=53107672&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Slick, but also a little sinister?</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Figure</small></p><p>This thing looks <em><em>slick</em></em>. I’d say that it’s maybe a little too far on the sinister side for a robot intended to work around humans, but the industrial design is badass and the packaging is excellent, with the vast majority of the wiring now integrated within the robot’s skins and flexible materials covering joints that are typically left bare. Figure, if you remember, <a href="https://spectrum.ieee.org/figure-robot-video" target="_self"><u>raised a US $675 million Series B that valued the company at $2.6 billion</u></a>, and somehow the look of this robot seems appropriate to that.</p><p>I do still have some questions about Figure 02, such as where the interesting foot design came from and whether a 16-degree-of-freedom hand is really worth it in the near term. It’s also worth mentioning that Figure seems to have a fair number of Figure 02 robots running around—at least five units at its California headquarters, plus potentially a couple of more at the BMW Spartanburg manufacturing facility. </p><p>I also want to highlight this boilerplate at the end of the release: “our humanoid is designed to perform human-like tasks within the workforce and in the home.” We are very, very far away from a humanoid robot in the home, but I appreciate that it’s still an explicit goal that Figure is trying to achieve. Because I want one.</p>]]></description><pubDate>Tue, 06 Aug 2024 13:06:44 +0000</pubDate><guid>https://spectrum.ieee.org/figure-new-humanoid-robot</guid><category>Humanoid robot</category><category>Figure</category><category>Robotics</category><dc:creator>Evan Ackerman</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-photo-of-the-torso-of-a-grey-and-black-humanoid-robot-with-a-shiny-black-face-plate-looking-at-its-hand.jpg?id=53107004&amp;width=980"></media:content></item><item><title>Rodney Brooks’s Three Laws of Robotics</title><link>https://spectrum.ieee.org/rodney-brooks-three-laws-robotics</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-red-robot-with-its-arms-around-a-a-man-with-a-blue-shirt-against-a-yellow-background.jpg?id=53100051&width=1200&height=800&coordinates=0%2C0%2C0%2C209"/><br/><br/><p>
<em><a href="https://rodneybrooks.com/" rel="noopener noreferrer" target="_blank">Rodney Brooks</a> is the <a href="https://people.csail.mit.edu/brooks/" target="_blank">Panasonic Professor of Robotics (emeritus) at MIT</a>, where he was director of the AI Lab and then <a href="https://www.csail.mit.edu/" target="_blank">CSAIL</a>. He has been cofounder of <a href="https://www.irobot.com/" target="_blank">iRobot</a>, <a href="https://spectrum.ieee.org/robotics/industrial-robots/rethink-robotics-baxter-robot-factory-worker" target="_self">Rethink Robotics</a>, and <a href="https://www.robust.ai/" target="_blank">Robust AI</a></em><em><u>, </u>where he is currently CTO. This article is shared with permission <a href="https://rodneybrooks.com/rodney-brooks-three-laws-of-robotics/" target="_blank">from his blog</a>.</em>
</p><p>
<span></span>Here are some of the things I’ve learned about robotics after working in the field for almost five decades.  In honor of Isaac Asimov and Arthur C. Clarke, my two boyhood go-to science fiction writers, I’m calling them my three laws of robotics.<br/>
</p><ol>
<li><strong>The visual appearance of a robot makes a promise about what it can do and how smart it is. It needs to deliver or slightly overdeliver on that promise or it will not be accepted.</strong></li>
<li><strong>When robots and people coexist in the same spaces, the robots must not take away from people’s agency, particularly when the robots are failing, as inevitably they will at times.</strong></li>
<li><strong>Technologies for robots need 10+ years of steady improvement beyond lab demos of the target tasks to mature to low cost and to have their limitations characterized well enough that they can deliver 99.9 percent of the time. Every 10 more years gets another 9 in reliability.</strong></li>
</ol><p>
	Below I explain each of these laws in more detail. But in a related post here are my <a href="https://rodneybrooks.com/rodney-brooks-three-laws-of-artificial-intelligence/" target="_blank">three laws of artificial intelligence</a>.
</p><p>
	Note that these laws are written from the point of view of making robots work in the real world, where people pay for them, and where people want return on their investment. This is very different from demonstrating robots or robot technologies in the laboratory.
</p><p>
	In the lab there is phalanx of graduate students eager to demonstrate their latest idea, on which they have worked very hard, to show its plausibility. Their interest is in showing that a technique or technology that they have developed is plausible and promising. They will do everything in their power to nurse the robot through the demonstration to make that point, and they will eagerly explain everything about what they have developed and what could come next.
</p><p>
	In the real world there is just the customer, or the employee or relative of the customer. The robot has to work with no external intervention from the people who designed and built it. It needs to be a good experience for the people around it or there will not be more sales to those, and perhaps other, customers.
</p><p>
	So these laws are not about what might, or could, be done. They are about real robots deployed in the real world. The laws are not about research demonstrations. They are about robots in everyday life.
</p><h2>The Promise Given By Appearance</h2><p>
	My various companies have produced all sorts of robots and sold them at scale. A lot of thought goes into the visual appearance of the robot when it is designed, as that tells the buyer or user what to expect from it.
</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="overhead view a black circle robot  with buttons on a white tiled floor" class="rm-shortcode rm-resized-image" data-rm-shortcode-id="ee4bc95025216173e5ae2fefcc89e5d7" data-rm-shortcode-name="rebelmouse-image" id="42223" loading="lazy" src="https://spectrum.ieee.org/media-library/overhead-view-a-black-circle-robot-with-buttons-on-a-white-tiled-floor.jpg?id=53087131&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">The iRobot Roomba was carefully designed to meld looks with function.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">iStock</small></p><p>
	The Roomba, from iRobot, looks like a flat disk. It cleans floors. The disk shape was so that it could turn in place without hitting anything it wasn’t already hitting. The low profile of the disk was so that it could get under the toe kicks in kitchens and clean the floor that is overhung just a little by kitchen cabinets. It does not look like it can go up and down stairs or even a single step up or step down in a house and it cannot. It has a handle, which makes it look like it can be picked up by a person, and it can be. Unlike fictional Rosey the Robot it does not look like it could clean windows, and it cannot. It cleans floors, and that is it.</p><p>
	The Packbot, the remotely operable military robot, also from iRobot, looked very different indeed. It has tracked wheels, like a miniature tank, and that appearance promises anyone who looks at it that it can go over rough terrain, and is not going to be stopped by steps or rocks or drops in terrain. When the Fukushima disaster happened, in 2011, Packbots were able to operate in the reactor buildings that had been smashed and wrecked by the tsunami, open door handles under remote control, drive up rubble-covered staircases and get their cameras pointed at analog pressure and temperature gauges so that workers trying to safely secure the nuclear plant had some data about what was happening in highly radioactive areas of the plant.
</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="a rectangle robot with treads and wheels and an arm in front tries to grab an object on the ground" class="rm-shortcode rm-resized-image" data-rm-shortcode-id="6d669c5db30a7180671d6cbbab447e31" data-rm-shortcode-name="rebelmouse-image" id="291a4" loading="lazy" src="https://spectrum.ieee.org/media-library/a-rectangle-robot-with-treads-and-wheels-and-an-arm-in-front-tries-to-grab-an-object-on-the-ground.jpg?id=53100428&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">An iRobot PackBot picks up a demonstration object at the Joint Robotics Repair Detachment at Victory Base Complex in Baghdad.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Alamy</small></p><p>
	The point of this first law of robotics is to warn against making a robot appear more than it actually is. Perhaps that will get funding for your company, leading investors to believe that in time the robot will be able to do all the things its physical appearance suggests it might be able to do. But it is going to disappoint customers when it cannot do the sorts of things that something with that physical appearance looks like it can do. Glamming up a robot risks overpromising what the robot as a product can actually do. That risks disappointing customers. And disappointed customers are not going to be advocates for your product/robot, nor be repeat buyers.</p><h2>Preserving People’s Agency</h2><p>
	The worst thing for its acceptance by people that a robot can do in the workplace is to make their jobs or lives harder, by not letting them do what they need to do.
</p><p>
	Robots that work in hospitals taking dirty sheets or dishes from a patient floor to where they are to be cleaned are meant to make the lives of the nurses easier. But often they do exactly the opposite. If the robots are not aware of what is happening and do not get out of the way when there is an emergency they will probably end up blocking some lifesaving work by the nurses—e.g., pushing a gurney with a critically ill patient on it to where they need to be for immediate treatment. That does not endear such a robot to the hospital staff. It has interfered with their main job function, a function of which the staff is proud, and what motivates them to do such work.
</p><p>
	A lesser, but still unacceptable behavior of robots in hospitals, is to have them wait in front of elevator doors, central, and blocking for people. It makes it harder for people to do some things they need to do all the time in that environment—enter and exit elevators.
</p><p>
	Those of us who live in San Francisco or Austin, Texas, have had firsthand views of robots annoying people daily for the last few years. The robots in question have been autonomous vehicles, driving around the city with no human occupant. I see these robots every single time I leave my house, whether on foot or by car.
</p><p>
	Some of the vehicles were notorious for blocking intersections, and there was absolutely nothing that other drivers, pedestrians, or police could do. We just had to wait until some remote operator hidden deep inside the company that deployed them decided to pay attention to the stuck vehicle and get it out of people’s way. Worse, they would wander into the scene of a fire where there were fire trucks and firefighters and actual buildings on fire, get confused and just stop, sometime on top of the fire hoses.
</p><p>
	There was no way for the firefighters to move the vehicles, nor communicate with them. This is in contrast to an automobile driven by a human driver. Firefighters can use their normal social interactions to communicate with a driver, and use their privileged position in society as frontline responders to apply social pressure on a human driver to cooperate with them. Not so with the autonomous vehicles.
</p><p>
	The autonomous vehicles took agency from people going about their regular business on the streets, but worse took away agency from firefighters whose role is to protect other humans. Deployed robots that do not respect people and what they need to do will not get respect from people and the robots will end up undeployed.
</p><h2>Robust Robots That Work Every Time<br/></h2><p>
	Making robots that work reliably in the real world is hard. In fact, making anything that works physically in the real world, and is reliable, is very hard.
</p><p>
	For a customer to be happy with a robot it must appear to work every time it tries a task, otherwise it will frustrate the user to the point that they will question whether it makes their life better or not.
</p><p>
	But what does <em><em>appear</em></em> mean here? It means that the user can have the assumption that it going to work, as their default understanding of what will happen in the world.
</p><p>
	The tricky part is that robots interact with the real physical world.
</p><p>
	Software programs interact with a well-understood abstracted machine, so they tend not fail in a manner where the instructions in them do not get executed in a consistent way by the hardware on which they are running. Those same programs may also interact with the physical world, be it a human being, a network connection, or an input device like a mouse. It is then that the programs might fail as the instructions in them are based on assumptions in the real world that are not met.
</p><p>
	Robots are subject to forces in the real world, subject to the exact position of objects relative to them, and subject to interacting with humans who are very variable in their behavior. There are no teams of graduate students or junior engineers eager to make the robot succeed on the 8,354th attempt to do the same thing that has worked so many times before. Getting software that adequately adapts to the uncertain changes in the world in that particular instance and that particular instant of time is where the real challenge arises in robotics.
</p><p>
	Great-looking videos are just not the same things as working for a customer every time. Most of what we see in the news about robots is lab demonstrations. There is no data on how general the solution is, nor how many takes it took to get the video that is shown. Even worse sometimes the videos are tele-operated or sped up many times over.
</p><p>
	I have rarely seen a new technology that is less than ten years out from a lab demo make it into a deployed robot. It takes time to see how well the method works, and to characterize it well enough that it is unlikely to fail in a deployed robot that is working by itself in the real world. Even then there will be failures, and it takes many more years of shaking out the problem areas and building it into the robot product in a defensive way so that the failure does not happen again.
</p><p>
	Most robots require kill buttons or estops on them so that a human can shut them down. If a customer ever feels the need to hit that button, then the people who have built and sold the robot have failed. They have not made it operate well enough that the robot never gets into a state where things are going that wrong.
</p>]]></description><pubDate>Tue, 06 Aug 2024 10:00:02 +0000</pubDate><guid>https://spectrum.ieee.org/rodney-brooks-three-laws-robotics</guid><category>Rodney brooks</category><category>Robotics</category><dc:creator>Rodney Brooks</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-red-robot-with-its-arms-around-a-a-man-with-a-blue-shirt-against-a-yellow-background.jpg?id=53100051&amp;width=980"></media:content></item><item><title>A New Type of Neural Network Is More Interpretable</title><link>https://spectrum.ieee.org/kan-neural-network</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/deep-purple-dots-and-lines-connected-together-with-0-s-and-1-s-inbetween-against-a-dark-background.jpg?id=53100147&width=1200&height=800&coordinates=0%2C104%2C0%2C105"/><br/><br/><p><a href="https://spectrum.ieee.org/what-is-deep-learning" target="_blank">Artificial neural networks</a>—algorithms inspired by biological brains—are at the center of modern artificial intelligence, behind both chatbots and image generators. But with their many neurons, they can be <a href="https://www.nature.com/articles/d41586-024-01314-y" rel="noopener noreferrer" target="_blank">black boxes</a>, their inner workings uninterpretable to users. </p><p>Researchers have now created a fundamentally new way to make neural networks that in some ways surpasses traditional systems. These new networks are more interpretable and also more accurate, proponents say, even when they’re smaller. Their developers say the way they learn to represent physics data concisely could help scientists uncover new laws of nature. </p><p class="pull-quote">“It’s great to see that there is a new architecture on the table.” <strong></strong><strong>—Brice Ménard, Johns Hopkins University</strong><strong></strong></p><p>For the past decade or more, engineers have mostly tweaked neural-network designs through trial and error, says Brice Ménard, a physicist at Johns Hopkins University who studies how neural networks operate but was not involved in the new work, which <a href="https://arxiv.org/abs/2404.19756" rel="noopener noreferrer" target="_blank">was posted on arXiv</a> in April. “It’s great to see that there is a new architecture on the table,” he says, especially one designed from first principles.</p><p>One way to think of neural networks is by analogy with neurons, or nodes, and synapses, or connections between those nodes. In traditional neural networks, called multi-layer perceptrons (MLPs), each synapse learns a weight—a number that determines <em>how strong</em> the connection is between those two neurons. The neurons are arranged in layers, such that a neuron from one layer takes input signals from the neurons in the previous layer, weighted by the strength of their synaptic connection. Each neuron then applies a simple function to the sum total of its inputs, called an activation function.</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="black text on a white background with red and blue lines connecting on the left and black lines connecting on the right " class="rm-shortcode" data-rm-shortcode-id="16c6aa72c8d9515c82ff8f3ee8448e30" data-rm-shortcode-name="rebelmouse-image" id="e42c9" loading="lazy" src="https://spectrum.ieee.org/media-library/black-text-on-a-white-background-with-red-and-blue-lines-connecting-on-the-left-and-black-lines-connecting-on-the-right.png?id=53100120&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">In traditional neural networks, sometimes called multi-layer perceptrons [left], each synapse learns a number called a weight, and each neuron applies a simple function to the sum of its inputs. In the new Kolmogorov-Arnold architecture [right], each synapse learns a function, and the neurons sum the outputs of those functions.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">The NSF Institute for Artificial Intelligence and Fundamental Interactions</small></p><p>In the new architecture, the synapses play a more complex role. Instead of simply learning <em>how strong</em> the connection between two neurons is, they learn the <em>full nature</em> of that connection—the function that maps input to output. Unlike the activation function used by neurons in the traditional architecture, this function could be more complex—in fact a “spline” or combination of several functions—and is different in each instance. Neurons, on the other hand, become simpler—they just sum the outputs of all their preceding synapses. The new networks are called Kolmogorov-Arnold Networks (KANs), after two mathematicians who studied how functions could be combined. The idea is that KANs would provide greater flexibility when learning to represent data, while using fewer learned parameters. </p><p class="pull-quote">“It’s like an alien life that looks at things from a different perspective but is also kind of understandable to humans.” <strong>—Ziming Liu, Massachusetts Institute of Technology</strong></p><p><span></span>The researchers tested their KANs on relatively simple scientific tasks. In some experiments, they took simple physical laws, such as the velocity with which two relativistic-speed objects pass each other. They used these equations to generate input-output data points, then, for each physics function, trained a network on some of the data and tested it on the rest. They found that increasing the size of KANs improves their performance at a faster rate than increasing the size of MLPs did. When solving partial differential equations, a KAN was 100 times as accurate as an MLP that had 100 times as many parameters.</p><p>In another experiment, they trained networks to predict one attribute of topological knots, called their signature, based on other attributes of the knots. An MLP achieved 78 percent test accuracy using about 300,000 parameters, while a KAN achieved 81.6 percent test accuracy using only about 200 parameters.</p><p>What’s more, the researchers could visually map out the KANs and look at the shapes of the activation functions, as well as the importance of each connection. Either manually or automatically they could prune weak connections and replace some activation functions with simpler ones, like sine or exponential functions. Then they could summarize the entire KAN in an intuitive one-line function (including all the component activation functions), in some cases perfectly reconstructing the physics function that created the dataset.</p><p>“In the future, we hope that it can be a <a href="https://spectrum.ieee.org/ai-for-science" target="_self">useful tool for everyday scientific research</a>,” says Ziming Liu, a computer scientist at the Massachusetts Institute of Technology and the paper’s first author. “Given a dataset we don’t know how to interpret, we just throw it to a KAN, and it can <a href="https://www.nature.com/articles/d41586-023-03596-0" target="_blank">generate some hypothesis</a> for you. You just stare at the brain [the KAN diagram] and you can even perform surgery on that if you want.” You might get a tidy function. “It’s like an alien life that looks at things from a different perspective but is also kind of understandable to humans.”</p><p>Dozens of papers have already cited the KAN preprint. “It seemed very exciting the moment that I saw it,” says Alexander Bodner, an undergraduate student of computer science at the University of San Andrés, in Argentina. Within a week, he and three classmates had combined KANs with convolutional neural networks, or CNNs, a popular architecture for processing images. They tested their <a href="https://arxiv.org/abs/2406.13155" target="_blank">Convolutional KANs</a> on their ability to categorize handwritten digits or pieces of clothing. The best one approximately matched the performance of a traditional CNN (99 percent accuracy for both networks on digits, 90 percent for both on clothing) but using about 60 percent fewer parameters. The datasets were simple, but Bodner says other teams with more computing power have begun scaling up the networks. Other people are combining KANs with transformers, an architecture popular in <a href="https://www.nature.com/articles/d41586-021-00530-0" target="_blank">large language models</a>.</p><p>One downside of KANs is that they take longer per parameter to train—in part because they can’t take advantage of GPUs. But they need fewer parameters. Liu notes that even if KANs don’t replace giant CNNs and transformers for processing images and language, training time won’t be an issue at the smaller scale of many physics problems. He’s looking at ways for experts to insert their prior knowledge into KANs—by manually choosing activation functions, say—and to easily extract knowledge from them using a simple interface. Someday, he says, KANs could help physicists discover high-temperature superconductors or ways to control nuclear fusion.</p>]]></description><pubDate>Mon, 05 Aug 2024 15:00:03 +0000</pubDate><guid>https://spectrum.ieee.org/kan-neural-network</guid><category>Neural networks</category><category>Machine learning</category><category>Ai safety</category><category>Artificial intelligence</category><dc:creator>Matthew Hutson</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/deep-purple-dots-and-lines-connected-together-with-0-s-and-1-s-inbetween-against-a-dark-background.jpg?id=53100147&amp;width=980"></media:content></item><item><title>Two Companies Plan to Fuel Cargo Ships With Ammonia</title><link>https://spectrum.ieee.org/ammonia-fuel</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/large-boat-with-shipping-containers-on-the-water.jpg?id=53086708&width=1200&height=800&coordinates=0%2C104%2C0%2C105"/><br/><br/><p>In July, two companies announced a collaboration aimed at helping to decarbonize maritime fuel technology. The companies, Brooklyn-based <a href="https://amogy.co/" target="_blank">Amogy</a> and Osaka-based <a href="https://www.yanmar.com/global/about/ygc/news/2024/01/09/128584.html" target="_blank">Yanmar</a>, say they plan to combine their respective areas of expertise to develop power plants for ships that use Amogy’s advanced technology for cracking ammonia to produce hydrogen fuel for Yanmar’s hydrogen internal combustion engines.<br/></p><p>This partnership responds directly to the maritime industry’s ambitious goals to significantly reduce greenhouse gas emissions. The <a href="https://www.imo.org/" target="_blank">International Maritime Organization</a> (IMO) has set stringent targets. It is calling for a <a href="https://www.imo.org/en/OurWork/Environment/Pages/2023-IMO-Strategy-on-Reduction-of-GHG-Emissions-from-Ships.aspx" target="_blank">40 percent reduction</a> in shipping’s carbon emissions from 2008 levels by 2030. But will the companies have a commercially available reformer-engine unit available in time for shipping fleet owners to launch vessels featuring this technology by the IMO’s deadline? The urgency is there, but so are the technical hurdles that come with new technologies.</p><p>Shipping accounts for less than<a href="https://theicct.org/sector/maritime-shipping/#:~:text=Shipping%20accounted%20for%20nearly%203,%2C%20which%20ICCT%20co%2Dauthored." target="_blank"> 3 percent of global human-caused CO<sub>2</sub> emissions</a>, but decarbonizing the industry would still have a profound impact on global efforts to combat climate change. According to the IMO’s <a href="https://www.imo.org/en/OurWork/Environment/Pages/Fourth-IMO-Greenhouse-Gas-Study-2020.aspx" rel="noopener noreferrer" target="_blank"><em>2020 Fourth Greenhouse Gas Study</em></a>, shipping produced 1,056 million tonnes of carbon dioxide in 2018.</p><p>Amogy and Yanmar did not respond to <em>IEEE Spectrum</em>‘s requests for comment about the specifics of how they plan to synergize their areas of focus. But John Prousalidis, a professor at the National Technical University of Athens’s School of Naval Architecture and Marine Engineering, spoke with <em>Spectrum</em> to help put the announcement in context.</p><p class="pull-quote">“We have a long way to go. I don’t mean to sound like a pessimist, but we have to be very cautious.” <strong>—John Prousalidis, National Technical University of Athens</strong></p><p>Prousalidis is among a group of researchers pushing for electrification of seaport activities as a means of cutting greenhouse gas emissions and reducing the amount of pollutants such as nitrogen oxides and sulfur oxides being spewed into the air by ships at berth and by the cranes, forklifts, and trucks that handle shipping containers in ports. He acknowledged that he hasn’t seen any information specific to Amogy and Yanmar’s technical ideas for using ammonia as ships’ primary fuel source for propulsion, but he has studied maritime sector trends long enough—and helped create standards for the <a href="https://www.ieee.org/" rel="noopener noreferrer" target="_blank">IEEE</a>, the <a href="https://www.iec.ch/homepage" rel="noopener noreferrer" target="_blank">International Electrotechnical Commission</a> (IEC), and the <a href="https://www.iso.org/home.html" rel="noopener noreferrer" target="_blank">International Organization for Standardization</a> (ISO)—in order to have a strong sense of how things will likely play out.</p><p>“We have a long way to go,” Prousalidis says. “I don’t mean to sound like a pessimist, but we have to be very cautious.” He points to NASA’s Artemis project, which is using hydrogen as its primary fuel for its rockets.</p><p>“The planned missile launch for a flight to the moon was repeatedly postponed because of a hydrogen leak that could not be well traced,” Prousalidis says. “If such a problem took place with one spaceship that is the singular focus of dozens of people who are paying attention to the most minor detail, imagine what could happen on any of the 100,000 ships sailing across the world?”</p><p>What’s more, he says, bold but ultimately unsubstantiated announcements from companies are fairly common. Amogy and Yanmar aren’t the first companies to suggest tapping into ammonia for cargo ships—<a href="https://spectrum.ieee.org/why-the-shipping-industry-is-betting-big-on-ammonia" target="_blank">the industry is no stranger</a> to plans to adopt the fuel to move massive ships across the world’s oceans.</p><p>“A couple of big pioneering companies have announced that they’re going to have ammonia-fueled ship propulsion pretty soon,” Prousalidis says. “Originally, they announced that it would be available at the end of 2022. Then they said the end of 2023. Now they’re saying something about 2025.”</p><p class="pull-quote">Shipping produced 1,056 million tonnes of carbon dioxide in 2018.</p><p>Prousalidis adds, “Everybody keeps claiming that ‘in a couple of years’ we’ll have [these alternatives to diesel for marine propulsion] ready. We periodically get these announcements about engines that will be hydrogen-ready or ammonia-ready. But I’m not sure what will happen during real operation. I’m sure that they performed several running tests in their industrial units. But in most cases, according to Murphy’s Law, failures will take place at the worst moment that we can imagine.”</p><p>All that notwithstanding, Prousalidis says he believes these technical hurdles will someday be solved, and engines running on alternative fuels will replace their diesel-fueled counterparts eventually. But he says he sees the rollout likely mirroring the introduction of natural gas. At the point when a few machines capable of running on that type of fuel were ready, the rest of the logistics chain was not. “We need to have all these brand-new pieces of equipment, including piping, that must be able to withstand the toxicity and combustibility of these new fuels. This is a big challenge, but it means that all engineers have work to do.”</p><p><em>Spectrum</em> also reached out to researchers at the U.S. Department of Energy’s Office of Energy Efficiency and Renewable Energy with several questions about what Amogy and Yanmar say they are looking to pull off. The DOE’s e-mail response: “Theoretically possible, but we don’t have enough technical details (temperature of coupling engine to cracker, difficulty of manifolding, startup dynamics, controls, etc.) to say for certain and if it is a good idea or not.”</p><p><em>This article was updated on 5 August 2024 to correct global shipping emission data.</em></p>]]></description><pubDate>Sat, 03 Aug 2024 13:00:02 +0000</pubDate><guid>https://spectrum.ieee.org/ammonia-fuel</guid><category>Climate tech</category><category>Decarbonize</category><category>Hydrogen</category><category>Cargo ships</category><category>Ammonia</category><dc:creator>Willie D. Jones</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/large-boat-with-shipping-containers-on-the-water.jpg?id=53086708&amp;width=980"></media:content></item><item><title>Video Friday: UC Berkeley’s Little Humanoid</title><link>https://spectrum.ieee.org/video-friday-berkeley-little-humanoid</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-compilation-of-shots-of-a-small-two-legged-robot-torso-falling-over-in-different-environments.gif?id=53076870&width=1200&height=800&coordinates=62%2C0%2C63%2C0"/><br/><br/><p>Video Friday is your weekly selection of awesome robotics videos, collected by your friends at <em>IEEE Spectrum</em> robotics. We also post a weekly calendar of upcoming robotics events for the next few months. Please <a href="mailto:automaton@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday">send us your events</a> for inclusion.<br/></p><h5><a href="https://icra40.ieee.org/">ICRA@40</a>: 23–26 September 2024, ROTTERDAM, NETHERLANDS</h5><h5><a href="https://iros2024-abudhabi.org/">IROS 2024</a>: 14–18 October 2024, ABU DHABI, UNITED ARAB EMIRATES</h5><h5><a href="https://icsr2024.dk/">ICSR 2024</a>: 23–26 October 2024, ODENSE, DENMARK</h5><h5><a href="https://cybathlon.ethz.ch/en/events/edition/cybathlon-2024">Cybathlon 2024</a>: 25–27 October 2024, ZURICH</h5><p>Enjoy today’s videos!</p><div class="horizontal-rule"></div><div style="page-break-after: always"><span style="display:none"> </span></div><blockquote class="rm-anchors" id="8pr1he-wmhw"><em>We introduce Berkeley Humanoid, a reliable and low-cost mid-scale humanoid research platform for learning-based control. Our lightweight, in-house-built robot is designed specifically for learning algorithms with low simulation complexity, anthropomorphic motion, and high reliability against falls. Capable of omnidirectional locomotion and withstanding large perturbations with a compact setup, our system aims for scalable, sim-to-real deployment of learning-based humanoid systems.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="44b5d5278b482dc831992ed45b20f1f8" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/8pR1HE-wMHw?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://berkeley-humanoid.com/">Berkeley Humanoid</a> ]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="xvzcvrct-2y"><em>This article presents Ray, a new type of audio-animatronic robot head. All the mechanical structure of the robot is built in one step by 3-D printing... This simple, lightweight structure and the separate tendon-based actuation system underneath allow for smooth, fast motions of the robot. We also develop an audio-driven motion generation module that automatically synthesizes natural and rhythmic motions of the head and mouth based on the given audio.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="b3eaaf3a2dccd5007ce02be1b40180b3" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/xVZCVRct-2Y?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://ieeexplore.ieee.org/document/10551622">Paper</a> ]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="w-l90bhfdfo"><em>CSAIL researchers introduce a novel approach allowing robots to be trained in simulations of scanned home environments, paving the way for customized household automation accessible to anyone.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="da6cbba32610c3bbe5c8e7488817c9c4" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/w-L90BhfDFo?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://news.mit.edu/2024/precision-home-robotics-real-sim-real-0731">MIT News</a> ]</p><div class="horizontal-rule"></div><p class="rm-anchors" id="4lryrrr4zpu">Okay, sign me up for this.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="94e47f01fe589cedc5a0594e4aef63d8" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/4lryrrR4zpU?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.deeprobotics.cn/en">Deep Robotics</a> ]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="ge7vdgr_j1g"><em>NEURA Robotics is among the first joining the early access NVIDIA Humanoid Robot Developer Program.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="54c3361f39be667981f578f5b9fd76ed" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/GE7VDgR_J1g?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>This could be great, but there’s an awful lot of jump cuts in that video.</p><p>[ <a href="https://neura-robotics.com/">Neura</a> ] via [ <a href="https://nvidianews.nvidia.com/news/nvidia-accelerates-worldwide-humanoid-robotics-development">NVIDIA</a> ]</p><div class="horizontal-rule"></div><p class="rm-anchors" id="iabddpujgly">I like that Unitree’s tagline in the video description here is “Let’s have fun together.”</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="c00428962e8b81468902b985247b5202" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/iaBDDpuJglY?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>Is that “please don’t do dumb stuff with our robots” at the end of the video new...?</p><p>[ <a href="https://www.unitree.com/">Unitree</a> ]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="jrd_de_kr1c"><em>NVIDIA CEO Jensen Huang presented a major breakthrough on Project GR00T with WIRED’s Lauren Goode at SIGGRAPH 2024. In a two-minute demonstration video, NVIDIA explained a systematic approach they discovered to scale up robot data, addressing one of the most challenging issues in robotics.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="0af0f3ba850d22d0a3a238f11bdaa21e" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/JRd_De_KR1c?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://nvidianews.nvidia.com/news/nvidia-accelerates-worldwide-humanoid-robotics-development" target="_blank">Nvidia</a> ]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="-nmgtxonwua"><em>In this research, we investigated the innovative use of a manipulator as a tail in quadruped robots to augment their physical capabilities. Previous studies have primarily focused on enhancing various abilities by attaching robotic tails that function solely as tails on quadruped robots. While these tails improve the performance of the robots, they come with several disadvantages, such as increased overall weight and higher costs. To mitigate these limitations, we propose the use of a 6-DoF manipulator as a tail, allowing it to serve both as a tail and as a manipulator.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="2065922425d3255f5e7ba1b895afbefa" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/-nMGtxonwUA?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://arxiv.org/abs/2407.10420">Paper</a> ]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="fzjdxd1pgpi"><em>In this end-to-end demo, we showcase how MenteeBot transforms the shopping experience for individuals, particularly those using wheelchairs. Through discussions with a global retailer, MenteeBot has been designed to act as the ultimate shopping companion, offering a seamless, natural experience.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="fc665df087dc0d63106e068e3662bf85" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/fzjdXD1pGpI?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://menteebot.com/blog/#shopping-companion-2024">Menteebot</a> ]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="rk_1whdlkb4"><em>Nature Fresh Farms, based in Leamington, Ontario, is one of North America’s largest greenhouse farms growing high-quality organics, berries, peppers, tomatoes, and cucumbers. In 2022, Nature Fresh partnered with Four Growers, a FANUC Authorized System Integrator, to develop a robotic system equipped with AI to harvest tomatoes in the greenhouse environment.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="fa080433250270e6b3c0a16935030463" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/Rk_1WHdlkb4?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.fanucamerica.com/case-studies/robotic-automation-ai-harvests-tomatoes">FANUC</a> ]</p><div class="horizontal-rule"></div><p class="rm-anchors" id="082whnjlunq">Contrary to what you may have been led to believe by several previous Video Fridays, WVUIRL’s open source rover is quite functional, most of the time.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="3cb10203ae3eaf8fb9df7e899a8c32ea" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/082wHnJlunQ?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://urc.orgs.wvu.edu/open-source-documentation">WVUIRL</a> ]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="wvrxenvlv0i"><em>Honeybee Robotics, a Blue Origin company, is developing Lunar Utility Navigation with Advanced Remote Sensing and Autonomous Beaming for Energy Redistribution, also known as LUNARSABER. In July 2024, Honeybee Robotics captured LUNARSABER’s capabilities during a demonstration of a scaled prototype.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="2b6c0e6fd6b4818e6242d15873671023" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/wvrxEnvLv0I?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.honeybeerobotics.com/">Honeybee Robotics</a> ]</p><div class="horizontal-rule"></div><p class="rm-anchors" id="kvedwjpd6lw">Bunker Mini is a compact tracked mobile robot specifically designed to tackle demanding off-road terrains.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="c365325ee279e56c426987fc0724a99d" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/kvedwJPd6lw?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://global.agilex.ai/products/bunker-mini">AgileX</a> ]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="cqx_f1t-bec"><em>In this video we present results of our lab from the latest field deployments conducted in the scope of the Digiforest EU project, in Stein am Rhein, Switzerland. Digiforest brings together various partners working on aerial and legged robots, autonomous harvesters, and forestry decision-makers. The goal of the project is to enable autonomous robot navigation, exploration, and mapping, both below and above the canopy, to create a data pipeline that can support and enhance foresters’ decision-making systems.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="31998f40d0f1c05b31ac24208f6e9728" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/CQx_F1t-bEc?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.autonomousrobotslab.com/">ARL</a> ]</p><div class="horizontal-rule"></div>]]></description><pubDate>Fri, 02 Aug 2024 16:00:02 +0000</pubDate><guid>https://spectrum.ieee.org/video-friday-berkeley-little-humanoid</guid><category>Video friday</category><category>Robotics</category><category>Humanoid robots</category><dc:creator>Evan Ackerman</dc:creator><media:content medium="image" type="image/gif" url="https://spectrum.ieee.org/media-library/a-compilation-of-shots-of-a-small-two-legged-robot-torso-falling-over-in-different-environments.gif?id=53076870&amp;width=980"></media:content></item><item><title>The President-Elect Candidates’ Plans to Further IEEE’s Mission</title><link>https://spectrum.ieee.org/2025-president-elect-position-statements</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/image.png?id=53073997&width=980"/><br/><br/><p>The <a href="https://www.ieee.org/about/corporate/election/index.html" rel="noopener noreferrer" target="_blank"><u>annual IEEE election process</u></a> begins this month, so be sure to check your mailbox for your ballot. To help you choose the 2025 IEEE president-elect, <em><em>The Institute</em></em> is publishing the official biographies and position statements of the three candidates, as approved by the IEEE Board of Directors. The candidates are IEEE Fellows Mary Ellen Randall, John Verboncoeur, and S.K. Ramesh.</p><p>In June, IEEE President Tom Coughlin moderated the <a href="https://ieeetv.ieee.org/channels/ieeetv-specials/meet-the-2025-ieee-president-elect-candidates-forum" rel="noopener noreferrer" target="_blank"><u>Meet the 2025 IEEE President-Elect Candidates Forum</u></a>, where the candidates were asked pressing questions from IEEE members.</p><h2>IEEE Fellow Mary Ellen Randall</h2><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="A smiling woman standing in front of a blue background." class="rm-shortcode rm-resized-image" data-rm-shortcode-id="9acaed9495e3c3714029aa736211016d" data-rm-shortcode-name="rebelmouse-image" id="bf2fa" loading="lazy" src="https://spectrum.ieee.org/media-library/a-smiling-woman-standing-in-front-of-a-blue-background.png?id=53074029&width=980" style="max-width: 100%"/>
<small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Deanna Decker Photography </small></p><p>Nominated by the IEEE Board of Directors</p><p>Randall founded Ascot Technologies in 2000 in Cary, N.C. Ascot develops enterprise applications using mobile data delivery technologies. She serves as the award-winning company’s CEO.</p><p>Before launching Ascot, she worked for IBM, where she held several technical and managerial positions in hardware and software development, digital video chips, and test design automation. She routinely managed international projects.</p><p>Randall has served as IEEE treasurer, director of IEEE Region 3, chair of IEEE Women in Engineering, and vice president of IEEE Member and Geographic Activities.</p><p>In 2016 she created the IEEE MOVE (Mobile Outreach VEhicle) program to assist with disaster relief efforts and for science, technology, engineering, and math educational purposes. </p><p>The IEEE-Eta Kappa Nu honor society member has received several honors including the 2020 IEEE Haraden Pratt Award, which recognizes outstanding volunteer service to IEEE.</p><p>She was named a top businesswoman in North Carolina’s Research Triangle Park area, and she made the 2003 <em><em>Business Leader</em></em> Impact 100 list.</p><h2>Candidate Statement</h2><p>Aristotle said, “the whole is greater than the sum of its parts.” Certainly, when looking at IEEE, this metaphysics phrase comes to my mind. In IEEE we have engineers and technical professionals developing, standardizing and utilizing technology from diverse perspectives. IEEE members around the world:</p><ul><li>perform and share research, product development activities, and standard development</li><li>network and engage with each other and their communities</li><li>educate current and future technology professionals</li><li>measure performance and quality</li><li>formulate ethics choices</li><li>and many more – these are just a few examples!</li></ul><p>We perform these actions across a wide spectrum of in-depth subjects. It is our diversity, yet oneness, that makes me confident we have a positive future ahead. How do we execute on Aristotle’s vision? First, we need to unite on mission goals which span our areas of interest. This way we can bring multiple disciplines and perspectives together to accomplish those big goals. Our strategy will guide our actions in this regard.</p><p>Second, we need to streamline our financing of new innovations and systematize the introduction of these programs. </p><p>Third, we need to execute and support our best ideas on a continuing basis. </p><p>As President, I pledge to:</p><p><strong>Institute</strong> innovative products and services to ensure our mutually successful future;</p><p><strong>Engage</strong> stakeholders (members, partners and communities) to unite on a comprehensive vision;</p><p><strong>Expand</strong> technology advancement and adoption throughout the world;</p><p><strong>Execute</strong> with excellence, ethics, and financial responsibility.</p><p>Finally, I promise to lead by example with enthusiasm and integrity and I humbly ask for your vote.</p><h2>IEEE Fellow John Verboncoeur</h2><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="A photo of a man in a grey suit and multicolored tie." class="rm-shortcode rm-resized-image" data-rm-shortcode-id="a2fd98c0d4d213fa53d08f1fd7740cce" data-rm-shortcode-name="rebelmouse-image" id="d27a7" loading="lazy" src="https://spectrum.ieee.org/media-library/a-photo-of-a-man-in-a-grey-suit-and-multicolored-tie.png?id=53074063&width=980" style="max-width: 100%"/>
<small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Steven Miller</small></p><p>Nominated by the IEEE Board of Directors</p><p>Verboncoeur is senior associate dean for research and graduate studies in Michigan State University’s (MSU) engineering college, in East Lansing.</p><p>In 2001 he founded the computational engineering science program at the University of California, Berkeley, chairing it until 2010.</p><p>In 2015 he cofounded the MSU computational mathematics, science, and engineering department.</p><p>His area of interest is plasma physics, with over 500 publications and over 6,800 citations.</p><p>He is on the boards of Physics of Plasmas, the American Center for Mobility, and the U.S. Department of Energy Fusion Energy Science Advisory Committee.</p><p>Verboncoeur has led startups developing digital exercise and health systems and the consumer credit report. He also had a role in developing the U.S. Postal Service’s mail-forwarding system.</p><p>His IEEE experience includes serving as 2023 vice president of Technical Activities, 2020 acting vice president of Publication Services and Products Board, 2019-2020 Division IV director, and 2015—2016 president of the Nuclear and Plasma Sciences Society. </p><p>He received a Ph.D. in 1992 in nuclear engineering from UC Berkeley.</p><h2>Candidate Statement</h2><p><strong>Ensure IEEE remains THE premier professional technical organization,</strong> deliver value via new participants, products and programs, including events, publications, and innovative personalized products and services, to enable our community to change the world. Key strategic programs include:</p><p><strong>Climate Change Technologies</strong> <strong>(CCT):</strong> Existential to humanity, addressing mitigation and adaptation must include technology R&D, local relevance for practitioners, university and K-12 students, the general public, media and policymakers and local and global standards.</p><p><strong>Smart Agrofood Systems</strong> <strong>(SmartAg):</strong> Smart technologies applied to the food supply chain from soil to consumer to compost.</p><p><strong>Artificial Intelligence</strong> <strong>(AI):</strong> Implications from technology to business to ethics. A key methodology for providing personalized IEEE products and services within our existing portfolio, and engaging new audiences such as technology decision makers in academia, government and technology finance by extracting value from our vast data to identify emerging trends.</p><p>Organizational growth opportunities include scaling and coordinating our public policy strategy worldwide, building on our credibility to inform and educate. Global communications capability is critical to coordinate and amplify our impact. Lastly, we need to enhance our ability to execute IEEE-wide programs and initiatives, from investment in transformative tools and products to mission-based education, outreach and engagement. This can be accomplished by judicious use of resources generated by business activities through creation of a strategic program to invest in our future with the goal of advancing technology for humanity.</p><p>With a passion for the nexus of technology with finance and public policy, I hope to earn your support.</p><h2>IEEE Fellow S.K. Ramesh</h2><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="A photo a smiling man in a dark suit and a red tie.  " class="rm-shortcode rm-resized-image" data-rm-shortcode-id="90f2539bd6345ab3441e6675b8096ac9" data-rm-shortcode-name="rebelmouse-image" id="d6e70" loading="lazy" src="https://spectrum.ieee.org/media-library/a-photo-a-smiling-man-in-a-dark-suit-and-a-red-tie.png?id=53074054&width=980" style="max-width: 100%"/>
<small class="image-media media-photo-credit" placeholder="Add Photo Credit...">S.K. Ramesh</small></p><p>Nominated by the IEEE Board of Directors</p><p>Ramesh is a professor of electrical and computer engineering at California State University Northridge’s college of engineering and computer science, where he served as dean from 2006 to 2017.</p><p>An IEEE volunteer for 42 years, he has served on the IEEE Board of Directors, the Publication Services and Products Board, Awards Board, and the Fellows Committee. Leadership positions he has held include vice president of IEEE Educational Activities, president of the IEEE-Eta Kappa Nu honor society, and chair of the IEEE Hearing Board.</p><p>As the 2016–2017 vice president of IEEE Educational Activities, he championed several successful programs including the IEEE Learning Network and the IEEE TryEngineering Summer Institute.</p><p>Ramesh served as the 2022–2023 president of ABET, the global accrediting organization for academic programs in applied science, computing, engineering, and technology.</p><p>He received his bachelor’s degree in electronics and communication engineering from the University of Madras in India. He earned his master’s degree in EE and Ph.D. in molecular science from Southern Illinois University, in Carbondale.</p><h2>Candidate Statement</h2><p>We live in an era of rapid technological development where change is constant. My leadership experiences of four decades across IEEE and ABET have taught me some timeless values in this rapidly changing world: To be <strong>I</strong>nclusive, <strong>C</strong>ollaborative, <strong>A</strong>ccountable, <strong>R</strong>esilient and <strong>E</strong>thical. Connection and community make a difference. IEEE’s mission is especially important, as the pace of change accelerates with advances in AI, Robotics and Biotechnology. I offer leadership that inspires others to believe and enable that belief to become reality. <strong>“I CARE”!</strong></p><p>My top priority is to serve our members and empower our technical communities worldwide to create and advance technologies to solve our greatest challenges.</p><p>If elected, I will focus on three strategic areas: </p><p><em><strong><em>Member Engagement:</em></strong></em></p><ul><li>Broaden participation of Students, Young Professionals (YPs), and Women in Engineering (WIE).</li><li>Expand access to affordable continuing education programs through the IEEE Learning Network (ILN).</li></ul><p><em><strong><em>Volunteer Engagement: </em></strong></em></p><ul><li>Nurture and support IEEE’s volunteer leaders to transform IEEE globally through a volunteer academy program that strengthens collaboration, inclusion, and recognition.</li><li>Incentivize volunteers to improve cross-regional collaboration, engagement and communications between Chapters and Sections.</li></ul><p><em><strong><em>Industry Engagement:</em></strong></em></p><ul><li>Transform hybrid/virtual conferences, and open access publications, to make them more relevant to engineers and technologists in industry.</li><li>Focus on innovation, standards, and sustainable development that address skills needed for jobs of the future.</li></ul><p>Our members are the “heart and soul” of IEEE. Let’s work together as <strong>one IEEE</strong> to attract, retain, and serve our diverse global members. Thank you for your participation and support.</p>]]></description><pubDate>Thu, 01 Aug 2024 18:00:02 +0000</pubDate><guid>https://spectrum.ieee.org/2025-president-elect-position-statements</guid><category>Ieee election</category><category>Ieee news</category><category>Type:ti</category><dc:creator>Joanna Goodrich</dc:creator><media:content medium="image" type="image/png" url="https://assets.rbl.ms/53073997/origin.png"></media:content></item><item><title>The Saga of AD-X2, the Battery Additive That Roiled the NBS</title><link>https://spectrum.ieee.org/ad-x2-battery-additive</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/photo-of-a-red-and-white-box-with-the-words-battery-ad-x2-and-a-photo-of-a-mans-face-on-it.jpg?id=52982022&width=1200&height=800&coordinates=0%2C0%2C0%2C372"/><br/><br/><p>Senate hearings, a post office ban, the resignation of the director of the National Bureau of Standards, and his reinstatement after more than 400 scientists threatened to resign. Who knew a little box of salt could stir up such drama?</p><h2>What was AD-X2?</h2><p>It all started in 1947 when a bulldozer operator with a 6th grade education, Jess M. Ritchie, teamed up with UC Berkeley chemistry professor <a href="https://oac.cdlib.org/view?docId=hb9g5008vb&doc.view=frames&chunk.id=div00007&toc.depth=1&toc.id=" rel="noopener noreferrer" target="_blank"><u>Merle Randall</u></a> to promote <a href="https://www.nist.gov/ad-x2" rel="noopener noreferrer" target="_blank"><u>AD-X2</u></a>, an additive to extend the life of lead-acid batteries. The problem of these rechargeable batteries’ dwindling capacity was well known. If AD-X2 worked as advertised, millions of car owners would save money.</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="Black and white photo of a man in a suit holding an object in his hands and talking." class="rm-shortcode rm-resized-image" data-rm-shortcode-id="32ff7fae7e81e8ab1c37839b93882536" data-rm-shortcode-name="rebelmouse-image" id="6603d" loading="lazy" src="https://spectrum.ieee.org/media-library/black-and-white-photo-of-a-man-in-a-suit-holding-an-object-in-his-hands-and-talking.jpg?id=52982052&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..." style="max-width: 100%;">Jess M. Ritchie demonstrates his AD-X2 battery additive before the Senate Select Committee on Small Business.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..." style="max-width: 100%;">National Institute of Standards and Technology Digital Collections</small></p><p>A basic lead-acid battery has two electrodes, one of lead and the other of lead dioxide, immersed in dilute sulfuric acid. When power is drawn from the battery, the chemical reaction splits the acid molecules, and lead sulfate is deposited in the solution. When the battery is charged, the chemical process reverses, returning the electrodes to their original state—almost. Each time the cell is discharged, the lead sulfate “hardens” and less of it can dissolve in the sulfuric acid. Over time, it flakes off, and the battery loses capacity until it’s dead.</p><p>By the 1930s, so many companies had come up with battery additives that the U.S. National Bureau of Standards stepped in. Its lab tests revealed that most were variations of salt mixtures, such as sodium and magnesium sulfates. Although the additives might help the battery charge faster, they didn’t extend battery life. In May 1931, NBS (now the <a href="https://www.nist.gov/" rel="noopener noreferrer" target="_blank"><u>National Institute of Standards and Technology</u></a>, or NIST) summarized its findings in <a href="https://www.nist.gov/system/files/documents/2023/04/28/nbslettercircular302.pdf" rel="noopener noreferrer" target="_blank"><u>Letter Circular No. 302</u></a>: “No case has been found in which this fundamental reaction is materially altered by the use of these battery compounds and solutions.” </p><p>Of course, innovation never stops. Entrepreneurs kept bringing new battery additives to market, and the NBS kept testing them and finding them ineffective.</p><h2>Do battery additives work?</h2><p>After World War II, the National Better Business Bureau decided to update its own publication on battery additives, “Battery Compounds and Solutions.” The publication included a March 1949 letter from NBS director <a href="https://en.wikipedia.org/wiki/Edward_Condon" rel="noopener noreferrer" target="_blank"><u>Edward Condon, reiterating </u></a>the NBS position on additives. Prior to heading NBS, Condon, a physicist, had been associate director of research at Westinghouse Electric in Pittsburgh and a consultant to the National Defense Research Committee. He helped set up <a href="https://www.ll.mit.edu/about/history/mit-radiation-laboratory" rel="noopener noreferrer" target="_blank"><u>MIT’s Radiation Laboratory</u></a>, and he was also briefly part of the Manhattan Project. Needless to say, Condon was familiar with standard practices for research and testing.</p><p>Meanwhile, Ritchie claimed that AD-X2’s secret formula set it apart from the hundreds of other additives on the market. He convinced his senator, William Knowland, a Republican from Oakland, Calif., to write to NBS and request that AD-X2 be tested. NBS declined, not out of any prejudice or ill will, but because it tested products only at the request of other government agencies. The bureau also had a longstanding policy of not naming the brands it tested and not allowing its findings to be used in advertisements.</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="Photo of a product box with directions printed on it." class="rm-shortcode rm-resized-image" data-rm-shortcode-id="78728691e0019c93f31f5780f2cffdc8" data-rm-shortcode-name="rebelmouse-image" id="bf39b" loading="lazy" src="https://spectrum.ieee.org/media-library/photo-of-a-product-box-with-directions-printed-on-it.jpg?id=52982060&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..." style="max-width: 100%;">AD-X2 consisted mainly of Epsom salt and Glauber’s salt.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..." style="max-width: 100%;">National Institute of Standards and Technology Digital Collections</small></p><p>Ritchie cried foul, claiming that NBS was keeping new businesses from entering the marketplace. Merle Randall launched an aggressive correspondence with Condon and George W. Vinal, chief of NBS’s electrochemistry section, extolling AD-X2 and the testimonials of many users. In its responses, NBS patiently pointed out the difference between anecdotal evidence and rigorous lab testing.</p><p>Enter the Federal Trade Commission. The FTC had received a complaint from the National Better Business Bureau, which suspected that Pioneers, Inc.—Randall and Ritchie’s distribution company—was making false advertising claims. On 22 March 1950, the FTC formally asked NBS to test AD-X2.</p><p>By then, NBS had already extensively tested the additive. A chemical analysis revealed that it was 46.6 percent magnesium sulfate (Epsom salt) and 49.2 percent sodium sulfate (Glauber’s salt, a horse laxative) with the remainder being water of hydration (water that’s been chemically treated to form a hydrate). That is, AD-X2 was similar in composition to every other additive on the market. But, because of its policy of not disclosing which brands it tests, NBS didn’t immediately announce what it had learned. </p><h2>The David and Goliath of battery additives</h2><p>NBS then did something unusual: It agreed to ignore its own policy and let the National Better Business Bureau include the results of its AD-X2 tests in a public statement, which was published in August 1950. The NBBB allowed Pioneers to include a dissenting comment: “These tests were not run in accordance with our specification and therefore did not indicate the value to be derived from our product.”</p><p>Far from being cowed by the NBBB’s statement, Ritchie was energized, and his story was taken up by the mainstream media. <em><em>Newsweek</em></em>’s coverage pitted an up-from-your-bootstraps David against an overreaching governmental Goliath. Trade publications, such as <em><em>Western Construction News </em></em>and <em><em>Batteryman, </em></em>also published flattering stories about Pioneers. AD-X2 sales soared. </p><p>Then, in January 1951, NBS released its updated pamphlet on battery additives, <a href="https://www.nist.gov/system/files/documents/2023/04/28/NBS%20Circular%20504.pdf" rel="noopener noreferrer" target="_blank"><u>Circular 504</u></a>. Once again, tests by the NBS found no difference in performance between batteries treated with additives and the untreated control group. The Government Printing Office sold the circular for 15 cents, and it was one of NBS’s most popular publications. AD-X2 sales plummeted.</p><p>Ritchie needed a new arena in which to challenge NBS. He turned to politics. He called on all of his distributors to write to their senators. Between July and December 1951, 28 U.S. senators and one U.S. representative wrote to NBS on behalf of Pioneers. </p><p>Condon was losing his ability to effectively represent the Bureau. Although the Senate had confirmed Condon’s nomination as director without opposition in 1945, he was under investigation by the <a href="https://www.aai.org/About/History/History-Articles/Protesting-the-Politicization-of-Science-AAI-Decr#gsc.tab=0" rel="noopener noreferrer" target="_blank"><u>House Committee on Un-American Activities</u></a> for several years. FBI Director J. Edgar Hoover suspected Condon to be a Soviet spy. (To be fair, Hoover suspected the same of many people.) Condon was repeatedly cleared and had the public backing of many prominent scientists.</p><p>But Condon felt the investigations were becoming too much of a distraction, and so he resigned on 10 August 1951. <a href="https://www.nasonline.org/member-directory/deceased-members/58143.html" rel="noopener noreferrer" target="_blank"><u>Allen V. Astin</u></a> became acting director, and then permanent director the following year. And he inherited the AD-X2 mess.</p><p>Astin had been with NBS since 1930. Originally working in the electronics division, he developed radio telemetry techniques, and he designed instruments to study dielectric materials and measurements. During World War II, he shifted to military R&D, most notably development of the <a href="https://nvlpubs.nist.gov/nistpubs/sp958-lide/059-062.pdf" rel="noopener noreferrer" target="_blank"><u>proximity fuse</u></a>, which detonates an explosive device as it approaches a target. I don’t think that work prepared him for the political bombs that Ritchie and his supporters kept lobbing at him.</p><h2>Mr. Ritchie almost goes to Washington</h2><p>On 6 September 1951, another government agency entered the fray. C.C. Garner, chief inspector of the U.S. Post Office Department, wrote to Astin requesting yet another test of AD-X2. NBS dutifully submitted a report that the additive had “no beneficial effects on the performance of lead acid batteries.” The post office then charged Pioneers with mail fraud, and Ritchie was ordered to appear at a hearing in Washington, D.C., on 6 April 1952. More tests were ordered, and the hearing was delayed for months.</p><p>Back in March 1950, Ritchie had lost his biggest champion when Merle Randall died. In preparation for the hearing, Ritchie hired another scientist: Keith J. Laidler, an assistant professor of chemistry at the <a href="https://www.catholic.edu/" rel="noopener noreferrer" target="_blank"><u>Catholic University of America</u></a>. Laidler wrote a critique of Circular 504, questioning NBS’s objectivity and testing protocols.</p><p>Ritchie also got Harold Weber, a professor of chemical engineering at MIT, to agree to test AD-X2 and to work as an unpaid consultant to the Senate Select Committee on Small Business. </p><p>Life was about to get more complicated for Astin and NBS.</p><h2>Why did the NBS Director resign?</h2><p>Trying to put an end to the Pioneers affair, Astin agreed in the spring of 1952 that NBS would conduct a public test of AD-X2 according to terms set by Ritchie. Once again, the bureau concluded that the battery additive had no beneficial effect.</p><p>However, NBS deviated slightly from the agreed-upon parameters for the test. Although the bureau had a good scientific reason for the minor change, Ritchie had a predictably overblown reaction—NBS cheated!</p><p>Then, on 18 December 1952, the Senate Select Committee on Small Business—for which Ritchie’s ally Harold Weber was consulting—issued a press release summarizing the results from the MIT tests: AD-X2 worked! The results “demonstrate beyond a reasonable doubt that this material is in fact valuable, and give complete support to the claims of the manufacturer.” NBS was “simply psychologically incapable of giving Battery AD-X2 a fair trial.”</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="Black and white photo of a man standing next to a row of lead-acid batteries. " class="rm-shortcode" data-rm-shortcode-id="dc9952334e35aa00d15511dbb56324ef" data-rm-shortcode-name="rebelmouse-image" id="9388f" loading="lazy" src="https://spectrum.ieee.org/media-library/black-and-white-photo-of-a-man-standing-next-to-a-row-of-lead-acid-batteries.jpg?id=52982062&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">The National Bureau of Standards’ regular tests of battery additives found that the products did not work as claimed.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">National Institute of Standards and Technology Digital Collections</small></p><p>But the press release distorted the MIT results.The MIT tests had focused on diluted solutions and slow charging rates, not the normal use conditions for automobiles, and even then AD-X2’s impact was marginal. Once NBS scientists got their hands on the report, they identified the flaws in the testing. </p><h2>How did the AD-X2 controversy end?</h2><p>The post office finally got around to holding its mail fraud hearing in the fall of 1952. Ritchie failed to attend in person and didn’t realize his reports would not be read into the record without him, which meant the hearing was decidedly one-sided in favor of NBS. On 27 February 1953, the Post Office Department issued a mail fraud alert. All of Pioneers’ mail would be stopped and returned to sender stamped “fraudulent.” If this charge stuck, Ritchie’s business would crumble.</p><p>But something else happened during the fall of 1952: Dwight D. Eisenhower, running on a pro-business platform, was elected U.S. president in a landslide. </p><p>Ritchie found a sympathetic ear in Eisenhower’s newly appointed Secretary of Commerce <a href="https://millercenter.org/president/eisenhower/essays/weeks-1953-secretary-of-commerce" rel="noopener noreferrer" target="_blank"><u>Sinclair Weeks</u></a>, who acted decisively. The mail fraud alert had been issued on a Friday. Over the weekend, Weeks had a letter hand-delivered to Postmaster General <a href="https://postalmuseum.si.edu/a-postmaster-general-for-the-future" rel="noopener noreferrer" target="_blank"><u>Arthur Summerfield</u></a>, another Eisenhower appointee. By Monday, the fraud alert had been suspended.</p><p>What’s more, Weeks found that Astin was “not sufficiently objective” and lacked a “business point of view,” and so he asked for Astin’s resignation on 24 March 1953. Astin complied. Perhaps Weeks thought this would be a mundane dismissal, just one of the thousands of political appointments that change hands with every new administration. That was not the case.</p><p>More than 400 NBS scientists—over 10 percent of the bureau’s technical staff— threatened to resign in protest. The American Academy for the Advancement of Science also backed Astin and NBS. In an editorial published in <em><em>Science</em></em>, the AAAS called the <a href="https://www.science.org/doi/pdf/10.1126/science.117.3042.3.s" rel="noopener noreferrer" target="_blank"><u>battery additive controversy</u></a> itself “minor.” “The important issue is the fact that the independence of the scientist in his findings has been challenged, that a gross injustice has been done, and that scientific work in the government has been placed in jeopardy,” the editorial stated. </p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="Two black and white portrait photos of men in suits. " class="rm-shortcode" data-rm-shortcode-id="34bd0da44f2dd2e09ae6c0483df84838" data-rm-shortcode-name="rebelmouse-image" id="91c8a" loading="lazy" src="https://spectrum.ieee.org/media-library/two-black-and-white-portrait-photos-of-men-in-suits.jpg?id=52982043&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">National Bureau of Standards director Edward Condon [left] resigned in 1951 because investigations into his political beliefs were impeding his ability to represent the bureau. Incoming director Allen V. Astin [right] inherited the AD-X2 controversy, which eventually led to Astin’s dismissal and then his reinstatement after a large-scale protest by NBS researchers and others. </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">National Institute of Standards and Technology Digital Collections</small></p><p>Clearly, AD-X2’s effectiveness was no longer the central issue. The controversy was a stand-in for a larger debate concerning the role of government in supporting small business, the use of science in making policy decisions, and the independence of researchers. Over the previous few years, highly respected scientists, including Edward Condon and J. Robert Oppenheimer, had been repeatedly investigated for their political beliefs. The request for Astin’s resignation was yet another government incursion into scientific freedom. </p><p>Weeks, realizing his mistake, temporarily reinstated Astin on 17 April 1953, the day the resignation was supposed to take effect. He also asked the National Academy of Sciences to test AD-X2 in both the lab and the field. By the time the academy’s report came out in October 1953, Weeks had permanently reinstated Astin. The report, unsurprisingly, concluded that NBS was correct: AD-X2 had no merit. Science had won.</p><h2>NIST makes a movie</h2><p>On 9 December 2023, NIST released the 20-minute docudrama <a href="https://www.youtube.com/watch?v=ve6b6wrGbdo" rel="noopener noreferrer" target="_blank"><u><em><em>The AD-X2 Controversy</em></em></u></a><em><em>. </em></em>The film won the Best True Story Narrative and Best of Festival at the 2023 <a href="https://www.thenewsfest.com/" rel="noopener noreferrer" target="_blank"><u>NewsFest Film Festival</u></a>. I recommend taking the time to watch it. </p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="493b2511852593bf84b1a83592748ef7" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/ve6b6wrGbdo?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
<small class="image-media media-caption" placeholder="Add Photo Caption...">The AD-X2 Controversy</small>
<small class="image-media media-photo-credit" placeholder="Add Photo Credit...">
<a href="https://www.youtube.com/watch?v=ve6b6wrGbdo" target="_blank">www.youtube.com</a>
</small>
</p><p>Many of the actors are NIST staff and scientists, and they really get into their roles. Much of the dialogue comes verbatim from primary sources, including congressional hearings and contemporary newspaper accounts.<br/></p><p>Despite being an in-house production, NIST’s film has a Hollywood connection. The film features brief interviews with actors John and Sean Astin (of <em><em>Lord of The Rings</em></em> and <em><em>Stranger Things</em></em> fame)—NBS director Astin’s son and grandson. </p><p>The AD-X2 controversy is just as relevant today as it was 70 years ago. Scientific research, business interests, and politics remain deeply entangled. If the public is to have faith in science, it must have faith in the integrity of scientists and the scientific method. I have no objection to science being challenged—that’s how science moves forward—but we have to make sure that neither profit nor politics is tipping the scales. </p><p><em><em>Part of a </em></em><a href="https://spectrum.ieee.org/collections/past-forward/" target="_self"><u><em><em>continuing series</em></em></u></a><em> </em><em><em>looking at historical artifacts that embrace the boundless potential of technology.</em></em></p><p><em><em>An abridged version of this article appears in the August 2024 print issue as “The AD-X2 Affair.”</em></em></p><h3>References</h3><br/><p>I first heard about AD-X2 after my <em>IEEE Spectrum</em> editor sent me a notice about NIST’s short docudrama <a href="https://www.youtube.com/watch?v=ve6b6wrGbdo" target="_blank"><u><em>The AD-X2 Controversy</em></u></a>, which you can, and should, stream online. NIST held a <a href="https://cdnapisec.kaltura.com/index.php/extwidget/preview/partner_id/684682/uiconf_id/31013851/entry_id/1_dppb3wa3/embed/dynamic" target="_blank"><u>colloquium</u></a> on 31 July 2018 with John Astin and his brother Alexander (Sandy), where they recalled what it was like to be college students when their father’s reputation was on the line. The agency has also compiled a wonderful <a href="https://www.nist.gov/ad-x2/resources" target="_blank"><u>list of resources</u></a>, including many of the primary source government documents.</p><p>The AD-X2 controversy played out in the popular media, and I read dozens of articles following the almost daily twists and turns in the case in the <em>New York Times, Washington Post, </em>and <em>Science.</em></p><p>I found Elio Passaglia’s <a href="https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication925.pdf" target="_blank"><u><em>A Unique Institution: The National Bureau of Standards 1950-1969</em></u></a><em> </em>to be particularly helpful. The AD-X2 controversy is covered in detail in Chapter 2: Testing Can Be Troublesome.</p><p>A number of graduate theses have been written about AD-X2. One I consulted was Samuel Lawrence’s 1958 thesis “<a href="https://aura.american.edu/articles/thesis/The_Battery_AD-X2_Controversy_A_Study_of_Federal_Regulation_of_Deceptive_Business_Practices/23836746" rel="noopener noreferrer" target="_blank"><u>The Battery AD-X2 Controversy: A Study of Federal Regulation of Deceptive Business Practices</u></a>.” Lawrence also published the 1962 book <em>The Battery Additive Controversy.</em></p><em><br/></em>]]></description><pubDate>Thu, 01 Aug 2024 14:00:03 +0000</pubDate><guid>https://spectrum.ieee.org/ad-x2-battery-additive</guid><category>Ad-x2 history</category><category>Battery additive</category><category>Lead-acid batteries</category><category>National bureau of standards</category><category>Nist</category><category>Past forward</category><category>Type:departments</category><dc:creator>Allison Marsh</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/photo-of-a-red-and-white-box-with-the-words-battery-ad-x2-and-a-photo-of-a-mans-face-on-it.jpg?id=52982022&amp;width=980"></media:content></item><item><title>Will This Flying Camera Finally Take Off?</title><link>https://spectrum.ieee.org/hoverair-x1</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-flat-white-hover-drone-over-a-person-s-outstretched-hand.jpg?id=53017438&width=1200&height=800&coordinates=0%2C88%2C0%2C88"/><br/><br/><p>Ten years. Two countries. Multiple redesigns. Some US $80 million invested. And, finally, <a href="https://zerozerorobotics.com/" target="_blank">Zero Zero Robotics</a> has a product it says is ready for consumers, not just robotics hobbyists—the <a href="https://hoverair.com/" target="_blank">HoverAir X1</a>. The company has sold several hundred thousand flying cameras since the HoverAir X1 started shipping last year. It hasn’t gotten the millions of units into consumer hands—or flying above them—that its founders would like to see, but it’s a start.<br/></p><p>“It’s been like a 10-year-long Ph.D. project,” says Zero Zero founder and CEO Meng Qiu Wang. “The thesis topic hasn’t changed. In 2014 I looked at my cell phone and thought that if I could throw away the parts I don’t need—like the screen—and add some sensors, I could build a tiny robot.”</p><p>I first spoke to Wang <a href="https://spectrum.ieee.org/camera-drone-could-be-a-robot-command-center" target="_self">in early 2016</a>, when Zero Zero came out of stealth with its version of a flying camera—at $600. Wang had been working on the project for two years. He started the project in Silicon Valley, where he and cofounder Tony Zhang were finishing up Ph.D.s in computer science at Stanford University. Then the two decamped for China, where development costs are far less.</p><p>Flying cameras were a hot topic at the time; startup <a href="https://spectrum.ieee.org/is-lily-a-drone-with-a-camera" target="_self">Lily Robotics</a> demonstrated a $500 flying camera in mid-2015 (and was later <a href="https://www.forbes.com/sites/aarontilley/2017/01/13/lawsuit-killed-lily-robotics-drones/" target="_blank">charged with fraud</a> for faking its demo video), and in March of 2016 drone-maker <a href="https://www.dji.com/" rel="noopener noreferrer" target="_blank">DJI</a> introduced a drone with autonomous flying and tracking capabilities that turned it into much the same type of flying camera that Wang envisioned, albeit at the high price of $1400.</p><p>Wang aimed to make his flying camera cheaper and easier to use than these competitors by relying on image processing for navigation—no altimeter, no GPS. In this approach, which has changed little since the first design, one camera looks at the ground and algorithms follow the camera’s motion to navigate. Another camera looks out ahead, using facial and body recognition to track a single subject.</p><p>The current version, at $349, does what Wang had envisioned, which is, he told me, “to turn the camera into a cameraman.” But, he points out, the hardware and software, and particularly the user interface, changed a lot. The size and weight have been cut in half; it’s just 125 grams. This version uses a different and more powerful chipset, and the controls are on board; while you can select modes from a smart phone app, you don’t have to.</p><p>I can verify that it is cute (about the size of a paperback book), lightweight, and extremely easy to use. I’ve never flown a standard drone without help or crashing but had no problem sending the HoverAir up to follow me down the street and then land on my hand.</p><p>It isn’t perfect. It can’t fly over water—the movement of the water confuses the algorithms that judge speed through video images of the ground. And it only tracks people; though many would like it to track their pets, Wang says animals behave erratically, diving into bushes or other places the camera can’t follow. Since the autonomous navigation algorithms rely on the person being filmed to avoid objects and simply follows that path, such dives tend to cause the drone to crash.</p><p>Since we last spoke eight years ago, Wang has been through the highs and lows of the startup rollercoaster, turning to contract engineering for a while to keep his company alive. He’s become philosophical about much of the experience.</p><p>Here’s what he had to say.</p><p><strong>We last spoke in 2016. Tell me how you’ve changed.</strong><br/></p><p><strong>Meng Qiu Wang: </strong>When I got out of Stanford in 2014 and started the company with Tony [Zhang], I was eager and hungry and hasty and I thought I was ready. But retrospectively, I wasn’t ready to start a company. I was chasing fame and money, and excitement.</p><p>Now I’m 42, I have a daughter—everything seems more meaningful now. I’m not a Buddhist, but I have a lot of Zen in my philosophy now.</p><p>I was trying so hard to flip the page to see the next chapter of my life, but now I realize, there is no next chapter, flipping the page itself is life.</p><p><strong>You were moving really fast in 2016 and 2017. What happened during that time?</strong></p><p><strong>Wang: </strong>After coming out of stealth, we ramped up from 60 to 140 people planning to take this product into mass production. We got a crazy amount of media attention—covered by 2,200 media outlets. We went to CES, and it seemed like we collected every trophy there was there.</p><p>And then Apple came to us, inviting us to retail at all the Apple stores. This was a big deal; I think we were the first third party robotic product to do live demos in Apple stores. We produced about 50,000 units, bringing in about $15 million in revenue in six months.</p><p>Then a giant company made us a generous offer and we took it. But it didn’t work out. It was a certainly lesson learned for us. I can’t say more about that, but at this point if I walk down the street and I see a box of pizza, I would not try to open it; there really is no free lunch.</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="a black caged drone with fans and a black box in the middle" class="rm-shortcode rm-resized-image" data-rm-shortcode-id="c1a02584face3321e3a97fc8d51fd54e" data-rm-shortcode-name="rebelmouse-image" id="c1998" loading="lazy" src="https://spectrum.ieee.org/media-library/a-black-caged-drone-with-fans-and-a-black-box-in-the-middle.jpg?id=53017581&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" data-gramm="false" data-lt-tmp-id="lt-525969" placeholder="Add Photo Caption..." spellcheck="false" style="max-width: 100%;">This early version of the Hover flying camera generated a lot of initial excitement, but never fully took off.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..." style="max-width: 100%;">Zero Zero Robotics</small></p><p><strong>How did you survive after that deal fell apart?</strong></p><p><strong>Wang:</strong> We went from 150 to about 50 people and turned to contract engineering. We worked with toy drone companies, with some industrial product companies. We built computer vision systems for larger drones. We did almost four years of contract work.</p><p><strong>But you kept working on flying cameras and launched a Kickstarter campaign in 2018. What happened to that product?</strong></p><p><strong>Wang:</strong> It didn’t go well. The technology wasn’t really there. We filled some orders and refunded ones that we couldn’t fill because we couldn’t get the remote controller to work.</p><p>We really didn’t have enough resources to create a new product for a new product category, a flying camera, to educate the market.</p><p>So we decided to build a more conventional drone—our V-Coptr, a V-shaped bi-copter with only two propellers—to compete against DJI. We didn’t know how hard it would be. We worked on it for four years. Key engineers left out of total dismay, they lost faith, they lost hope.</p><p>We came so close to going bankrupt so many times—at least six times in 10 years I thought I wasn’t going to be able to make payroll for the next month, but each time I got super lucky with something random happening. I never missed paying one dime—not because of my abilities, just because of luck.</p><p>We still have a relatively healthy chunk of the team, though. And this summer my first ever software engineer is coming back. The people are the biggest wealth that we’ve collected over the years. The people who are still with us are not here for money or for success. We just realized along the way that we enjoy working with each other on impossible problems.</p><p><strong>When we talked in 2016, you envisioned the flying camera as the first in a long line of personal robotics products. Is that still your goal?</strong></p><p><strong>Wang:</strong> In terms of short-term strategy, we are focusing 100 percent on the flying camera. I think about other things, but I’m not going to say I have an AI hardware company, though we do use AI. After 10 years I’ve given up on talking about that.</p><p><strong>Do you still think there’s a big market for a flying camera?</strong></p><p><strong>Wang:</strong> I think flying cameras have the potential to become the second home robot [the first being the robotic vacuum] that can enter tens of millions of homes.</p>]]></description><pubDate>Wed, 31 Jul 2024 12:00:02 +0000</pubDate><guid>https://spectrum.ieee.org/hoverair-x1</guid><category>Drones</category><category>Entrepreneur</category><category>Startups</category><category>Zero zero robotics</category><category>Flying camera</category><dc:creator>Tekla S. Perry</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-flat-white-hover-drone-over-a-person-s-outstretched-hand.jpg?id=53017438&amp;width=980"></media:content></item><item><title>Gladys West: The Hidden Figure Behind GPS</title><link>https://spectrum.ieee.org/gladys-west-hidden-figure-gps</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/black-and-white-portrait-of-a-woman-with-glasses-and-a-tied-bow-around-neck-smiling-for-the-camera.jpg?id=52981413&width=1200&height=800&coordinates=0%2C0%2C0%2C209"/><br/><br/><p>
	Schoolchildren around the world are told that they have the potential to be great, often with the cheery phrase: “The sky’s the limit!”
</p><p>
<a href="https://en.wikipedia.org/wiki/Gladys_West" rel="noopener noreferrer" target="_blank">Gladys West</a> took those words literally.
</p><p>
	While working for four decades as a mathematician and computer programmer at the U.S. Naval Proving Ground (now the <a href="https://www.navsea.navy.mil/Home/Warfare-Centers/NSWC-Dahlgren/" rel="noopener noreferrer" target="_blank">Naval Surface Warfare Center</a>) in Dahlgren, Va., she prepared the way for a satellite constellation in the sky that became an indispensable part of modern life: the Global Positioning System, or <a href="https://spectrum.ieee.org/gps" target="_self">GPS</a>.
</p><p>
	The second Black woman to ever work at the proving ground, West led a group of analysts who used satellite sensor data to calculate the shape of the Earth and the orbital routes around it. Her meticulous calculations and programming work established the flight paths now used by GPS satellites, setting the stage for navigation and positioning systems on which the world has come to rely.
</p><p>
	For decades, West’s contributions went unacknowledged. But she has begun receiving overdue recognition. In 2018 she was inducted into the U.S. <a href="https://www.af.mil/News/Article-Display/Article/1707659/mathematician-inducted-into-space-and-missiles-pioneers-hall-of-fame/" rel="noopener noreferrer" target="_blank">Air Force Space and Missile Pioneers Hall of Fame</a>. In 2021 the <a href="https://www.iadas.net/" rel="noopener noreferrer" target="_blank">International Academy of Digital Arts and Sciences</a> presented her its Webby Lifetime Achievement Award, while the U.K. <a href="https://raeng.org.uk/" rel="noopener noreferrer" target="_blank">Royal Academy of Engineering</a> gave her the <a href="https://raeng.org.uk/programmes-and-prizes/prizes/prince-philip-medal" rel="noopener noreferrer" target="_blank">Prince Philip Medal</a>, the organization’s highest individual honor.
</p><p>
	West was presented the 2024 <a href="https://corporate-awards.ieee.org/recipients/presidents-award/" rel="noopener noreferrer" target="_blank">IEEE President’s Award</a> for “mathematical modeling and development of satellite geodesy models that played a pivotal role in the development of the Global Positioning System.” The award is sponsored by <a href="https://www.ieee.org/" rel="noopener noreferrer" target="_blank">IEEE</a>.
</p><h2>How the “hidden figure” overcame barriers</h2><p>
	West’s path to becoming a technology professional and an IEEE honoree was an unlikely one. Born in 1930 in Sutherland, Va., she grew up working on her family’s farm. To supplement the family’s income, her mother worked at a tobacco factory and her father was employed by a railroad company.
</p><p>
	Physical toil in the hot sun from daybreak until sundown with paltry financial returns, West says, made her determined to do something other than farming.
</p><p>
	Every day when she ventured into the fields to sow or harvest crops with her family, her thoughts were on the little red schoolhouse beyond the edge of the farm. She recalls gladly making the nearly 5-kilometer trek from her house, through the woods and over streams, to reach the one-room school.
</p><p>
	She knew that postsecondary education was her ticket out of farm life, so throughout her school years she made sure she was a standout student and a model of focus and perseverance.
</p><p>
	Her parents couldn’t afford to pay for her college education, but as valedictorian of her high school class, she earned a full-tuition scholarship from the state of Virginia. Money she earned as a babysitter paid for her room and board.
</p><p>
	West decided to pursue a degree in mathematics at Virginia State College (now <a href="https://www.vsu.edu/" rel="noopener noreferrer" target="_blank">Virginia State University</a>), a historically Black school in Petersburg.
</p><p>
	At the time, the field was dominated by men. She earned a bachelor’s degree in the subject in 1952 and became a schoolteacher in Waverly, Va. After two years in the classroom, she returned to Virginia State to pursue a master’s degree in mathematics, which she earned in 1955.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="black and white image of a woman sitting at a desk writing on a pad of paper" class="rm-shortcode" data-rm-shortcode-id="d4e12a7dc299a35c7dfbed0113137ad4" data-rm-shortcode-name="rebelmouse-image" id="eb771" loading="lazy" src="https://spectrum.ieee.org/media-library/black-and-white-image-of-a-woman-sitting-at-a-desk-writing-on-a-pad-of-paper.jpg?id=52981423&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Gladys West at her desk, meticulously crunching numbers manually in the era before computers took over such tasks.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Gladys West</small></p><h2>Setting the groundwork for GPS</h2><p>
	West began her career at the Naval Proving Ground in early 1956. She was hired as a mathematician, joining a cadre of workers who used linear algebra, calculus, and other methods to manually solve complex problems such as differential equations. Their mathematical wizardry was used to handle trajectory analysis for ships and aircraft as well as other applications.
</p><p>
	 She was one of four Black employees at the facility, she says, adding that her determination to prove the capability of Black professionals drove her to excel.
</p><p>
	 As computers were introduced into the Navy’s operations in the 1960s, West became proficient in <a href="https://en.wikipedia.org/wiki/Fortran" rel="noopener noreferrer" target="_blank">Fortran IV</a>. The programming language enabled her to use the <a href="https://www.ibm.com/history/stretch" rel="noopener noreferrer" target="_blank">IBM 7030</a>—the world’s fastest supercomputer at the time—to process data at an unprecedented rate.
</p><p>
	Because of her expertise in mathematics and computer science, she was appointed director of projects that extracted valuable insights from satellite data gathered during NASA missions. West and her colleagues used the data to create ever more accurate models of the <a href="https://oceanservice.noaa.gov/facts/geoid.html" rel="noopener noreferrer" target="_blank">geoid</a>—the shape of the Earth—factoring in gravitational fields and the planet’s rotation.
</p><p>
	One such mission was <a href="https://www.jpl.nasa.gov/missions/seasat" rel="noopener noreferrer" target="_blank">Seasat</a>, which lasted from June to October 1978. Seasat was launched into orbit to test oceanographic sensors and gain a better understanding of Earth’s seas using the first space-based <a href="https://www.earthdata.nasa.gov/learn/backgrounders/what-is-sar" rel="noopener noreferrer" target="_blank">synthetic aperture radar</a> (SAR) system, which enabled the first remote sensing of the Earth’s oceans.
</p><p>
	SAR can acquire high-resolution images at night and can penetrate through clouds and rain. Seasat captured many valuable 2D and 3D images before a malfunction caused the satellite to be taken down.
</p><p>
	Enough data was collected from Seasat for West’s team to refine existing geodetic models to better account for gravity and magnetic forces. The models were important for precisely mapping the Earth’s topography, determining the orbital routes that would later be used by GPS satellites, as well as documenting the spatial relationships that now let GPS determine exactly where a receiver is.
</p><p>
	In 1986 she published the “<a href="https://upload.wikimedia.org/wikipedia/commons/2/2e/Data_processing_for_GeoSat.pdf" rel="noopener noreferrer" target="_blank">Data Processing System Specifications for the GEOSAT Satellite Radar Altimeter</a>” technical report. It contained new calculations that could make her geodetic models more accurate. The calculations were made possible by data from the radio altimeter on the <a href="https://science.nasa.gov/mission/geosat/" rel="noopener noreferrer" target="_blank">GEOSAT</a>, a Navy satellite that went into orbit in March 1985.
</p><p>
	West’s career at Dahlgren lasted 42 years. By the time she retired in 1998, all 24 satellites in the GPS constellation had been launched to help the world keep time and handle navigation. But her role was largely unknown.
</p><h2> A model of perseverance</h2><p>
	Neither an early bout of <a href="https://www.ncbi.nlm.nih.gov/books/NBK585058/#:~:text=Imposter%20syndrome%20(IS)%20is%20a,accomplishments%20among%20high%2Dachieving%20individuals." rel="noopener noreferrer" target="_blank">imposter syndrome</a> nor the racial tensions that were an everyday element of her work life during the height of the <a href="https://www.history.com/topics/black-history/civil-rights-movement" rel="noopener noreferrer" target="_blank">Civil Rights Movement</a> were able to knock her off course, West says.
</p><p>
	In the early 1970s, she decided that her career advancement was not proceeding as smoothly as she thought it should, so she decided to go to graduate school part time for another degree. She considered pursuing a doctorate in mathematics but realized, “I already had all the technical credentials I would ever need for my work for the Navy.” Instead, to solidify her skills as a manager, she earned a master’s degree in 1973 in public administration from the <a href="https://www.ou.edu/" rel="noopener noreferrer" target="_blank">University of Oklahoma</a> in Norman.
</p><p>
	After retiring from the Navy, she earned a doctorate in public administration in 2000 from <a href="https://www.vt.edu/" rel="noopener noreferrer" target="_blank">Virginia Tech</a>. Although she was recovering from a stroke at the time that affected her physical abilities, she still had the same drive to pursue an education that had once kept her focused on a little red schoolhouse.
</p><h2>A formidable legacy</h2><p>
	West’s contributions have had a lasting impact on the fields of mathematics, geodesy, and computer science. Her pioneering efforts in a predominantly male and racially segregated environment set a precedent for future generations of female and minority scientists.
</p><p>
	West says her life and career are testaments to the power of perseverance, skill, and dedication—or “stick-to-it-iveness,” to use her parlance. Her story continues to inspire people who strive to push boundaries. She has shown that the sky is indeed not the limit but just the beginning.
</p>]]></description><pubDate>Tue, 30 Jul 2024 18:00:03 +0000</pubDate><guid>https://spectrum.ieee.org/gladys-west-hidden-figure-gps</guid><category>Careers</category><category>Gps</category><category>Hidden figures</category><category>Ieee awards</category><category>Ieee member news</category><category>Type:ti</category><dc:creator>Willie D. Jones</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/black-and-white-portrait-of-a-woman-with-glasses-and-a-tied-bow-around-neck-smiling-for-the-camera.jpg?id=52981413&amp;width=980"></media:content></item><item><title>The Doyen of the Valley Bids Adieu</title><link>https://spectrum.ieee.org/tekla-perry-ieee-spectrum</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/photo-of-a-woman-in-glasses-holding-an-award.jpg?id=52968923&width=1200&height=800&coordinates=0%2C0%2C0%2C594"/><br/><br/><p>When Senior Editor Tekla S. Perry started in this magazine’s New York office in 1979, she was issued the standard tools of the trade: notebooks, purple-colored pencils for making edits and corrections on page proofs, a push-button telephone wired into a WATS line for unlimited long distance calling, and an IBM Selectric typewriter, “the latest and greatest technology, from my perspective,” she recalled recently.</p><p>And she put that typewriter through its paces. “In this period she was doing deep and outstanding reporting on major Silicon Valley startups, outposts, and institutions, most notably Xerox PARC,” says Editorial Director for Content Development Glenn Zorpette, who began his career at <em><em>IEEE Spectrum</em></em> five years later. “She did some of this reporting and writing with Paul Wallich, another staffer in the 1980s. Together they produced stories that hold up to this day as invaluable records of a pivotal moment in Silicon Valley history.” </p><p>Indeed, the October 1985 feature story about <a href="https://spectrum.ieee.org/playing-woodward-and-bernstein-at-xerox-parc" target="_self"><u>Xerox PARC</u></a>, which she cowrote with Wallich in 1985, ranks as Perry’s favorite article.</p><p>“While now it’s widely known that PARC invented history-making technology and blew its commercialization—there have been entire books written about that—Paul Wallich and I were the first to really dig into what had happened at PARC,” she says. “A few of the key researchers had left and were open to talking, and some people who were still there had hit the point of being frustrated enough to tell their stories. So we interviewed a huge number of them, virtually all in person and at length. Think about who we met! Alan Kay, Larry Tesler, Alvy Ray Smith, Bob Metcalfe, John Warnock and Chuck Geschke, Richard Shoup, Bert Sutherland, Charles Simonyi, Lynn Conway, and many others.”</p><p class="pull-quote">“I know without a doubt that my path and those of my younger women colleagues have been smoothed enormously by the very fact that Tekla came before us and showed us the way.” <strong>–Jean Kumagai</strong></p><p>After more than seven years of reporting trips to Silicon Valley, Perry relocated there permanently as <em><em>Spectrum</em></em>’s first “field editor.” </p><p>Over the course of more than four decades, Perry became known for her profiles of Valley visionaries and <a href="https://spectrum.ieee.org/engineer-profile" target="_self"><u>IEEE Medal of Honor </u></a>recipients, most recently <a href="https://spectrum.ieee.org/vint-cerf" target="_self"><u>Vint Cerf</u></a> and <a href="https://spectrum.ieee.org/bob-kahn-2667754905" target="_self"><u>Bob Kahn</u></a>. She established working relationships—and, in some cases, friendships—with some of the most important people in Northern California tech, including Kay and Smith, Steve Wozniak (Apple), Al Alcorn and Nolan Bushnell (Atari), Andy Grove (Intel), Judy Estrin (Bridge, Cisco, Packet Design), and John Hennessy (chairperson of Alphabet and former president of Stanford).</p><p>Just as her interview subjects were regarded as pioneers in their fields, Perry herself ranks as a pioneer for women tech journalists. As the first woman editor hired at <em><em>Spectrum</em></em> and one of a precious few women journalists reporting on technology at the time, she blazed a trail that others have followed, including several current <em><em>Spectrum</em></em> staff members. </p><p>“Tekla had already been at <em><em>Spectrum</em></em> for 20 years when I joined the staff,” Executive Editor Jean Kumagai told me. “I know without a doubt that my path and those of my younger women colleagues have been smoothed enormously by the very fact that Tekla came before us and showed us the way.”</p><p>Perry is retiring this month after 45 years of service to IEEE and its members. We’re sad to see her go and I know many readers are, too—from personal experience. I met an IEEE Life Member for breakfast a few weeks ago. I asked him, as an avid <em><em>Spectrum</em></em> reader since 1964, what he liked most about it. He began talking about Perry’s stories, and how she inspired him through the years. The connections forged between reader and writer are rare in this age of blurbage and spew, but the way Perry connected readers to their peers was, well, peerless. Just like Perry herself. </p><p><em>This article appears in the August 2024 print issue.</em></p>]]></description><pubDate>Tue, 30 Jul 2024 13:41:24 +0000</pubDate><guid>https://spectrum.ieee.org/tekla-perry-ieee-spectrum</guid><category>Silicon valley</category><category>Tech journalism</category><category>Women in technology</category><category>Tekla perry</category><dc:creator>Harry Goldstein</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/photo-of-a-woman-in-glasses-holding-an-award.jpg?id=52968923&amp;width=980"></media:content></item><item><title>A Robot Dentist Might Be a Good Idea, Actually</title><link>https://spectrum.ieee.org/robot-dentist</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-man-wearing-red-googles-and-a-blue-dental-bib-lies-prone-with-metallic-arms-near-his-face.jpg?id=53005021&width=1200&height=800&coordinates=0%2C1%2C0%2C2"/><br/><br/><p>
	I’ll be honest: when I first got this pitch for an autonomous robot dentist, I was like: “Okay, I’m going to talk to these folks and then write an article, because there’s no possible way for this thing to be anything but horrific.” Then they sent me some video that was, in fact, horrific, in the way that only watching a high speed drill remove most of a tooth can be.
</p><p>
	But fundamentally this has very little to do with robotics, because getting your teeth drilled just sucks no matter what. So the real question we should be asking is this: How can we make a dental procedure as quick and safe as possible, to minimize that inherent horrific-ness?And the answer, surprisingly, may be this robot from a startup called <a href="https://www.perceptive.io/" target="_blank">Perceptive</a>.
</p><p>
	Perceptive is today announcing two new technologies that I very much hope will make future dental experiences better for everyone. While it’s easy to focus on the robot here (because, well, it’s a robot), the reason the robot can do what it does (which we’ll get to in a minute) is because of a new imaging system. The handheld imager, which is designed to operate inside of your mouth, uses <a href="https://spectrum.ieee.org/tag/optical-coherence-tomography" target="_blank">optical coherence tomography (OCT)</a> to generate a 3D image of the inside of your teeth, and even all the way down below the gum line and into the bone. This is vastly better than the 2D or 3D x-rays that dentists typically use, both in resolution and positional accuracy. <br/>
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A hand in a blue medical glove holds a black wand-like device with a circuit board visible." class="rm-shortcode" data-rm-shortcode-id="a2b0652f97ea75afbf5e51d020eeb0e4" data-rm-shortcode-name="rebelmouse-image" id="03b3b" loading="lazy" src="https://spectrum.ieee.org/media-library/a-hand-in-a-blue-medical-glove-holds-a-black-wand-like-device-with-a-circuit-board-visible.jpg?id=53005092&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Perceptive’s handheld optical coherence tomography imager scans for tooth decay.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Perceptive</small>
</p><p>
	X-Rays, it turns out, are actually really bad at detecting cavities; <a href="https://www.linkedin.com/in/christopher-ciriello/" target="_blank">Perceptive CEO Chris Ciriello</a> tells us that the accuracy is on the order of 30 percent of figuring out the location and extent of tooth decay. In practice, this isn’t as much of a problem as it seems like it should be, because the dentist will just start drilling into your tooth and keep going until they find everything. But obviously this won’t work for a robot, where you need all of the data beforehand. That’s where the OCT comes in. You can think of OCT as similar to an ultrasound, in that it uses reflected energy to build up an image, but OCT uses light instead of sound for much higher resolution.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A short video shows outlines of teeth in progressively less detail, but highlights some portions in blood red." class="rm-shortcode" data-rm-shortcode-id="da1433d70ca2d712acc7995555f1dd1b" data-rm-shortcode-name="rebelmouse-image" id="6914d" loading="lazy" src="https://spectrum.ieee.org/media-library/a-short-video-shows-outlines-of-teeth-in-progressively-less-detail-but-highlights-some-portions-in-blood-red.gif?id=53005412&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Perceptive’s imager can create detailed 3D maps of the insides of teeth.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Perceptive</small>
</p><p>
	The reason OCT has not been used for teeth before is because with conventional OCT, the exposure time required to get a detailed image is several seconds, and if you move during the exposure, the image will blur. Perceptive is instead using a structure from motion approach (which will be familiar to many robotics folks), where they’re relying on a much shorter exposure time resulting in far fewer data points, but then moving the scanner and collecting more data to gradually build up a complete 3D image. According to Ciriello, this approach can localize pathology within about 20 micrometers with over 90 percent accuracy, and it’s easy for a dentist to do since they just have to move the tool around your tooth in different orientations until the scan completes.
</p><p>
	Again, this is not just about collecting data so that a robot can get to work on your tooth. It’s about better imaging technology that helps your dentist identify and treat issues you might be having. “We think this is a fundamental step change,” Ciriello says. “We’re giving dentists the tools to find problems better.”
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A silvery robotic arm with a small drill at the end." class="rm-shortcode" data-rm-shortcode-id="1e80d2a540270031fcdbd2c431c21855" data-rm-shortcode-name="rebelmouse-image" id="a2c26" loading="lazy" src="https://spectrum.ieee.org/media-library/a-silvery-robotic-arm-with-a-small-drill-at-the-end.jpg?id=53005188&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">The robot is mechanically coupled to your mouth for movement compensation.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Perceptive</small>
</p><p>
	Ciriello was a practicing dentist in a small mountain town in British Columbia, Canada. People in such communities can have a difficult time getting access to care. “There aren’t too many dentists who want to work in rural communities,” he says. “Sometimes it can take months to get treatment, and if you’re in pain, that’s really not good. I realized that what I had to do was build a piece of technology that could increase the productivity of dentists.”
</p><p>
	Perceptive’s robot is designed to take a dental procedure that typically requires several hours and multiple visits, and complete it in minutes in a single visit. The entry point for the robot is crown installation, where the top part of a tooth is replaced with an artificial cap (the crown). This is an incredibly common procedure, and it usually happens in two phases. First, the dentist will remove the top of the tooth with a drill. Next, they take a mold of the tooth so that a crown can be custom fit to it. Then they put a temporary crown on and send you home while they mail the mold off to get your crown made. A couple weeks later, the permanent crown arrives, you go back to the dentist, and they remove the temporary one and cement the permanent one on.
</p><p>
	With Perceptive’s system, it instead goes like this: on a previous visit where the dentist has identified that you need a crown in the first place, you’d have gotten a scan of your tooth with the OCT imager. Based on that data, the robot will have planned a drilling path, and then the crown could be made before you even arrive for the drilling to start, which is only possible because the precise geometry is known in advance. You arrive for the procedure, the robot does the actually drilling in maybe five minutes or so, and the perfectly fitting permanent crown is cemented into place and you’re done.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A silvery robotic arm with a small drill at the end. The arm is mounted on a metal cart with a display screen." class="rm-shortcode" data-rm-shortcode-id="376deafa0e9edb20f89a26deb27a5c93" data-rm-shortcode-name="rebelmouse-image" id="93fc5" loading="lazy" src="https://spectrum.ieee.org/media-library/a-silvery-robotic-arm-with-a-small-drill-at-the-end-the-arm-is-mounted-on-a-metal-cart-with-a-display-screen.jpg?id=53005257&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">The robot is still in the prototype phase but could be available within a few years.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Perceptive</small>
</p><p>
	Obviously, safety is a huge concern here, because you’ve got a robot arm with a high-speed drill literally working inside of your skull. Perceptive is well aware of this.
</p><p>
	The most important thing to understand about the Perceptive robot is that it’s physically attached to you as it works. You put something called a bite block in your mouth and bite down on it, which both keeps your mouth open and keeps your jaw from getting tired. The robot’s end effector is physically attached to that block through a series of actuated linkages, such that any motions of your head are instantaneously replicated by the end of the drill, even if the drill is moving. Essentially, your skull is serving as the robot’s base, and your tooth and the drill are in the same reference frame. Purely mechanical coupling means there’s no vision system or encoders or software required: it’s a direct physical connection so that motion compensation is instantaneous. As a patient, you’re free to relax and move your head somewhat during the procedure, because it makes no difference to the robot.
</p><p>
	Human dentists do have some strategies for not stabbing you with a drill if you move during a procedure, like putting their fingers on your teeth and then supporting the drill on them. But this robot should be safer and more accurate than that method, because of the rigid connection leading to only a few tens of micrometers of error, even on a moving patient. It’ll move a little bit slower than a dentist would, but because it’s only drilling exactly where it needs to, it can complete the procedure faster overall, says Ciriello.
</p><p>
	There’s also a physical counterbalance system within the arm, a nice touch that makes the arm effectively weightless. (It’s somewhat similar to the PR2 arm, for you OG robotics folks.) And the final safety measure is the dentist-in-the-loop via a foot pedal that must remain pressed or the robot will stop moving and turn off the drill.
</p><p>
	Ciriello claims that not only is the robot able to work faster, it also will produce better results. Most restorations like fillings or crowns last about five years, because the dentist either removed too much material from the tooth and weakened it, or removed too little material and didn’t completely solve the underlying problem. Perceptive’s robot is able to be far more exact. Ciriello says that the robot can cut geometry that’s “not humanly possible,” fitting restorations on to teeth with the precision of custom-machined parts, which is pretty much exactly what they are.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A short video shows a d dental drill working on a tooth in a person's mouth." class="rm-shortcode" data-rm-shortcode-id="ec4300dcd0d94303904bb501e1cb58e9" data-rm-shortcode-name="rebelmouse-image" id="4edba" loading="lazy" src="https://spectrum.ieee.org/media-library/a-short-video-shows-a-d-dental-drill-working-on-a-tooth-in-a-person-s-mouth.gif?id=53005587&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Perceptive has successfully used its robot on real human patients, as shown in this sped-up footage. In reality the robot moves slightly slower than a human dentist.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Perceptive</small>
</p><p>
	While it’s easy to focus on the technical advantages of Perceptive’s system, dentist <a href="https://www.linkedin.com/in/painlessdrz/" rel="noopener noreferrer" target="_blank"><u>Ed Zuckerberg</u></a> (who’s an investor in Perceptive) points out that it’s not just about speed or accuracy, it’s also about making patients feel better. “Patients think about the precision of the robot, versus the human nature of their dentist,” Zuckerberg says. It gives them confidence to see that their dentist is using technology in their work, especially in ways that can address common phobias. “If it can enhance the patient experience or make the experience more comfortable for phobic patients, that automatically checks the box for me.”
</p><p>
	There is currently one other dental robot on the market. Called <a href="https://www.neocis.com/" rel="noopener noreferrer" target="_blank"><u>Yomi</u></a>, it offers assistive autonomy for one very specific procedure for dental implants. Yomi is not autonomous, but instead provides guidance for a dentist to make sure that they drill to the correct depth and angle.
</p><p>
	While Perceptive has successfully tested their first-generation system on humans, it’s not yet ready for commercialization. The next step will likely be what’s called a pivotal clinical trial with the FDA, and if that goes well, Cirello estimates that it could be available to the public in “several years”. Perceptive has raised US $30 million in funding so far, and here’s hoping that’s enough to get them across the finish line.
</p>]]></description><pubDate>Tue, 30 Jul 2024 12:00:02 +0000</pubDate><guid>https://spectrum.ieee.org/robot-dentist</guid><category>Medical robots</category><category>Optical coherence tomography</category><category>Robot dentist</category><category>Robotics</category><dc:creator>Evan Ackerman</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-man-wearing-red-googles-and-a-blue-dental-bib-lies-prone-with-metallic-arms-near-his-face.jpg?id=53005021&amp;width=980"></media:content></item><item><title>Your Gateway to a Vibrant Career in the Expanding Semiconductor Industry</title><link>https://spectrum.ieee.org/purdue-online-msmesc</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/two-individuals-closely-examine-a-piece-of-chip-packaging-imaging-equipment-in-a-laboratory-with-one-adjusting-a-component-whil.jpg?id=52972141&width=1200&height=800&coordinates=0%2C0%2C0%2C0"/><br/><br/><p>
<em>This sponsored article is brought to you by <a href="https://engineering.purdue.edu/online/programs/masters-degrees/semiconductors?utm_source=association&utm_medium=ieee_spectrum&utm_campaign=IDE-MSE-OL_IDE_SC" target="_blank">Purdue University</a>.</em>
</p><p>
	The CHIPS America Act was a response to a worsening shortfall in engineers equipped to meet the growing demand for advanced electronic devices. That need persists. In its 2023 policy report,
	<em><a href="https://www.semiconductors.org/chipping-away-assessing-and-addressing-the-labor-market-gap-facing-the-u-s-semiconductor-industry/" target="_blank">Chipping Away</a></em><em>: Assessing and Addressing the Labor Market Gap Facing the U.S. Semiconductor Industry</em>, the Semiconductor Industry Association forecast a demand for 69,000 microelectronic and semiconductor engineers between 2023 and 2030—including 28,900 new positions created by industry expansion and 40,100 openings to replace engineers who retire or leave the field.
</p><p>
	This number does
	<em>not</em> include another 34,500 computer scientists (13,200 new jobs, 21,300 replacements), nor does it count jobs in other industries that require advanced or custom-designed semiconductors for controls, automation, communication, product design, and the emerging systems-of-systems technology ecosystem.
</p><p>
	Purdue University is taking charge, leading semiconductor technology and workforce development in the U.S. As early as Spring 2022, Purdue University became the first top engineering school to offer an online
	<a href="https://engineering.purdue.edu/online/programs/masters-degrees/semiconductors?utm_source=association&utm_medium=ieee_spectrum&utm_campaign=IDE-MSE-OL_IDE_SC" rel="noopener noreferrer" target="_blank">Master’s Degree in Microelectronics and Semiconductors</a>.
</p><p class="pull-quote">
	U.S. News & World Report has ranked the university’s graduate engineering program among America’s 10 best every year since 2012 (and among the top 4 since 2022)
</p><p>
	“The degree was developed as part of Purdue’s overall semiconductor degrees program,” says Purdue <a href="https://www.cerias.purdue.edu/site/people/faculty/view/1036" target="_blank">Prof. Vijay Raghunathan</a>, one of the architects of the semiconductor program. “It was what I would describe as the nation’s most ambitious semiconductor workforce development effort.”
</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="A person dressed in a dark suit with a white shirt and red tie poses for a professional portrait against a dark background." class="rm-shortcode rm-resized-image" data-rm-shortcode-id="dc10df37b7b80d41e4d7b9dfae4c0f11" data-rm-shortcode-name="rebelmouse-image" id="616b6" loading="lazy" src="https://spectrum.ieee.org/media-library/a-person-dressed-in-a-dark-suit-with-a-white-shirt-and-red-tie-poses-for-a-professional-portrait-against-a-dark-background.jpg?id=52974150&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Prof. Vijay Raghunathan, one of the architects of the online Master’s Degree in Microelectronics and Semiconductors at Purdue.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Purdue University</small>
</p><p>
	Purdue built and announced its bold high-technology online program while the U.S. Congress was still debating the $53 billion “Creating Helpful Incentives to Produce Semiconductors for America Act” (CHIPS America Act), which would be passed in July 2022 and signed into law in August.
</p><p>
	Today, the online Master’s in Microelectronics and Semiconductors is well underway. Students learn leading-edge equipment and software and prepare to meet the challenges they will face in a rejuvenated, and critical, U.S. semiconductor industry.
</p><p>
	Is the drive for semiconductor education succeeding?
</p><p>
	“I think we have conclusively established that the answer is a resounding ‘Yes,’” says Raghunathan. Like understanding big data, or being able to program, “the ability to understand how semiconductors and semiconductor-based systems work, even at a rudimentary level, is something that everybody should know. Virtually any product you design or make is going to have chips inside it. You need to understand how they work, what the significance is, and what the risks are.”
</p><h2>Earning a Master’s in Microelectronics and Semiconductors</h2><p>
	Students pursuing the Master’s Degree in Microelectronics and Semiconductors will take courses in circuit design, devices and engineering, systems design, and supply chain management offered by several schools in the university, such as Purdue’s Mitch Daniels School of Business, the Purdue Polytechnic Institute, the Elmore Family School of Electrical and Computer Engineering, and the School of Materials Engineering, among others.
</p><p>
	Professionals can also take one-credit-hour courses, which are intended to help students build “breadth at the edges,” a notion that grew out of feedback from employers: Tomorrow’s engineering leaders will need broad knowledge to connect with other specialties in the increasingly interdisciplinary world of artificial intelligence, robotics, and the Internet of Things.
</p><p>
	“This was something that we embarked on as an experiment 5 or 6 years ago,” says Raghunathan of the one-credit courses. “I think, in hindsight, that it’s turned out spectacularly.”
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A researcher wearing a white lab coat, hairnet, and gloves works with scientific equipment, with a computer monitor displaying a detailed scientific pattern." class="rm-shortcode" data-rm-shortcode-id="0995cfa63ccc8e934d59d60eb57c4a68" data-rm-shortcode-name="rebelmouse-image" id="6ffac" loading="lazy" src="https://spectrum.ieee.org/media-library/a-researcher-wearing-a-white-lab-coat-hairnet-and-gloves-works-with-scientific-equipment-with-a-computer-monitor-displaying-a.jpg?id=52974157&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">A researcher adjusts imaging equipment in a lab in Birck Nanotechnology Center, home to Purdue’s advanced research and development on semiconductors and other technology at the atomic scale.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Rebecca Robiños/Purdue University</small>
</p><h2>The Semiconductor Engineering Education Leader</h2><p>
	Purdue, which opened its first classes in 1874, is today an acknowledged leader in engineering education.
	<em>U.S. News & World Report</em> has <a href="https://www.usnews.com/education/online-education/purdue-university-main-campus-OENG0108/engineering" target="_blank">ranked</a> the university’s graduate engineering program among America’s 10 best every year since 2012 (and among the top 4 since 2022). And Purdue’s online graduate engineering program has ranked in the country’s top three since the publication started evaluating online grad programs in 2020. (Purdue has offered distance Master’s degrees since the 1980s. Back then, of course, course lectures were videotaped and mailed to students. With the growth of the web, “distance” became “online,” and the program has swelled.)
</p><p>
	Thus, Microelectronics and Semiconductors Master’s Degree candidates can study online or on-campus. Both tracks take the same courses from the same instructors and earn the same degree. There are no footnotes, asterisks, or parentheses on the diploma to denote online or in-person study.
</p><p class="pull-quote">
	“If you look at our program, it will become clear why Purdue is increasingly considered America’s leading semiconductors university” <strong>—Prof. Vijay Raghunathan, Purdue University</strong>
</p><p>
	Students take classes at their own pace, using an integrated suite of proven online-learning applications for attending lectures, submitting homework, taking tests, and communicating with faculty and one another. Texts may be purchased or downloaded from the school library. And there is frequent use of modeling and analytical tools like Matlab. In addition, Purdue is also the home of national the national design-computing resources
	<a href="https://nanohub.org/" target="_blank">nanoHUB.org</a> (with hundreds of modeling, simulation, teaching, and software-development tools) and its offspring, <a href="https://chipshub.org/" target="_blank">chipshub.org</a> (specializing in tools for chip design and fabrication).
</p><h2>From R&D to Workforce and Economic Development</h2><p>
	“If you look at our program, it will become clear why Purdue is increasingly considered America’s leading semiconductors university, because this is such a strategic priority
	for the entire university, from our President all the way down,” Prof. Raghunathan sums up. “We have a task force that reports directly to the President, a task force focused only on semiconductors and microelectronics. On all aspects—R&D, the innovation pipeline, workforce development, economic development to bring companies to the state. We’re all in as far as chips are concerned.”
</p>]]></description><pubDate>Tue, 30 Jul 2024 10:00:02 +0000</pubDate><guid>https://spectrum.ieee.org/purdue-online-msmesc</guid><category>Professional development</category><category>Purdue university</category><category>Type:sponsored</category><category>Microelectronics</category><category>Semiconductor industry</category><category>Semiconductors</category><dc:creator>Douglas McCormick</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/two-individuals-closely-examine-a-piece-of-chip-packaging-imaging-equipment-in-a-laboratory-with-one-adjusting-a-component-whil.jpg?id=52972141&amp;width=980"></media:content></item><item><title>Build a Radar Cat Detector</title><link>https://spectrum.ieee.org/feral-cat-radar-detector</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-see-through-illustration-shows-a-cat-relaxing-inside-a-windowless-shelter-a-box-with-two-lights-is-pressed-against-the-side-o.jpg?id=52966652&width=1200&height=800&coordinates=0%2C172%2C0%2C172"/><br/><br/><p>
	You have a closed box. There may be a live cat inside, but you won’t know until you open the box. For most people, this situation is a <a href="https://en.wikipedia.org/wiki/Schr%C3%B6dinger%27s_cat" rel="noopener noreferrer" target="_blank">theoretical conundrum</a> that probes the foundations of quantum mechanics. For me, however, it’s a pressing practical problem, not least because physics completely skates over the vital issue of how annoyed the cat will be when the box is opened. But fortunately, engineering comes to the rescue, in the form of a new US $50 maker-friendly <a href="https://www.sparkfun.com/products/24540" rel="noopener noreferrer" target="_blank">pulsed coherent radar sensor from SparkFun</a>.
</p><p>
	Perhaps I should back up a little bit. Working from home during the pandemic, my wife and I discovered a colony of feral cats living in the backyards of our block in New York City. We reversed the colony’s growth by doing <a href="https://www.nyc.gov/site/doh/health/health-topics/trap-neuter-return-of-feral-cats-in-nyc.page" rel="noopener noreferrer" target="_blank">trap-neuter-return</a> (TNR) on as many of its members as we could, and we purchased three <a href="https://feralvilla-com.3dcartstores.com/Feralvilla-Outdoor-Cat-Shelter-II_p_14.html" rel="noopener noreferrer" target="_blank">Feralvilla outdoor shelters</a> to see our furry neighbors through the harsh New York winters. These roughly cube-shaped insulated shelters allow the cats to enter via an opening in a raised floor. A removable lid on top allows us to replace straw bedding every few months. It’s impossible to see inside the shelter without removing the lid, meaning you run the risk of surprising a clawed predator that, just moments before, had been enjoying a quiet snooze.
</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="A set of components, including an enclosure with two large holes for LEDs and what looks like cat ears on top. " class="rm-shortcode rm-resized-image" data-rm-shortcode-id="a0f10fbef28532c9f24b6cd51d97c25b" data-rm-shortcode-name="rebelmouse-image" id="50bee" loading="lazy" src="https://spectrum.ieee.org/media-library/a-set-of-components-including-an-enclosure-with-two-large-holes-for-leds-and-what-looks-like-cat-ears-on-top.jpg?id=52966715&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..." style="max-width: 100%;">The enclosure for the radar [left column] is made of basswood (adding cat ears on top is optional). A microcontroller [top row, middle column] processes the results from the radar module [top row, right column] and illuminates the LEDs [right column, second from top] accordingly. A battery and on/off switch [bottom row, left to right] make up the power supply.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..." style="max-width: 100%;">James Provost</small>
</p><p>
	 Feral cats respond to humans differently than socialized pet cats do. They see us as threats rather than bumbling servants. Even after years of daily feeding, most of the cats in our block’s colony will not let us approach closer than a meter or two, let alone suffer being touched. They have claws that have never seen a clipper. And they don’t like being surprised or feeling hemmed in. So I wanted a way to find out if a shelter was occupied <em><em>before</em></em> I popped open its lid for maintenance. And that’s where radar comes in.
</p><p>
	 SparkFun’s pulsed coherent radar module is based on <a href="https://www.acconeer.com/products/" rel="noopener noreferrer" target="_blank">Acconeer</a>’s low-cost<a href="https://developer.acconeer.com/download/a121-datasheet/" rel="noopener noreferrer" target="_blank"> A121 sensor</a>. Smaller than a fingernail, the sensor operates at 60 gigahertz, which means its signal can penetrate many common materials. As the signal passes through a material, some of it is reflected back to the sensor, allowing you to determine distances to multiple surfaces with millimeter-level precision. The radar can be put into a “presence detector” mode—intended to flag whether or not a human is present—in which it looks for changes in the distance of reflections to identify motion.
</p><p>
	 As soon as I saw the announcement for SparkFun’s module, the wheels began turning. If the radar could detect a human, why not a feline? Sure, I could have solved my is-there-a-cat-in-the-box problem with less sophisticated technology, by, say, putting a pressure sensor inside the shelter. But that would have required a permanent setup complete with weatherproofing, power, and some way of getting data out. Plus I’d have to perform three installations, one for each shelter. For information I needed only once every few months, that seemed a bit much. So I ordered the radar module, along with a $30 <a href="https://www.sparkfun.com/products/19177" rel="noopener noreferrer" target="_blank">IoT RedBoard microcontroller</a>. The RedBoard operates at the same 3.3 volts as the radar and can configure the module and parse its output.
</p><p class="pull-quote">
	If the radar could detect a human, why not a feline?
</p><p>
	 Connecting the radar to the RedBoard was a breeze, as they both have <a href="https://www.sparkfun.com/qwiic" rel="noopener noreferrer" target="_blank">Qwiic</a> 4-wire interfaces, which provides power along with an<a href="https://www.i2c-bus.org/" rel="noopener noreferrer" target="_blank"> I2C serial connection</a> to peripherals. SparkFun’s <a href="https://github.com/sparkfun/SparkFun_Qwiic_XM125_Arduino_Library" rel="noopener noreferrer" target="_blank">Arduino libraries</a> and example code let me quickly test the idea’s feasibility by connecting the microcontroller to a host computer via USB, and I could view the results from the radar via a serial monitor. Experiments with our indoor cats (two defections from the colony) showed that the motion of their breathing was enough to trigger the presence detector, even when they were sound asleep. Further testing showed the radar could penetrate the wooden walls of the shelters and the insulated lining.
</p><p>
	 The next step was to make the thing portable. I added a small <a href="https://www.sparkfun.com/products/18286" rel="noopener noreferrer" target="_blank">$11 lithium battery</a> and spliced an on/off switch into its power lead. I hooked up two gumdrop LEDs to the RedBoard’s input/output pins and modified SparkFun’s sample scripts to illuminate the LEDs based on the output of the presence detector: a green LED for “no cat” and red for “cat.” I built an enclosure out of basswood, mounted the circuit boards and battery, and cut a hole in the back as a window for the radar module. (Side note: Along with tending feral cats, another thing I tried during the pandemic was 3D-printing plastic enclosures for projects. But I discovered that cutting, drilling, and gluing wood was faster, sturdier, and much more forgiving when making one-offs or prototypes.)
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="An outgoing sine-wave pulse from the radar is depicted on top. A series of returning pulses of lower amplitudes and at different distances are depicted on the bottom." class="rm-shortcode" data-rm-shortcode-id="79c4d3217001ff0a0df2d8cd98197648" data-rm-shortcode-name="rebelmouse-image" id="d20ad" loading="lazy" src="https://spectrum.ieee.org/media-library/an-outgoing-sine-wave-pulse-from-the-radar-is-depicted-on-top-a-series-of-returning-pulses-of-lower-amplitudes-and-at-different.jpg?id=52966758&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">The radar sensor sends out 60-gigahertz pulses through the walls and lining of the shelter. As the radar penetrates the layers, some radiation is reflected back to the sensor, which it detects to determine distances. Some materials will reflect the pulse more strongly than others, depending on their electrical permittivity. </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">James Provost</small></p><p>
	 I also modified the scripts to adjust the range over which the presence detector scans. When I hold the detector against the wall of a shelter, it looks only at reflections coming from the space inside that wall and the opposite side, a distance of about 50 centimeters. As all the cats in the colony are adults, they take up enough of a shelter’s volume to intersect any such radar beam, as long as I don’t place the detector near a corner.
</p><p>I performed in-shelter tests of the portable detector with one of our indoor cats, bribed with treats to sit in the open box for several seconds at a time. The detector did successfully spot him whenever he was inside, although it is prone to false positives. I will be trying to reduce these errors by adjusting the <a href="https://docs.acconeer.com/en/latest/radar_data_and_control/a121/index.html" rel="noopener noreferrer" target="_blank">plethora of available configuration settings</a> for the radar. But in the meantime, false positives are much more desirable than false negatives: A “no cat” light means it’s definitely safe to open the shelter lid, and my nerves (and the cats’) are the better for it.</p>]]></description><pubDate>Mon, 29 Jul 2024 14:00:02 +0000</pubDate><guid>https://spectrum.ieee.org/feral-cat-radar-detector</guid><category>Radar</category><category>Type:departments</category><category>Sparkfun</category><category>Internet of cats</category><category>Arduino</category><dc:creator>Stephen Cass</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-see-through-illustration-shows-a-cat-relaxing-inside-a-windowless-shelter-a-box-with-two-lights-is-pressed-against-the-side-o.jpg?id=52966652&amp;width=980"></media:content></item><item><title>How LG and Samsung Are Making TV Screens Disappear</title><link>https://spectrum.ieee.org/transparent-tv</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/an-array-of-15-large-transparent-displays-showing-cascading-water-with-a-crowd-of-people-is-visible-through-the-screens.png?id=52961244&width=1200&height=800&coordinates=378%2C0%2C379%2C0"/><br/><br/><p>
<strong>A transparent television might </strong><span style="background-color: initial;">seem like magic, but both </span><a href="https://www.lg.com/us/business/digital-signage/oled-signage/transparent-oled-displays" target="_blank">LG</a> and <a href="https://www.samsung.com/us/tvs/micro-led/highlights/" target="_blank">Samsung</a> <span style="background-color: initial;">demonstrated</span> such displays this past January in Las Vegas at CES 2024. And those large transparent TVs, which attracted countless spectators peeking through video images dancing on their screens, <span style="background-color: initial;">were showstoppers.</span>
</p><p>
	Although they are indeed impressive, transparent TVs are not likely to appear—or disappear—in your living room any time soon. Samsung and LG have taken two very different approaches to achieve a similar end—LG is betting on OLED displays, while Samsung is pursuing microLED screens—and neither technology is quite ready for prime time. Understanding the hurdles that still need to be overcome, though, requires a deeper dive into each of these display technologies.
</p><h2>How does LG’s see-through OLED work?</h2><p>
	OLED stands for organic light-emitting diode, and that pretty much describes how it works. OLED materials are carbon-based compounds that emit light when energized with an electrical current. Different compounds produce different colors, which can be combined to create full-color images.
</p><p>
	To construct a display from these materials, manufacturers deposit them as thin films on some sort of substrate. The most common approach arranges red-, green-, and blue-emitting (RGB) materials in patterns to create a dense array of full-color pixels. A display with what is known as 4K resolution contains a matrix of 3,840 by 2,160 pixels—8.3 million pixels in all, formed from nearly 25 million red, green, and blue subpixels.
</p><h3></h3><br/><div class="rblad-ieee_in_content"></div><p>
	The timing and amount of electrical current sent to each subpixel determines how much light it emits. So by controlling these currents properly, you can create the desired image on the screen. To accomplish this, each subpixel must be electrically connected to two or more transistors, which act as switches. Traditional wires wouldn’t do for this, though: They’d block the light. You need to use transparent (or largely transparent) conductive traces.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="An image of an array of 15 transparent TVs, shot with a fish-eye lens and displaying white trees with pink and green swaths of color above them.    " class="rm-shortcode" data-rm-shortcode-id="70ea364ea7a9bda570506de1bee303be" data-rm-shortcode-name="rebelmouse-image" id="2fa5e" loading="lazy" src="https://spectrum.ieee.org/media-library/an-image-of-an-array-of-15-transparent-tvs-shot-with-a-fish-eye-lens-and-displaying-white-trees-with-pink-and-green-swaths-of-c.png?id=52961258&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">LG’s demonstration of transparent OLED displays at CES 2024 seemed almost magical.  </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Ethan Miller/Getty Images </small>
</p><p>
	A display has thousands of such traces arranged in a series of rows and columns to provide the necessary electrical connections to each subpixel. The transistor switches are also fabricated on the same substrate. That all adds up to a lot of materials that must be part of each display. And those materials must be carefully chosen for the OLED display to appear transparent.
</p><p>
	The conductive traces are the easy part. The display industry has long used indium tin oxide as a thin-film conductor. A typical layer of this material is only 135 nanometers thick but allows about 80 percent of the light impinging on it to pass through.
</p><p>
	The transistors are more of a problem, because the materials used to fabricate them are inherently opaque. The solution is to make the transistors as small as you can, so that they block the least amount of light. The amorphous silicon layer used for transistors in most LCD displays is inexpensive, but its low electron mobility means that transistors composed of this material can only be made so small. This silicon layer can be annealed with lasers to create low-temperature polysilicon, a crystallized form of silicon, which improves electron mobility, reducing the size of each transistor. But this process works only for small sheets of glass substrate.
</p><p>
	Faced with this challenge, designers of transparent OLED displays have turned to indium gallium zinc oxide (IGZO). This material has high enough electron mobility to allow for smaller transistors than is possible with amorphous silicon, meaning that IGZO transistors block less light.
</p><p>
	These tactics help solve the transparency problem, but OLEDs have some other challenges. For one, exposure to oxygen or water vapor destroys the light-emissive materials. So these displays need an encapsulating layer, something to cover their surfaces and edges. Because this layer creates a visible gap when two panels are placed edge to edge, you can’t tile a set of smaller displays to create a larger one. If you want a big OLED display, you need to fabricate a single large panel.
</p><p>
	The result of even the best engineering here is a “transparent” display that still blocks some light. You won’t mistake LG’s transparent TV for window glass: People and objects behind the screen appear noticeably darker than when viewed directly. According to one informed observer, the LG prototype appears to have 45 percent transparency.
</p><h2>How does Samsung’s magical MicroLED work?</h2><p>
	For its transparent displays, Samsung is using inorganic LEDs. These devices, which are very efficient at converting electricity into light, are commonplace today: in household lightbulbs, in automobile headlights and taillights, and in electronic gear, where they often show that the unit is turned on.
</p><p>
	In LED displays, each pixel contains three LEDs, one red, one green, and one blue. This works great for the giant digital displays used in highway billboards or in sports-stadium jumbotrons, whose images are meant to be viewed from a good distance. But up close, these LED pixel arrays are noticeable.
</p><p>
	TV displays, on the other hand, are meant to be viewed from modest distances and thus require far smaller LEDs than the chips used in, say, power-indicator lights. Two years ago, these “microLED” displays used chips that were just 30 by 50 micrometers. (A typical sheet of paper is 100 micrometers thick.) Today, such displays use chips less than half that size: 12 by 27 micrometers.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A wooden frame surrounds a transparent display featuring an advertisement for a Black Friday Sale and a large image of a smartwatch. " class="rm-shortcode" data-rm-shortcode-id="da32250abbffa436cc0bab507c25ab6f" data-rm-shortcode-name="rebelmouse-image" id="80878" loading="lazy" src="https://spectrum.ieee.org/media-library/a-wooden-frame-surrounds-a-transparent-display-featuring-an-advertisement-for-a-black-friday-sale-and-a-large-image-of-a-smartwa.jpg?id=52961422&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">While transparent displays are stunning, they might not be practical for home use as televisions. Expect to see them adopted first as signage in retail settings.  </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">AUO </small>
</p><p>
	These tiny LED chips block very little light, making the display more transparent. The Taiwanese display maker
	<a href="https://www.auo.com/en-global/products/index/Display_Panel_Products" target="_blank">AUO</a> recently demonstrated a microLED display with more than 60 percent transparency.
</p><p>
	Oxygen and moisture don’t affect microLEDs, so they don’t need to be encapsulated. This makes it possible to tile smaller panels to create a seamless larger display. And the silicon coating on such small panels can be annealed to create polysilicon, which performs better than IGZO, so the transistors can be even smaller and block less light.
</p><p>
	But the microLED approach has its own problems. Indeed, the technology is still in its infancy, with costing a great deal to manufacture and requiring some contortions to get uniform brightness and color across the entire display.
</p><p>
	For example, individual OLED materials emit a well-defined color, but that’s not the case for LEDs. Minute variations in the physical characteristics of an LED chip can alter the wavelength of light it emits by a measurable—and noticeable—amount. Manufacturers have typically addressed this challenge by using a binning process: They test thousands of chips and then group them into bins of similar wavelengths, discarding those that don’t fit the desired ranges. This explains in part why those large digital LED screens are so expensive: Many LEDs created for their construction must be discarded.
</p><p>
	But binning doesn’t really work when dealing with microLEDs. The tiny chips are difficult to test and are so expensive that costs would be astronomical if too many had to be rejected.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A person wearing a white shirt with red text and a name badge is placing his hand behind a transparent display screen. The screen shows an image of splashing liquid and fire." class="rm-shortcode" data-rm-shortcode-id="264e05220f2d2f38c1e88cf9305f66c3" data-rm-shortcode-name="rebelmouse-image" id="f741b" loading="lazy" src="https://spectrum.ieee.org/media-library/a-person-wearing-a-white-shirt-with-red-text-and-a-name-badge-is-placing-his-hand-behind-a-transparent-display-screen-the-scree.png?id=52961443&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Though you can see through today’s transparent displays, they do block a noticeable amount of light, making the background darker than when viewed directly. </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Tekla S. Perry </small>
</p><p>
	Instead, manufacturers test microLED displays for uniformity after they’re assembled, then calibrate them to adjust the current applied to each subpixel so that color and brightness are uniform across the display. This calibration process, which involves scanning an image on the panel and then reprogramming the control circuitry, can sometimes require thousands of iterations.
</p><p>
	Then there’s the problem of assembling the panels. Remember those 25 million microLED chips that make up a 4K display? Each must be positioned precisely, and each must be connected to the correct electrical contacts.
</p><p>
	The LED chips are initially fabricated on sapphire wafers, each of which contains chips of only one color. These chips must be transferred from the wafer to a carrier to hold them temporarily before applying them to the panel backplane. The Taiwanese microLED company
	<a href="https://www.playnitride.com/en/" target="_blank">PlayNitride</a> has developed a process for creating large tiles with chips spaced less than 2 micrometers apart. Its process for positioning these tiny chips has better than 99.9 percent yields. But even at a 99.9 percent yield, you can expect about 25,000 defective subpixels in a 4K display. They might be positioned incorrectly so that no electrical contact is made, or the wrong color chip is placed in the pattern, or a subpixel chip might be defective. While correcting these defects is sometimes possible, doing so just adds to the already high cost.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A person looks at a transparent micro led screen displaying splashes of liquid in red, yellow, and green. " class="rm-shortcode" data-rm-shortcode-id="d4c2f8d71053ccb191750db6acaa2024" data-rm-shortcode-name="rebelmouse-image" id="1e653" loading="lazy" src="https://spectrum.ieee.org/media-library/a-person-looks-at-a-transparent-micro-led-screen-displaying-splashes-of-liquid-in-red-yellow-and-green.png?id=52961477&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Samsung’s microLED technology allows the image to extend right up to the edge of the glass panel, making it possible to create larger displays by tiling smaller panels together.  </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Brendan Smialowski/AFP/Getty Images </small>
</p><p>
	Could MicroLEDs still be the future of flat-panel displays? “Every display analyst I know believes that microLEDs should be the ‘next big thing’ because of their brightness, efficiency, color, viewing angles, response times, and lifetime, “ says Bob Raikes, editor of the
	<a href="https://8kassociation.com/8k-monitor-newsletters/" target="_blank"><em><em>8K Monitor</em></em></a> newsletter. “However, the practical hurdles of bringing them to market remain huge. That Apple, which has the deepest pockets of all, has abandoned microLEDs, at least for now, and after billions of dollars in investment, suggests that mass production for consumer markets is still a long way off.”
</p><p>
	At this juncture, even though microLED technology offers some clear advantages, OLED is more cost-effective and holds the early lead for practical applications of transparent displays.
</p><h2>But what is a transparent display good for?</h2><p>
	Samsung and LG aren’t the only companies to have demonstrated transparent panels recently.
</p><p>
	AUO’s 60-inch transparent display, made of tiled panels, won the
	<a href="https://auo.com/en-global/New_Archive/detail/news_awards_20240517" target="_blank"><u>People’s Choice Award</u></a> for Best MicroLED-Based Technology at the Society for Information Display’s Display Week, held in May in San Jose, Calif. And the Chinese company BOE Technology Group demonstrated <a href="https://ces.vporoom.com/2024-01-12-BOE-Showcases-Cutting-Edge-Display-Technologies-at-the-CES-2024-and-Inspires-Innovative-Smart-Life-with-Partners" target="_blank">a 49-inch transparent OLED display</a> at CES 2024.
</p><p>
	These transparent displays all have one feature in common: They will be insanely expensive. Only LG’s transparent OLED display has been announced as a commercial product. It’s without a price or a ship date at this point, but it’s not hard to guess how costly it will be, given that nontransparent versions are expensive enough. For example, LG prices its top-end 77-inch OLED TV at US $4,500.
</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="A diagram of the structure of a display pixel represented as a grey rectangle, which frames an open area labeled transmissive space, and three rectangular blocks labeled R, G, and B." class="rm-shortcode rm-resized-image" data-rm-shortcode-id="3926328fa04e8ce10f5bce25ed40190b" data-rm-shortcode-name="rebelmouse-image" id="eea2a" loading="lazy" src="https://spectrum.ieee.org/media-library/a-diagram-of-the-structure-of-a-display-pixel-represented-as-a-grey-rectangle-which-frames-an-open-area-labeled-transmissive-sp.png?id=52961486&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..." style="max-width: 100%;">Displays using both microLED technology [above] and OLED technology have some components in each pixel that block light coming from the background. These include the red, green, and blue emissive materials along with the transistors required to switch them on and off. Smaller components mean that you can have a larger transmissive space that will provide greater transparency.  </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..." style="max-width: 100%;">Illustration: Mark Montgomery; Source: Samsung</small>
</p><p>
	Thanks to seamless tiling, transparent microLED displays can be larger than their OLED counterparts. But their production costs are larger as well. Much larger. And that is reflected in prices. For example, Samsung’s nontransparent 114-inch microLED TV sells for $150,000. We can reasonably expect transparent models to cost even more.
</p><p>
	Seeing these prices, you really have to ask: What are the practical applications of transparent displays?
</p><p>
	Don’t expect these displays to show up in many living rooms as televisions. And high price is not the only reason. After all, who wants to see their bookshelves showing through in the background while they’re watching
	<em><em>Dune</em></em>? That’s why the transparent OLED TV LG demonstrated at CES 2024 included a “contrast layer”—basically, a black cloth—that unrolls and covers the back of the display on demand.
</p><p>
	Transparent displays could have a place on the desktop—not so you can see through them, but so that a camera can sit behind the display, capturing your image while you’re looking directly at the screen. This would help you maintain eye contact during a Zoom call. One company—<a href="https://veeonow.com/" target="_blank">Veeo</a>—demonstrated a prototype of such a product <a href="https://www.newswire.com/news/veeo-joins-lg-display-in-partnership-to-create-the-first-behind-22057991" target="_blank">at CES 2024</a>, and it plans to release a 30-inch model for about $3,000 and a 55-inch model for about $8,500 later this year. Veeo’s products use LG’s transparent OLED technology.
</p><p>
	Transparent screens are already showing up as signage and other public-information displays. LG has
	<a href="https://www.lgcorp.com/media/release/27627" target="_blank">installed transparent 55-inch OLED panels</a> in the windows of Seoul’s new high-speed underground rail cars, which are part of a system known as the Great Train eXpress. Riders can browse maps and other information on these displays, which can be made clear when needed for passengers to see what’s outside.
</p><p>
	LG transparent panels have also been featured in
	<a href="https://news.lgdisplay.com/en/2022/01/rising-potential-of-transparent-oled/" target="_blank">an E35e excavator prototype</a> by Doosan Bobcat. This touchscreen display can act as the operator’s front or side window, showing important machine data or displaying real-time images from cameras mounted on the vehicle. Such transparent displays can serve a similar function as the head-up displays in some aircraft windshields.
</p><p>
	And so, while the large transparent displays are striking, you’ll be more likely to see them initially as displays for machinery operators, public entertainment, retail signage, and even car windshields. The early adopters might cover the costs of developing mass-production processes, which in turn could drive prices down. But even if costs eventually reach reasonable levels, whether the average consumer really want a transparent TV in their home is something that remains to be seen—unlike the device itself, whose whole point is not to be.
	<span class="ieee-end-mark"></span>
</p>]]></description><pubDate>Mon, 29 Jul 2024 13:00:02 +0000</pubDate><guid>https://spectrum.ieee.org/transparent-tv</guid><category>Displays</category><category>Tv</category><category>Transparent tv</category><category>Microled</category><category>Oled</category><dc:creator>Alfred Poor</dc:creator><media:content medium="image" type="image/png" url="https://spectrum.ieee.org/media-library/an-array-of-15-large-transparent-displays-showing-cascading-water-with-a-crowd-of-people-is-visible-through-the-screens.png?id=52961244&amp;width=980"></media:content></item><item><title>How India Is Starting a Chip Industry From Scratch</title><link>https://spectrum.ieee.org/semiconductor-industry-in-india</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-photo-of-smiling-man-surrounded-by-color-icons-and-shapes.jpg?id=52966098&width=1200&height=800&coordinates=0%2C0%2C0%2C45"/><br/><br/><p>
	In March, India announced a major investment to establish a semiconductor-manufacturing industry. With US $15 billion in investments from companies, state governments, and the central government, India now has plans for several chip-packaging plants and the country’s first modern chip fab as part of a larger effort to grow its electronics industry.
</p><p>
	But turning India into a chipmaking powerhouse will also require a substantial investment in R&D. And so the Indian government turned to IEEE Fellow and retired Georgia Tech professor <a href="https://www.mse.gatech.edu/people/rao-tummala" rel="noopener noreferrer" target="_blank"><u>Rao Tummala</u></a>, a pioneer of some of the chip-packaging technologies that have become critical to modern computers. Tummala spoke with <em><em>IEEE Spectrum</em></em> during the IEEE Electronic Component Technology Conference in Denver, Colo., in May.
</p><h3>Rao Tummala</h3><br/><p>Rao Tummala is a pioneer of semiconductor packaging and a longtime research leader at Georgia Tech.</p><p>
<strong>What are you helping the government of India to develop?</strong>
</p><p>
<strong>Rao Tummala:</strong> I’m helping to develop the R&D side of India’s semiconductor efforts. We picked 12 strategic research areas. If you explore research in those areas, you can make almost any electronic system. For each of those 12 areas, there’ll be one primary center of excellence. And that’ll be typically at an IIT (Indian Institute of Technology) campus. Then there’ll be satellite centers attached to those throughout India. So when we’re done with it, in about five years, I expect to see probably almost all the institutions involved.
</p><p>
<strong>Why did you decide to spend your retirement doing this?</strong>
</p><p>
<strong>Tummala:</strong> It’s my giving back. India gave me the best education possible at the right time.
</p><p>
	I’ve been going to India and wanting to help for 20 years. But I wasn’t successful until the current government decided they’re going to make manufacturing and semiconductors important for the country. They asked themselves: What would be the need for semiconductors, in 10 years, 20 years, 30 years? And they quickly concluded that if you have 1.4 billion people, each consuming, say, $5,000 worth of electronics each year, it requires billions and billions of dollars’ worth of semiconductors.
</p><p class="pull-quote">
	“It’s my giving back. India gave me the best education possible at the right time.” <strong>—Rao Tummala, advisor to the government of India</strong>
</p><p>
<strong>What advantages does India have in the global semiconductor space?</strong>
</p><p>
<strong>Tummala: </strong>India has the best educational system in the world for the masses. It produces the very best students in science and engineering at the undergrad level and lots of them. India is already a success in design and software. All the major U.S. tech companies have facilities in India. And they go to India for two reasons. It has a lot of people with a lot of knowledge in the design and software areas, and those people are cheaper [to employ].
</p><p>
<strong>What are India’s weaknesses, and is the government response adequate to overcoming them?</strong>
</p><p>
<strong>Tummala: </strong>India is clearly behind in semiconductor manufacturing. It’s behind in knowledge and behind in infrastructure. Government doesn’t solve these problems. All that the government does is set the policies and give the money. This has given companies incentives to come to India, and therefore the semiconductor industry is beginning to flourish.
</p><p>
<strong>Will India ever have leading-edge chip fabs?</strong>
</p><p><strong>Tummala: </strong>Absolutely. Not only will it have leading-edge fabs, but in about 20 years, it will have the most comprehensive system-level approach of any country, including the United States. In about 10 years, the size of the electronics industry in India will probably have grown about 10 times.</p><p>
<em>This article appears in the August 2024 print issue as “5 Questions for Rao Tummala.”</em>
</p>]]></description><pubDate>Sun, 28 Jul 2024 15:01:00 +0000</pubDate><guid>https://spectrum.ieee.org/semiconductor-industry-in-india</guid><category>5 questions</category><category>Semiconductor manufacturing</category><category>India</category><category>Chip fabrication</category><category>Semiconductors</category><dc:creator>Samuel K. Moore</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-photo-of-smiling-man-surrounded-by-color-icons-and-shapes.jpg?id=52966098&amp;width=980"></media:content></item><item><title>Try IEEE’s New Virtual Testbed for 5G and 6G Tech</title><link>https://spectrum.ieee.org/ieee-testbed-5g-6g-tech</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/6g-and-5g-sitting-on-top-of-glowing-hexagons-against-a-dark-background-with-glowing-lights.jpg?id=52974222&width=1200&height=800&coordinates=0%2C104%2C0%2C105"/><br/><br/><p>Telecom engineers and researchers face several challenges when it comes to testing their 5G and 6G prototypes. One is finding a testbed where they can run experiments with their new hardware and software.</p><p>The experimentation platforms, which resemble real-world conditions, can be pricey. Some have a time limit. Others may be used only by specific companies or for testing certain technologies. </p><p>The new <a href="https://testbed.ieee.org/" rel="noopener noreferrer" target="_blank"><u>IEEE 5G/6G Innovation Testbed</u></a> has eliminated many of those barriers. Built by IEEE, the platform is for those who want to try out their 5G enhancements, run trials of future 6G functions, or test updates for converged networks. Users may test and retest as many times as they want at no additional cost.</p><p>Telecom operators can use the new virtual testbed, as can application developers, researchers, educators, and vendors from any industry.</p><p>“The IEEE 5G/6G Innovation Testbed creates an environment where industry can break new ground and work together to develop the next generation of technology innovations,” says <a href="https://www.linkedin.com/in/anwer-al-dulaimi-32511478/" rel="noopener noreferrer" target="_blank"><u>Anwer Al-Dulaimi</u></a>, cochair of the IEEE 5G/6G Innovation Testbed working group. Al-Dulaimi, an IEEE senior member, is a senior strategy manager of connectivity and Industry 4.0 for <a href="https://www.veltris.com/" rel="noopener noreferrer" target="_blank"><u>Veltris</u></a>, in Toronto.</p><p>The testbed was launched this year with support from <a href="https://www.att.com/" rel="noopener noreferrer" target="_blank"><u>AT&T</u></a>, <a href="https://www.exfo.com/en/" rel="noopener noreferrer" target="_blank"><u>Exfo</u></a>, <a href="https://www.eurecom.fr/en" rel="noopener noreferrer" target="_blank"><u>Eurecom</u></a>, <a href="https://www.veltris.com/" rel="noopener noreferrer" target="_blank"><u>Veltris</u></a>, <a href="https://www.vmware.com/" rel="noopener noreferrer" target="_blank"><u>VMWare</u></a>, and <a href="https://www.techmahindra.com/" rel="noopener noreferrer" target="_blank"><u>Tech Mahindra</u></a>.</p><p>The subscription-based testbed is available only to organizations. Customers receive their own private, secure<em> </em>session of the testing platform in the cloud along with the ability to add new users.</p><h2>A variety of architectures and experiments</h2><p>The platform eliminates the need for customers to travel to a location and connect to physical hardware, Al-Dulaimi says. That’s because its digital hub is based in the cloud, allowing companies, research facilities, and organizations to access it. The testbed allows customers to upload their own software components for testing. </p><p>“IEEE 5G/6G Innovation Testbed provides a unique platform for the service providers, and various vertical industries—including defense, homeland security, agriculture, and automotive—to experiment various use cases that can take advantage of advanced 5G technologies like ultra low latency, machine-to-machine type communications and massive broadband to help solve their pain points,” says IEEE Fellow <a href="https://engineering.jhu.edu/doctor-of-engineering/news/ashutosh-dutta-named-director-of-doctor-of-engineering-program/" rel="noopener noreferrer" target="_blank"><u>Ashutosh Dutta</u></a>, who is a cochair of the working group. Dutta works as chief 5G strategist at the <a href="https://www.jhuapl.edu/" rel="noopener noreferrer" target="_blank"><u>Johns Hopkins University Applied Physics Laboratory</u></a>, in Laurel, Md. He also heads the university’s <a href="https://engineering.jhu.edu/doctor-of-engineering/" rel="noopener noreferrer" target="_blank"><u>Doctor of Engineering program</u></a>.</p><p class="pull-quote">“The IEEE 5G/6G Innovation Testbed creates an environment where industry can break new ground and work together to develop the next generation of technology innovations.” </p><p>The collaborative, secure, cloud-based platform also can emulate a 5G end-to-end network within the<a href="https://spectrum.ieee.org/5g-release-16" target="_self"> <u>3rd Generation Partnership Program</u></a> (3GPP), which defines cellular communications standards.</p><p>“Companies can use the platform for testing, but they can also use the environment as a virtual hands-on showcase of new products, services, and network functions,” Dutta says.</p><p>In addition to the cloud-based end-to-end environment, the testbed supports other architectures including multiaccess edge computing for reduced latency, physical layer testing via 5G access points and phones installed at IEEE, and Open RAN (radio access network) environments where wireless radio functionality is disaggregated to allow for better flexibility in mixing hardware and software components.</p><p>A variety of experiments can be conducted, Al-Dulaimi says, including:</p><ul><li>Voice and video call emulation.</li><li>Authentication and encryption impact evaluation across different 5G platforms.</li><li>Network slicing.</li><li>Denial-of-service attacks and interoperability and overload incidents.</li><li>Verifying the functionality, compatibility, and interoperability of products.</li><li>Assessing conformity of networks, components, and products.</li></ul><p>The testbed group plans to release a new graphical user interface soon, as well as a test orchestration tool that contains hundreds of plug-and-play test cases to help customers quickly determine if their prototypes are working as intended across a variety of standards and scenarios. In addition to basic “sanity testing,” it includes tools to measure a proposed product’s real-time performance.</p><p>The proofs of concept—lessons learned from experiments—will help advance existing standards and create new ones, Dutta says, and they will expedite the deployment of 5G and 6G technologies.</p><p>The IEEE 5G/6G testbed is an asset that can be used by the academics, researchers, and R&D labs, he says, to help “close the gap between theory and practice. Students across the world can take advantage of this testbed to get hands-on experience as part of their course curriculum.”</p><h2>Partnership with major telecom companies</h2><p>The IEEE 5G/6G Innovation Testbed recently joined the <a href="https://www.ntia.gov/press-release/2024/biden-harris-administration-awards-42m-wireless-innovation" rel="noopener noreferrer" target="_blank"><u>Acceleration of Compatibility and Commercialization for Open RAN Deployments</u></a> project. A public-private consortium, ACCORD includes AT&T, <a href="https://www.verizon.com/" rel="noopener noreferrer" target="_blank"><u>Verizon</u></a>, <a href="https://www.vt.edu/" rel="noopener noreferrer" target="_blank"><u>Virginia Tech</u></a> and the <a href="https://www.utdallas.edu/" rel="noopener noreferrer" target="_blank"><u>University of Texas at Dallas</u></a>. The group is funded by the U.S. Department of Commerce’s <a href="https://www.ntia.gov/" rel="noopener noreferrer" target="_blank"><u>National Telecommunications and Information Administration</u></a>, whose programs and policymaking efforts focus on expanding broadband Internet access and adoption throughout the country. </p><p>“The 3GPP-compliant end-to-end 5G network is built with a suite of open-source modules, allowing companies to customize the network architecture and tailor their testbed environment according to their needs,” Al-Dulaimi says.</p><p>The testbed was made possible with a grant from the <a href="https://www.ieee.org/about/corporate/initiatives/initiatives-committee.html" target="_blank"><u>IEEE New Initiatives Committee</u></a>, which funds potential IEEE services, products, and other creations that could significantly benefit members, the public, customers, or the technical community.</p>To get a free trial of the testbed, complete <a href="https://testbed.ieee.org/contact/" target="_blank"><u>this form</u></a>.<p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="aa00f797b9d68003ddc10f3b57817f8b" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/Se1lusw4qZE?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Watch this short demonstration of how the IEEE 5G/6G Innovation Testbed works.</small>
<small class="image-media media-photo-credit" placeholder="Add Photo Credit...">
<a href="https://youtu.be/Se1lusw4qZE?si=NBPBWhqWHQJXMwmL" target="_blank">youtube</a>
</small>
</p>]]></description><pubDate>Fri, 26 Jul 2024 18:00:02 +0000</pubDate><guid>https://spectrum.ieee.org/ieee-testbed-5g-6g-tech</guid><category>5g</category><category>6g</category><category>Ieee products and services</category><category>Telecommunications</category><category>Test bed</category><category>Type:ti</category><dc:creator>Kathy Pretz</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/6g-and-5g-sitting-on-top-of-glowing-hexagons-against-a-dark-background-with-glowing-lights.jpg?id=52974222&amp;width=980"></media:content></item><item><title>Video Friday: Robot Baby With a Jet Pack</title><link>https://spectrum.ieee.org/video-friday-robot-baby-jetpack</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-robot-with-jet-engines-with-blue-flames-coming-out-attached-to-its-arms-and-back-stands-in-a-safety-frame-on-a-rooftop-at-nigh.gif?id=52974821&width=1200&height=800&coordinates=62%2C0%2C63%2C0"/><br/><br/><p>Video Friday is your weekly selection of awesome robotics videos, collected by your friends at <em>IEEE Spectrum</em> robotics. We also post a weekly calendar of upcoming robotics events for the next few months. Please <a href="mailto:automaton@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday">send us your events</a> for inclusion.<br/></p><h5><a href="https://icra40.ieee.org/">ICRA@40</a>: 23–26 September 2024, ROTTERDAM, NETHERLANDS</h5><h5><a href="https://iros2024-abudhabi.org/">IROS 2024</a>: 14–18 October 2024, ABU DHABI, UAE</h5><h5><a href="https://icsr2024.dk/">ICSR 2024</a>: 23–26 October 2024, ODENSE, DENMARK</h5><h5><a href="https://cybathlon.ethz.ch/en/events/edition/cybathlon-2024">Cybathlon 2024</a>: 25–27 October 2024, ZURICH</h5><p>Enjoy today’s videos!</p><div class="horizontal-rule"></div><div style="page-break-after: always"><span style="display:none"> </span></div><p class="rm-anchors" id="wkhqym57usw">If the Italian Institute of Technology’s iRonCub3 looks this cool while <em>learning</em> to fly, just imagine how cool it will look when it actually takes off!</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="d6dae7256db9afa12f8a7317404c4962" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/wKhqym57USw?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>Hovering is in the works, but this is a really hard problem, which you can read more about in Daniele Pucci’s post on LinkedIn.</p><p>[ <a href="https://www.linkedin.com/posts/daniele-pucci-78428420_flying-jetpowered-humanoidrobotics-activity-7221876503725166595-nWSL?utm_source=share&utm_medium=member_desktop">LinkedIn</a> ]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="8j_wit-rd74"><em>Stanford Engineering and the Toyota Research Institute achieve the world’s first autonomous tandem drift. Leveraging the latest AI technology, Stanford Engineering and TRI are working to make driving safer for all. By automating a driving style used in motorsports called drifting—in which a driver deliberately spins the rear wheels to break traction—the teams have unlocked new possibilities for future safety systems.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="4a3a13a9bc7a4bfee74f2c445698288d" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/8J_WiT-RD74?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://toyotaresearch.medium.com/stanford-engineering-and-toyota-research-institute-achieve-worlds-first-autonomous-tandem-drift-131fcb9a76a9">TRI</a> ]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="dx7y0bchwmw"><em>Researchers at the Istituto Italiano di Tecnologia (Italian Institute of Technology) have demonstrated that under specific conditions, humans can treat robots as coauthors of the results of their actions. The condition that enables this phenomenon is a robot that behaves in a social, humanlike manner. Engaging in eye contact and participating in a common emotional experience, such as watching a movie, are key.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="60307c73020e7095cf8efbc264655a7b" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/DX7y0bChWmw?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.science.org/doi/10.1126/scirobotics.adj3665">Science Robotics</a> ]</p><div class="horizontal-rule"></div><p class="rm-anchors" id="hjueskudnds">If Aibo is not quite catlike enough for you, here you go.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="c923a7cf3666cd8c8b9815fe1804893d" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/hJUesKudNDs?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.makuake.com/project/maicat/">Maicat</a> ] via [ <a href="https://robotstart.info/2024/07/24/cat-robot-maicat-on-sale-japan.html">RobotStart</a> ]</p><div class="horizontal-rule"></div><p class="rm-anchors" id="69svc-43oqg">I’ve never been more excited for a sim-to-real gap to be bridged.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="bcb4c13c6097ae2ac5f1fb6809ba752c" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/69SVc-43Oqg?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://sites.usc.edu/quann/">USC Viterbi</a> ]</p><div class="horizontal-rule"></div><p class="rm-anchors" id="m-utklmylhk">I’m sorry, but this looks exactly like a quadrotor sitting on a test stand.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="51c5a294715c623d6e6295b2c3111ce7" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/M-utKlMYlHk?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><blockquote><em>The 12-pound Quad-Biplane combines four rotors and two wings without any control surfaces. The aircraft takes off like a conventional quadcopter and transitions to a more-efficient horizontal cruise flight, similar to that of a biplane. This combines the simplicity of a quadrotor design, providing vertical flight capability, with the cruise efficiency of a fixed-wing aircraft. The rotors are responsible for aircraft control both in vertical and forward cruise flight regimes.</em></blockquote><p>[ <a href="https://avfl.engr.tamu.edu/">AVFL</a> ]</p><div class="horizontal-rule"></div><p class="rm-anchors" id="okj-newfeci">Tensegrity robots are so weird, and I so want them to be useful.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="a09f13b1b2e8b94f78ceb2164b0a153c" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/OkJ-nEWFEcI?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="http://www-robot.mes.titech.ac.jp/index_e.html">Suzumori Endo Lab</a> ]</p><div class="horizontal-rule"></div><p class="rm-anchors" id="xqqxnbgr3ii">Top-performing robots need all the help they can get.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="d1993d56b4ee8850163cce456795cd04" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/xqqxnbGR3II?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://b-human.de/index.html">Team B-Human</a> ]</p><div class="horizontal-rule"></div><p class="rm-anchors" id="-ca3xqe6z_4">And now: a beetle nearly hit by an autonomous robot.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="61aec8c749093473535f7c49aea3aa54" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/-Ca3xqe6z_4?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://yugu.faculty.wvu.edu/">WVUIRL</a> ]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="tmgk0ata5hk"><em>Humans possess a remarkable ability to react to unpredictable perturbations through immediate mechanical responses, which harness the visco-elastic properties of muscles to maintain balance. Inspired by this behavior, we propose a novel design of a robotic leg utilizing fiber-jammed structures as passive compliant mechanisms to achieve variable joint stiffness and damping.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="a6f722be407faca7426d51b4544886d4" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/TMGk0ATA5Hk?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://arxiv.org/abs/2308.01758">Paper</a> ]</p><div class="horizontal-rule"></div><p class="rm-anchors" id="wjb4l4gly8c">I don’t know what this piece of furniture is, but your cats will love it.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="9bded74939eec40eb92d0903446ae5bf" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/WJB4L4gLY8c?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://new.abb.com/news/detail/117938/cstmr-robot-wound-veneer-a-sustainable-building-material-for-the-future">ABB</a> ]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="sd0p5mmaykm"><em>This video shows a dexterous avatar humanoid robot with VR teleoperation, hand tracking, and speech recognition to achieve highly dexterous mobile manipulation. Extend Robotics is developing a dexterous remote-operation interface to enable data collection for embodied AI and humanoid robots.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="eace905cc56c5c3645894e2bd582f71e" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/sD0p5mmAYKM?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.extendrobotics.com/">Extend Robotics</a> ]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="diautbgrxki"><em>I never really thought about this, but wind turbine blades are hollow inside and need to be inspected sometimes, which is really one of those jobs where you’d much rather have a robot do it.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="a7b14059d0c2df39a96be46cde0e920d" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/dIAUTbgRXkI?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.flyability.com/">Flyability</a> ]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="zqypazevbia"><em>Here’s a full, uncut drone-delivery mission, including a package pickup from our AutoLoader—a simple, nonpowered mechanical device that allows retail partners to utilize drone delivery with existing curbside-pickup workflows.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="c7ba3c7bd5f25392d1b00056834e5b90" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/zQYPaZeVbIA?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://blog.wing.com/2024/01/customer-demand-and-wings-aircraft.html">Wing</a> ]</p><div class="horizontal-rule"></div><p class="rm-anchors" id="687hxi0_5li">Daniel Simu and his acrobatic robot competed in “America’s Got Talent,” and even though his robot did a very robot thing by breaking itself immediately beforehand, the performance went really well.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="c46e071cf5be504cace4699b280a190a" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/687HXI0_5lI?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://acrobot.nl/">Acrobot</a> ]</p><div class="horizontal-rule"></div><p class="rm-anchors" id="k5_r0r8tiby">A tour of the Creative Robotics Mini Exhibition at the Creative Computing Institute, University of the Arts London.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="41739cdf34dec916c9f5872c4d828466" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/k5_R0R8TiBY?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.arts.ac.uk/subjects/creative-computing/postgraduate/msc-creative-robotics">UAL</a> ]</p><p>Thanks, Hooman!</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="dycujjms3uo"><em>Zoox CEO Aicha Evans and cofounder and chief technology officer Jesse Levinson hosted a LinkedIn Live last week to reflect on the past decade of building Zoox and their predictions for the next 10 years of the autonomous-vehicle industry.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="62682d4df022a6c98247bd82b5ca608e" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/DYcujjMs3Uo?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://zoox.com/">Zoox</a> ]</p><div class="horizontal-rule"></div>]]></description><pubDate>Fri, 26 Jul 2024 16:59:31 +0000</pubDate><guid>https://spectrum.ieee.org/video-friday-robot-baby-jetpack</guid><category>Robotics</category><category>Video friday</category><dc:creator>Evan Ackerman</dc:creator><media:content medium="image" type="image/gif" url="https://spectrum.ieee.org/media-library/a-robot-with-jet-engines-with-blue-flames-coming-out-attached-to-its-arms-and-back-stands-in-a-safety-frame-on-a-rooftop-at-nigh.gif?id=52974821&amp;width=980"></media:content></item><item><title>The Engineer Who Pins Down the Particles at the LHC</title><link>https://spectrum.ieee.org/cern-engineer-irene-deglinnocenti</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-woman-in-a-blue-shirt-standing-in-front-of-a-black-closet-full-of-colored-wires-and-next-to-the-black-closet-is-a-white-platf.png?id=52960458&width=1200&height=800&coordinates=0%2C0%2C0%2C10"/><br/><br/><p>
	The
	<a href="https://home.cern/science/accelerators/large-hadron-collider" rel="noopener noreferrer" target="_blank">Large Hadron Collider</a> has transformed our understanding of physics since it began operating in 2008, enabling researchers to investigate the fundamental building blocks of the universe. Some 100 meters below the border between France and Switzerland, particles accelerate along the LHC’s 27-kilometer circumference, nearly reaching the speed of light before smashing together.
</p><p>
	The LHC is often described as the biggest machine ever built. And while the physicists who carry out experiments at the facility tend to garner most of the attention, it takes
	<a href="https://www.home.cern/science/engineering" rel="noopener noreferrer" target="_blank">hundreds of engineers and technicians</a> to keep the LHC running. One such engineer is<a href="https://www.linkedin.com/in/irene-degl-innocenti-4650b6133/" rel="noopener noreferrer" target="_blank"> Irene Degl’Innocenti</a>, who works in digital electronics at the <a href="https://home.cern/" rel="noopener noreferrer" target="_blank">European Organization for Nuclear Research</a> (CERN), which operates the LHC. As a member of CERN’s beam instrumentation group, Degl’Innocenti creates custom electronics that measure the position of the particle beams as they travel.
</p><h3>Irene Degl’Innocenti</h3><br/><p>
<strong>Employer:</strong>
</p><p>
	CERN
</p><p>
<strong>Occupation:</strong>
</p><p>
	Digital electronics engineer
</p><p><strong>Education:</strong></p><p>Bachelor’s and master’s degrees in electrical engineering; Ph.D. in electrical, electronics, and communications engineering, University of Pisa, in Italy</p><p>
	“It’s a huge machine that does very challenging things, so the amount of expertise needed is vast,” Degl’Innocenti says.
</p><p>
	The electronics she works on make up only a tiny part of the overall operation, something Degl’Innocenti is keenly aware of when she descends into the LHC’s cavernous tunnels to install or test her equipment. But she gets great satisfaction from working on such an important endeavor.
</p><p>
	“You’re part of something that is very huge,” she says. “You feel part of this big community trying to understand what is actually going on in the universe, and that is very fascinating.”
</p><h2>Opportunities to Work in High-energy Physics</h2><p>
	Growing up in Italy, Degl’Innocenti wanted to be a novelist. Throughout high school she leaned toward the humanities, but she had a natural affinity for math, thanks in part to her mother, who is a science teacher.
</p><p>
	“I’m a very analytical person, and that has always been part of my mind-set, but I just didn’t find math charming when I was little,” Degl’Innocenti says. “It took a while to realize the opportunities it could open up.”
</p><p>
	She started exploring electronics around age 17 because it seemed like the most direct way to translate her logical, mathematical way of thinking into a career. In 2011, she enrolled in
	<a href="https://www.unipi.it/index.php/english" rel="noopener noreferrer" target="_blank"> </a>the <a href="https://www.unipi.it/index.php/english" rel="noopener noreferrer" target="_blank">University of Pisa</a>, in Italy, earning a bachelor’s degree in electrical engineering in 2014 and staying on to earn a master’s degree in the same subject.
</p><p>
	At the time, Degl’Innocenti had no idea there were opportunities for engineers to work in high-energy physics. But she learned that a fellow student had attended a summer internship at
	<a href="https://www.fnal.gov/" rel="noopener noreferrer" target="_blank">Fermilab</a>, the participle physics and accelerator laboratory in Batavia, Ill. So she applied for and won an internship there in 2015. Since Fermilab and CERN closely collaborate, she was able to help design a data-processing board for LHC’s <a href="https://home.cern/science/experiments/cms" rel="noopener noreferrer" target="_blank">Compact Muon Solenoid</a> experiment.
</p><p>
	Next she looked for an internship closer to home and discovered CERN’s
	<a href="https://careers.cern/tech-projects" rel="noopener noreferrer" target="_blank">technical student program</a>, which allows students to work on a project over the course of a year. Working in the beam-instrumentation group, Degl’Innocenti designed a digital-acquisition system that became the basis for her master’s thesis.
</p><h2>Measuring the Position of Particle Beams</h2><p>
	After receiving her master’s in 2017, Degl’Innocenti went on to pursue a Ph.D., also at the University of Pisa. She conducted her research at CERN’s beam-position section, which builds equipment to measure the position of particle beams within CERN’s accelerator complex. The LHC has roughly 1,000 monitors spaced around the accelerator ring. Each monitor typically consists of two pairs of sensors positioned on opposite sides of the accelerator pipe, and it is possible to measure the beam’s horizontal and vertical positions by comparing the strength of the signal at each sensor.
</p><p>
	The underlying concept is simple, Degl’Innocenti says, but these measurements must be precise. Bunches of particles pass through the monitors every 25 nanoseconds, and their position must be tracked to within 50 micrometers.
</p><p class="pull-quote">“We start developing a system years in advance, and then it has to work for a couple of decades.”</p><p>
	Most of the signal processing is normally done in analog, but during her Ph.D., she focused on shifting as much of this work as possible to the digital domain because analog circuits are finicky, she says. They need to be precisely calibrated, and their accuracy tends to drift over time or when temperatures fluctuate.
</p><p>
	“It’s complex to maintain,” she says. “It becomes particularly tricky when you have 1,000 monitors, and they are located in an accelerator 100 meters underground.”
</p><p>
	Information is lost when analog is converted to digital, however, so Degl’Innocenti analyzed the performance of the latest analog-to-digital converters (ADCs) and investigated their effect on position measurements.
</p><h2>Designing Beam-Monitor Electronics</h2><p>
	After completing her Ph.D. in electrical, electronics, and communications engineering in 2021, Degl’Innocenti joined CERN as a senior postdoctoral fellow. Two years later, she became a full-time employee there, applying the results of her research to developing new hardware. She’s currently designing a new beam-position monitor for the
	<a href="https://home.cern/science/accelerators/high-luminosity-lhc" rel="noopener noreferrer" target="_blank">High-Luminosity upgrade</a> to the LHC, expected to be completed in 2028. This new system will likely use a system-on-chip to house most of the electronics, including several ADCs and a field-programmable gate array (FPGA) that Degl’Innocenti will program to run a new digital signal-processing algorithm.
</p><p>
	She’s part of a team of just 15 who handle design, implementation, and ongoing maintenance of CERN’s beam-position monitors. So she works closely with the engineers who design sensors and software for those instruments and the physicists who operate the accelerator and set the instruments’ requirements.
</p><p>
	“We start developing a system years in advance, and then it has to work for a couple of decades,” Degl’Innocenti says.
</p><h2>Opportunities in High-Energy Physics</h2><p>
	High-energy physics has a variety of interesting opportunities for engineers, Degl’Innocenti says, including high-precision electronics, vacuum systems, and cryogenics.
</p><p>
	“The machines are very large and very complex, but we are looking at very small things,” she says. “There are a lot of big numbers involved both at the large scale and also when it comes to precision on the small scale.”
</p><p>
	FPGA design skills are in high demand at all kinds of research facilities, and embedded systems are also becoming more important, Degl’Innocenti says. The key is keeping an open mind about where to apply your engineering knowledge, she says. She never thought there would be opportunities for people with her skill set at CERN.
</p><p>
	“Always check what technologies are being used,” she advises. “Don’t limit yourself by assuming that working somewhere would not be possible.”
</p><p><em>This article appears in the August 2024 print issue as “Irene Degl’Innocenti.”</em></p>]]></description><pubDate>Fri, 26 Jul 2024 13:00:02 +0000</pubDate><guid>https://spectrum.ieee.org/cern-engineer-irene-deglinnocenti</guid><category>Careers</category><category>Cern</category><category>Large hadron collider</category><category>Particle accelerator</category><category>Physics</category><category>Type:departments</category><dc:creator>Edd Gent</dc:creator><media:content medium="image" type="image/png" url="https://spectrum.ieee.org/media-library/a-woman-in-a-blue-shirt-standing-in-front-of-a-black-closet-full-of-colored-wires-and-next-to-the-black-closet-is-a-white-platf.png?id=52960458&amp;width=980"></media:content></item><item><title>Why a Technical Master’s Degree Can Accelerate Your Engineering Career</title><link>https://spectrum.ieee.org/purdue-online-msece</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-person-wearing-a-hard-hat-and-a-high-visibility-vest-is-holding-a-tablet-in-front-of-a-large-array-of-solar-panels-with-wind-t.jpg?id=52952120&width=1200&height=800&coordinates=711%2C0%2C0%2C0"/><br/><br/><p>
<em>This sponsored article is brought to you by <a href="https://engineering.purdue.edu/ECE/Academics/Online?utm_source=association&utm_medium=ieee_spectrum&utm_campaign=ECE-MSECE-OL-ECEN" target="_blank">Purdue University</a>.</em>
</p><p>
	Companies large and small are seeking engineers with up-to-date, subject-specific knowledge in disciplines like computer engineering, automation, artificial intelligence, and circuit design. Mid-level engineers need to advance their skillsets to apply and integrate these technologies and be competitive.</p><h3></h3><br/><p>As applications for new technologies continue to grow, demand for knowledgeable electrical and computer engineers is also on the rise. According to the Bureau of Labor Statistics, job outlook for <a href="https://www.bls.gov/ooh/architecture-and-engineering/electrical-and-electronics-engineers.htm" rel="noopener noreferrer" target="_blank">electrical and electronics engineers</a>—as well as <a href="https://www.bls.gov/ooh/architecture-and-engineering/computer-hardware-engineers.htm" rel="noopener noreferrer" target="_blank">computer hardware engineers</a>—is set to grow 5 percent through 2032. Electrical and computer engineers work in almost every industry. They design systems, work on power transmission and power supplies, run computers and communication systems, innovate chips for embedded and so much more.</p><p>To take advantage of this job growth and get more return-on-investment, engineers are advancing their knowledge by going back to school. The 2023 <a href="https://ieeeusa.org/product/ieee-usa-salary-benefits-survey-report-2023-edition/" target="_blank"><em>IEEE-USA Salary and Benefits Survey Report</em></a> shows that engineers with focused master’s degrees (e.g., electrical and computer engineering, electrical engineering, or computer engineering) earned median salaries almost US $27,000 per year higher than their colleagues with bachelors’ degrees alone.</p><h3></h3><br/><p>Purdue’s online MSECE program has been ranked in the top 3 of U.S. News and World Report’s Best Online Electrical Engineering Master’s Programs for five years running</p><h3></h3><br/><p>Universities like Purdue University work with companies and professionals to provide upskilling opportunities via distance and online education. Purdue has offered a distance <a href="https://engineering.purdue.edu/ECE/Academics/Online?utm_source=association&utm_medium=ieee_spectrum&utm_campaign=ECE-MSECE-OL-ECEN" rel="noopener noreferrer" target="_blank">Master of Science in Electrical and Computer Engineering</a> (MSECE) since the 1980s. In its early years, the program’s course lectures were videotaped and mailed to students. Now, “distance” has transformed into “online,” and the program has grown with the web, expanding its size and scope. Today, the online MSECE has awarded master’s degrees to 190+ online students since the Fall 2021 semester.</p><h3></h3><br/><img alt="A person with shoulder-length brown hair is wearing a black blazer over a dark blouse. They have a silver necklace with a pendant. The background consists of a brick wall." class="rm-shortcode" data-rm-shortcode-id="fa02638e7e7dcc3242257e19fb15e094" data-rm-shortcode-name="rebelmouse-image" id="9a7eb" loading="lazy" src="https://spectrum.ieee.org/media-library/a-person-with-shoulder-length-brown-hair-is-wearing-a-black-blazer-over-a-dark-blouse-they-have-a-silver-necklace-with-a-pendan.png?id=52952107&width=980"/><h3></h3><br/><p>“Purdue has a long-standing reputation of engineering excellence and Purdue engineers work worldwide in every company, including General Motors, Northrop Grumman, Raytheon, Texas Instruments, Apple, and Sandia National Laboratories among scores of others,” said Lynn Hegewald, the senior program manager for Purdue’s online MSECE. “Employers everywhere are very aware of Purdue graduates’ capabilities and the quality of the education they bring to the job.”<br/></p><h3></h3><br><p>Today, the online MSECE program continues to select from among the world’s best professionals and gives them an affordable, award-winning education. The program has been ranked in the top 3 of <em>U.S. News and World Report’s </em><a href="https://www.usnews.com/education/online-education/purdue-university-main-campus-OENG0108/engineering" rel="noopener noreferrer" target="_blank">Best Online Electrical Engineering Master’s Programs</a> for five years running (2020, 2021, 2022, 2023, and 2024).</p><h3></h3><br/><p>The online MSECE offers high-quality research and technical skills, high-level analytical thinking and problem-solving skills, and new ideas to help innovate—all highly sought-after, according to one of the few <a href="https://ieeexplore.ieee.org/document/8593487" target="_blank">studies</a> to systematically inventory what engineering employers want (information corroborated on occupational guidance websites like <a href="https://www.onetonline.org/link/summary/17-2071.00" target="_blank">O-Net</a> and the <a href="https://www.bls.gov/ooh/architecture-and-engineering/electrical-and-electronics-engineers.htm#tab-1" target="_blank">Bureau of Labor Statistics</a>).</p><p>Remote students get the same education as on-campus students and become part of the same alumni network.</p><p>“Our online MSECE program offers the same exceptional quality as our on-campus offerings to students around the country and the globe,” says Prof. Milind Kulkarni, Michael and Katherine Birck Head of the Elmore Family School of Electrical and Computer Engineering. “Online students take the same classes, with the same professors, as on-campus students; they work on the same assignments and even collaborate on group projects.</p><h3></h3><br/><p>“Our online MSECE program offers the same exceptional quality as our on-campus offerings to students around the country and the globe” <strong>—Prof. Milind Kulkarni, Purdue University</strong></p><h3></h3><br/><p>“We’re very proud,” he adds, “that we’re able to make a ‘full-strength’ Purdue ECE degree available to so many people, whether they’re working full-time across the country, live abroad, or serve in the military. And the results bear this out: graduates of our program land jobs at top global companies, move on to new roles and responsibilities at their current organizations, or even continue to pursue graduate education at top PhD programs.”</p><h3></h3><br/><img alt="A person wearing a dark blazer over a light blue, patterned shirt is smiling at the camera and standing indoors with a modern background featuring large windows and wooden panels." class="rm-shortcode" data-rm-shortcode-id="4c8c8c9a7a82a7dec667c0a41aa91fc0" data-rm-shortcode-name="rebelmouse-image" id="4efb8" loading="lazy" src="https://spectrum.ieee.org/media-library/a-person-wearing-a-dark-blazer-over-a-light-blue-patterned-shirt-is-smiling-at-the-camera-and-standing-indoors-with-a-modern-ba.png?id=52952141&width=980"/><h3></h3><br/><h2>Variety and Quality in Purdue’s MSECE</h2><p>As they study for their MSECE degrees, online students can select from among a hundred <a href="https://engineering.purdue.edu/ECE/Academics/Undergraduates/UGO/CourseInfo/coursesGrad" rel="noopener noreferrer" target="_blank">graduate-level courses</a> in their primary areas of interest, including innovative one-credit-hour courses that extend the students’ knowledge. New courses and new areas of interest are always in the pipeline.</p><h3>Purdue MSECE Area of Interest and Course Options</h3><br/><ul><li>Automatic Control</li><li>Communications, Networking, Signal and Image Processing</li><li>Computer Engineering</li><li>Fields and Optics</li><li>Microelectronics and Nanotechnology</li><li>Power and Energy Systems</li><li>VLSI and Circuit Design</li><li>Semiconductors</li><li>Data Mining</li><li>Quantum Computing</li><li>IoT</li><li>Big Data</li></ul>
<h3></h3><br/><p>Heather Woods, a process engineer at Texas Instruments, was one of the first students to enroll and chose the microelectronics and nanotechnology focus area. She offers this advice: “Take advantage of the one credit-hour classes! They let you finish your degree faster while not taking six credit hours every semester.”</p><h3></h3><br/><p>Completing an online MSECE from Purdue University also teaches students professional skills that employers value like motivation, efficient time-management, high-level analysis and problem-solving, and the ability to learn quickly and write effectively.</p><p>“Having an MSECE shows I have the dedication and knowledge to be able to solve problems in engineering,” said program alumnus Benjamin Francis, now an engineering manager at AkzoNobel. “As I continue in my career, this gives me an advantage over other engineers both in terms of professional advancement opportunity and a technical base to pull information from to face new challenges.”</p><h3></h3><br/><h2>Finding Tuition Assistance</h2><p>Working engineers contemplating graduate school should contact their human resources departments and find out what their tuition-assistance options are. Does your company offer tuition assistance? What courses of study do they cover? Do they cap reimbursements by course, semester, etc.? Does your employer pay tuition directly, or will you pay out-of-pocket and apply for reimbursement?</p><p>Prospective U.S. students who are veterans or children of veterans should also check with the <a href="https://www.va.gov/education/survivor-dependent-benefits/dependents-education-assistance/" rel="noopener noreferrer" target="_blank">U.S. Department of Veterans Affairs</a> to see if they qualify to for tuition or other assistance.</p><h3></h3><br/><h2>The MSECE Advantage</h2><p>In sum, the online <a href="https://engineering.purdue.edu/ECE/Academics/Online?utm_source=association&utm_medium=ieee_spectrum&utm_campaign=ECE-MSECE-OL-ECEN" target="_blank">Master’s degree in Electrical and Computer Engineering from Purdue University</a> does an extraordinary job giving students the tools they need to succeed in school and then in the workplace: developing the technical knowledge, the confidence, and the often-overlooked professional skills that will help them excel in their careers.</p></br>]]></description><pubDate>Thu, 25 Jul 2024 13:00:01 +0000</pubDate><guid>https://spectrum.ieee.org/purdue-online-msece</guid><category>Computer engineering</category><category>Electrical engineering</category><category>Professional development</category><category>Purdue university</category><category>Type:sponsored</category><dc:creator>Douglas McCormick</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-person-wearing-a-hard-hat-and-a-high-visibility-vest-is-holding-a-tablet-in-front-of-a-large-array-of-solar-panels-with-wind-t.jpg?id=52952120&amp;width=980"></media:content></item><item><title>The Rise of Groupware</title><link>https://spectrum.ieee.org/groupware</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-photo-illustration-showing-3-people-working-on-1980s-era-computers-with-lines-and-arrows-connecting-them.jpg?id=52963507&width=1200&height=800&coordinates=0%2C0%2C256%2C0"/><br/><br/><div class="intro-text">
<em>A version of this post <a href="https://tedium.co/2024/04/13/groupware-workgroup-history/" rel="noopener noreferrer" target="_blank">originally appeared</a> on <a href="https://tedium.co/" rel="noopener noreferrer" target="_blank">Tedium</a>, Ernie Smith’s newsletter, which hunts for the end of the long tail.</em>
</div><p class="drop-caps">
<strong>These days, computer users </strong>take collaboration software for granted. <a href="https://www.google.com/docs/about/" rel="noopener noreferrer" target="_blank">Google Docs</a>, <a href="https://www.microsoft.com/en-us/microsoft-teams/group-chat-software" rel="noopener noreferrer" target="_blank">Microsoft Teams</a>, <a href="https://slack.com/" rel="noopener noreferrer" target="_blank">Slack</a>, <a href="https://www.salesforce.com/" rel="noopener noreferrer" target="_blank">Salesforce</a>, and so on, are such a big part of many people’s daily lives that they hardly notice them. But they are the outgrowth of years of hard work done before the Internet became a thing, when there was a thorny problem: How could people collaborate effectively when everyone’s using a stand-alone personal computer?
</p><p class="rm-anchors" id="top">
	The answer was groupware, an early term for collaboration software designed to work across multiple computers attached to a network. At first, those computers were located in the same office, but the range of operation slowly expanded from there, forming the highly collaborative networked world of today. This post will trace some of this history, starting from
	<a href="#collaboration">early ideas formed at Stanford Research Institute</a> by the team of famed computer pioneer <a href="https://spectrum.ieee.org/tag/doug-engelbart" target="_blank">Douglas Engelbart</a>, to a smaller company, <a href="#lotus">Lotus, that hit the market with its groupware</a> program, Notes, at the right time, to Microsoft’s <a href="#microsoft">ill-fated attempt to enter the groupware market</a>, including
	<a href="#gatesvideo">never before seen footage of Bill Gates on Broadway</a>.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A black and white photo of an old IBM PC on a desk next to computer manuals" class="rm-shortcode" data-rm-shortcode-id="e66f43e23df78482479cfa191615ed5b" data-rm-shortcode-name="rebelmouse-image" id="2e3cb" loading="lazy" src="https://spectrum.ieee.org/media-library/a-black-and-white-photo-of-an-old-ibm-pc-on-a-desk-next-to-computer-manuals.jpg?id=52963525&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">In the early days of the computing era, when IBM’s PC reigned supreme, collaboration was difficult. </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Ross Anthony Willis/Fairfax Media/Getty Images</small>
</p><h2 class="rm-anchors" id="collaboration">
How the PC made us forget about collaboration for a while
</h2><p>
	Imagine that it’s the early-to-mid-1980s and that you run a large company. You’ve invested a lot of money into personal computers, which your employees are now using—IBM PCs, Apple Macintoshes, clones, and the like. There’s just one problem: You have a bunch of computers, but they don’t talk to one another.
</p><p>
	If you’re in a small office and need to share a file, it’s no big deal: You can just hand a floppy disk off to someone on the other side of the room. But what if you’re part of an enterprise company and the person you need to collaborate with is on the other side of the country? Passing your colleague a disk doesn’t work.
</p><p>
	The new personal-computing technologies clearly needed to do more to foster collaboration. They needed to be able to take input from a large group of people inside an office, to allow files to be shared and distributed, and to let multiple users tweak and mash information with everyone being able to sign off on the final version.
</p><p>
	The hardware that would enable such collaboration software, or “groupware” as it tended to be called early on, varied by era. In the 1960s and ’70s, it was usually a mainframe-to-terminal setup, rather than something using PCs. Later, in the 1980s, it was either a
	<a href="https://www.techtarget.com/searchnetworking/definition/Token-Ring" rel="noopener noreferrer" target="_blank"> token ring</a> or <a href="https://www.spiceworks.com/tech/networking/articles/what-is-ethernet/#:~:text=Ethernet%20Connections,coaxial%20or%20fiber%20optic%20cables." rel="noopener noreferrer" target="_blank">Ethernet</a> network, which were competing local-networking technologies. But regardless of the hardware used for networking, the software for collaboration needed to be developed.
</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="Black and white photo of a man talking from behind a desk. " class="rm-shortcode rm-resized-image" data-rm-shortcode-id="8e966d3201c8bed1027ae3e40f483dd4" data-rm-shortcode-name="rebelmouse-image" id="ff3b1" loading="lazy" src="https://spectrum.ieee.org/media-library/black-and-white-photo-of-a-man-talking-from-behind-a-desk.jpg?id=52963528&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..." style="max-width: 100%;">Stanford Research Institute engineer Douglas Engelbart is <a href="https://dougengelbart.org/content/view/157/" rel="noopener noreferrer" target="_blank">sometimes called</a> “the father of groupware.”</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..." style="max-width: 100%;">Getty Images</small>
</p><p>
	Some of the basic ideas behind groupware
	<a href="https://www.internationaljournalcorner.com/index.php/ijird_ojs/article/view/136445/95568" rel="noopener noreferrer" target="_blank">were first forged</a> at the Stanford Research Institute by a Douglas Engelbart–led team, in the 1960s, working on what they called an <a href="https://www.darpa.mil/about-us/timeline/nls" rel="noopener noreferrer" target="_blank">oN-Line System (NLS)</a>. An early version of NLS was presented in 1968 during what became known as the “<a href="https://en.wikipedia.org/wiki/The_Mother_of_All_Demos" rel="noopener noreferrer" target="_blank">Mother of All Demos</a>.” It was essentially a coming-out party for many computing innovations that would eventually become commonplace. If you have 90 minutes and want to see something 20-plus years ahead of its time, <a href="https://www.youtube.com/watch?v=yJDv-zdhzMY" rel="noopener noreferrer" target="_blank">watch this video</a>.
</p><p>
	In the years that followed, on top of well-known innovations like the mouse, Engelbart’s team developed tools that anticipated groupware, including an “<a href="https://www.youtube.com/watch?v=AzCoNB4JKRU&t=1596s" rel="noopener noreferrer" target="_blank">information center</a>,” an early precursor of the server in a <a href="https://en.wikipedia.org/wiki/Client%E2%80%93server_model#:~:text=A%20client%20usually%20does%20not,and%20the%20World%20Wide%20Web." rel="noopener noreferrer" target="_blank">client-server architecture</a>, and <a href="https://www.youtube.com/watch?v=Odv_vNFGOX4&t=1634s" rel="noopener noreferrer" target="_blank">tracking edits made to text files by different people</a>, an early precursor of <a href="https://en.wikipedia.org/wiki/Version_control" rel="noopener noreferrer" target="_blank">version control</a>.
</p><p>
	By the late 1980s, at a point when the PC had begun to dominate the workplace, Engelbart was less impressed with what had been gained than with what had been lost in the process. He
	<a href="https://www.dougengelbart.org/pubs/seminars/sembinder1992nov/R.pdf" rel="noopener noreferrer" target="_blank">wrote (with Harvey Lehtman) in <em><em>Byte</em></em> magazine in 1988</a>:
</p><blockquote>
	The emergence of the personal computer as a major presence in the 1970s and 1980s led to tremendous increases in personal productivity and creativity. It also caused setbacks in the development of tools aimed at increasing organizational effectiveness—tools developed on the older time-sharing systems.
	<br/>
	To some extent, the personal computer was a reaction to the overloaded and frustrating time-sharing systems of the day. In emphasizing the power of the individual, the personal computer revolution turned its back on those tools that led to the empowering of both co-located and distributed work groups collaborating simultaneously and over time on common knowledge work.
	<br/>
	The introduction of local- and wide-area networks into the personal computer environment and the development of mail systems are leading toward some of the directions explored on the earlier systems. However, some of the experiences of those earlier pioneering systems should be considered anew in evolving newer collaborative environments.
</blockquote><p class="caption">
<a href="#top">Back to top</a>
</p><h2 class="rm-anchors" id="lotus">Groupware comes of age</h2><p>
	Groupware finally started to catch on in the late 1980s, with tech companies putting considerable resources into developing collaboration software—perhaps taken in by the idea of “orchestrating work teams,” as
	<a href="https://books.google.com/books?id=pDsEAAAAMBAJ&pg=PA39#v=onepage&q&f=false" rel="noopener noreferrer" target="_blank">an <em><em>Infoworld</em></em> piece characterized the challenge in 1988</a>. The <em><em>San Francisco Examiner </em></em><a href="https://www.newspapers.com/article/the-san-francisco-examiner-early-story-a/151071223/" rel="noopener noreferrer" target="_blank">reported</a>, for example, that General Motors had invested in the technology, and was beginning to require its suppliers to accept purchase orders electronically.
</p><p>
	Focusing on collaboration software was a great way for independent software companies to stand out, this being an area that large companies—<a href="https://www.microsoft.com/en-us" rel="noopener noreferrer" target="_blank">Microsoft</a> in particular—had basically ignored. Today, Microsoft is the <a href="https://www.windowscentral.com/software-apps/microsoft-teams-surpasses-320-million-active-monthly-users-amid-pressure-from-rivals-to-unbundle-the-service-from-office" rel="noopener noreferrer" target="_blank">800-pound gorilla of collaboration</a> software, thanks to its combination of <a href="https://www.microsoft.com/en-us/microsoft-teams/group-chat-software" rel="noopener noreferrer" target="_blank">Teams</a> and <a href="https://www.microsoft.com/en-us/microsoft-365" rel="noopener noreferrer" target="_blank">Office 365</a>. But it took the tech giant a very long while to get there: Microsoft started <a href="https://books.google.com/books?id=acxkQcYK_cUC&pg=PP4#v=onepage&q&f=false" rel="noopener noreferrer" target="_blank">taking the market seriously only around 1992</a>.
</p><p>
	One company in particular was well-positioned to take advantage of the opening that existed in the 1980s. That was the
	<a href="https://en.wikipedia.org/wiki/Lotus_Software" rel="noopener noreferrer" target="_blank"> Lotus Development Corporation</a>, a Cambridge, Mass.–based software company that made its name with its <a href="https://en.wikipedia.org/wiki/Lotus_1-2-3" rel="noopener noreferrer" target="_blank">Lotus 1-2-3</a> spreadsheet program for IBM PCs.
</p><p>
	Lotus did not invent groupware or coin the word—on top of Engelbart’s formative work at Stanford, the term
	<a href="https://dl.acm.org/doi/pdf/10.1145/290575.290585" rel="noopener noreferrer" target="_blank"> had been around</a> for years before Lotus Notes came on the scene. But it was the company that brought collaboration software to everyone’s attention.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="On the left, a black and white photo of a man in a field talking. On the right, a box with disks." class="rm-shortcode" data-rm-shortcode-id="e6b83decd22f0743cf180e92436c4a78" data-rm-shortcode-name="rebelmouse-image" id="0a13a" loading="lazy" src="https://spectrum.ieee.org/media-library/on-the-left-a-black-and-white-photo-of-a-man-in-a-field-talking-on-the-right-a-box-with-disks.jpg?id=52963539&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Ray Ozzie [left] was primarily responsible for the development of Lotus Notes, the first popular groupware solution.</small>
<small class="image-media media-photo-credit" placeholder="Add Photo Credit...">
	Left: Ann E. Yow-Dyson/Getty Images; Right: James Keyser/Getty Images
	</small>
</p><p>
	The person most associated with the development of Notes was
	<a href="https://en.wikipedia.org/wiki/Ray_Ozzie" rel="noopener noreferrer" target="_blank"> Ray Ozzie</a>, who was recruited to <a href="https://spectrum.ieee.org/pride-in-tech-how-lotus-championed-gay-rights-during-the-aids-crisis" target="_self">Lotus</a> after spending time working on VisiCalc, an early spreadsheet program. Ozzie essentially built out what became Notes while <a href="https://computerhistory.org/profile/raymond-ozzie/" rel="noopener noreferrer" target="_blank">working at Iris Associates</a>, a direct offshoot of Lotus that Ozzie founded to develop the <a href="https://www.notesmail.com/home.nsf/ls-NDHistory-pdf.pdf" rel="noopener noreferrer" target="_blank">Notes application</a>. After some years of development in stealth mode, the product was released in 1989.
</p><p>
	Ozzie explained his inspiration for Notes to Jessica Livingston, who described this history in her book,
	<a href="http://www.foundersatwork.com/" rel="noopener noreferrer" target="_blank"><em><em>Founders At Work</em></em></a>:
</p><blockquote>
	In Notes, it was (and this is hard to imagine because it was a different time) the concept that we’d all be using computers on our desktops, and therefore we might want to use them as communication tools. This was a time when PCs were just emerging as spreadsheet tools and word processing replacements, still available only on a subset of desks, and definitely no networks. It was ’82 when I wrote the specs for it. It had been based on a system called PLATO [Programmed Logic for Automatic Teaching Operations] that I’d been exposed to at college, which was a large-scale interactive system that people did learning and interactive gaming on, and things like that. It gave us a little bit of a peek at the future—what it would be like if we all had access to interactive systems and technology.
</blockquote><p>
	Building an application based on PLATO turned out to be the right idea at the right time, and it gave Lotus an edge in the market. Notes included email, a calendaring and scheduling tool, an address book, a shared database, and programming capabilities, all in a single front-end application.
</p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="f2fabcd8e6f1bda7e8e71ce1bc0c49df" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/9MAPPum19d0?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Lotus Notes on Computer Chronicles Fall 1989</small>
</p><p>
	As an all-in-one platform built for scale, Notes
	<a href="https://books.google.com/books?id=ZwYAAAAAMBAJ&pg=PT4#v=onepage&q&f=false" rel="noopener noreferrer" target="_blank"> gained a strong reputation</a> as an early example of what today would be called a business-transformation tool, one that managed many elements of collaboration. It was <a href="https://books.google.com/books?id=lxz4-U4f7QMC&pg=PA112" target="_blank">complicated from an IT standpoint</a> and required a significant investment to maintain. In a way, what Notes did that was perhaps most groundbreaking was that it helped turn PCs into something that large companies could readily use.<br/>
</p><p>
<a href="https://money.cnn.com/magazines/fortune/fortune_archive/1994/12/12/80046/index.htm" rel="noopener noreferrer" target="_blank">As <em><em>Fortune</em></em> noted in 1994</a>, Lotus had a massive lead in the groupware space, in part because the software worked essentially the same anywhere in a company’s network. We take that for granted now, but back then it was considered magical:
</p><blockquote>
	Like Lotus 1-2-3, Notes is easy to customize. A sales organization, for instance, might use it to set up an electronic bulletin board that lets people pool information about prospective clients. If some of the info is confidential, it can be restricted so not everyone can call it up.
	<br/>
	Notes makes such homegrown applications and the data they contain accessible throughout an organization. The electronic bulletin board you consult in Singapore is identical to the one your counterparts see in Sioux City, Iowa. The key to this universality is a procedure called replication, by which Notes copies information from computer to computer throughout the network. You might say Ozzie figured out how to make the machines telepathic—each knows what the others are thinking.
</blockquote><p>
	This article reported that around 4,000 major companies had purchased Notes, including
	<a href="https://www.chase.com/" rel="noopener noreferrer" target="_blank"> Chase Manhattan</a>, <a href="https://www.compaq.com/" rel="noopener noreferrer" target="_blank">Compaq Computer</a>,<a href="https://www.delta.com/" rel="noopener noreferrer" target="_blank"> Delta Air Lines</a>,<a href="https://www.fluor.com/" rel="noopener noreferrer" target="_blank"> Fluor</a>, <a href="https://www.gm.com/" rel="noopener noreferrer" target="_blank">General Motors</a>, <a href="https://www.harley-davidson.com/us/en/index.html" rel="noopener noreferrer" target="_blank">Harley-Davidson</a>, <a href="https://www.hpe.com/us/en/home.html" rel="noopener noreferrer" target="_blank">Hewlett-Packard</a>, <a href="https://www.ibm.com/us-en" rel="noopener noreferrer" target="_blank">IBM</a>, <a href="https://www.jnj.com/" rel="noopener noreferrer" target="_blank">Johnson & Johnson</a>, <a href="https://www.jpmorganchase.com/" rel="noopener noreferrer" target="_blank">J.P. Morgan</a>, <a href="https://en.wikipedia.org/wiki/NYNEX" rel="noopener noreferrer" target="_blank">Nynex</a>, <a href="https://en.wikipedia.org/wiki/Sybase" rel="noopener noreferrer" target="_blank">Sybase</a>, and <a href="https://www.3m.com/" rel="noopener noreferrer" target="_blank">3M</a>. While it wasn’t dominant in the way Windows was, its momentum was hard to ignore.
</p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="d5e5995ff687dadf07f0b2f88ac65a63" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/XN95z7Z50no?rel=0&start=1" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
<small class="image-media media-caption" placeholder="Add Photo Caption...">A 1996 commercial for Notes highlighted its use by FedEx. Other commercials would use the stand-up comedian Denis Leary or be highly conceptual. Rarely, if ever, would these television advertisements show the software.</small>
</p><p>
	In the mid-1990’s, it was common for magazines to publish stories about how Notes reshaped businesses large and small.
	<a href="https://www.inc.com/magazine/19960201/1573.html" rel="noopener noreferrer" target="_blank">A 1996 <em><em>Inc.</em></em> piece</a>, for example, described how a natural-foods company successfully produced a new product in just eight months, a feat the company directly credited to Notes.<br/>
</p><p>
	“It’s become our general manager,” Groveland Trading Co. president Steve McDonnell recalled.
</p><p>
	Notes wasn’t cheap (<a href="https://books.google.com/books?id=ezAEAAAAMBAJ&pg=PA43" rel="noopener noreferrer" target="_blank">InfoWorld lists</a> the price circa 1990 as US $62,000), and it was complicated to manage. But the positive results it enabled were immensely hard to ignore. IBM noticed and ended up buying Lotus in 1995, <a href="https://www.latimes.com/archives/la-xpm-1995-06-12-mn-12346-story.html" rel="noopener noreferrer" target="_blank">almost entirely to get ahold of Notes</a>. Even earlier, Microsoft had realized that office collaboration was a big deal, and they wanted in.
</p><p class="caption">
<a href="#top">Back to top</a>
</p><h2 class="rm-anchors" id="microsoft">
Microsoft jumps on the groupware bandwagon
</h2><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="White old book on yellow background titled Microsoft Workgroup Add-on for Windows" class="rm-shortcode" data-rm-shortcode-id="3ccf23bbce782010b1a3f5980a6d8add" data-rm-shortcode-name="rebelmouse-image" id="726e4" loading="lazy" src="https://spectrum.ieee.org/media-library/white-old-book-on-yellow-background-titled-microsoft-workgroup-add-on-for-windows.jpg?id=52963545&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Microsoft’s first foray into collaboration software was its 1992 release of Windows for Workgroups. Despite great efforts to promote the release, the software was not a commercial success. </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..."><a href="https://www.flickr.com/photos/daltoris/5574403868" rel="noopener noreferrer" target="_blank">Daltrois/Flickr</a></small>
</p><p>
	Microsoft had high hopes for
	<a href="https://en.wikipedia.org/wiki/Windows_3.1#Windows_for_Workgroups_3.11" rel="noopener noreferrer" target="_blank"> Windows for Workgroups</a>, the networking-focused variant of its popular <a href="https://en.wikipedia.org/wiki/Windows_3.1" rel="noopener noreferrer" target="_blank">Windows 3.1</a> software suite. To create buzz for it, the company pulled out all the stops. Seriously.
</p><p class="rm-anchors" id="gatesvideo">
	In the fall of 1992, Microsoft
	<a href="https://books.google.com/books?id=hVEEAAAAMBAJ&pg=PA103" rel="noopener noreferrer" target="_blank">paid something like $2 million</a> to put on a Broadway production with Bill Gates literally center stage, at New York City’s Gershwin Theater, one of the largest on Broadway. It was a wild show, and yet, somehow, there is no video of this event currently posted online—until now. The only person I know of who has a video recording of this extravaganza is, fittingly enough, Ray Ozzie, the groupware guru and Notes inventor. Ozzie later served as a top executive at Microsoft, famously replacing Bill Gates as Chief Software Architect in the mid-2000s, and he has shared this video with us for this post:
</p><h3></h3><br/><span class="rm-shortcode" data-rm-shortcode-id="70a45f24e3afc0fe3e67df02edc6d92c" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/pGvMcE7o1KI?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span><p class="caption">
	The 1992 one-day event was not a hit. Watch to see why. (Courtesy of Ray Ozzie and the Microsoft Corporation)
</p><p class="caption">
	00:00 Opening number
	<br/>
	02:23 “My VGA can hardly wait for your CPU to reciprocate”
	<br/>
	05:17 Bill Gates enters the stage
	<br/>
	27:55 “Get ready, get set” musical number
	<br/>
	31:50 Bit with Mike Appe, Microsoft VP of sales
	<br/>
	58:30 Bill Gates does jumping jacks
</p><p class="caption">
<a href="#top">Back to top</a>
</p><p>
<a href="https://www.washingtonpost.com/archive/business/1992/11/02/now-windows-the-musical/e3225174-576a-4a65-abad-9cf462d8ff9b/" rel="noopener noreferrer" target="_blank">A 1992 <em><em>Washington Post</em></em> article</a> describes the performance, which involved dozens of actors, some of whom were dressed like the Blues Brothers. At one point, Gates did jumping jacks. <a href="https://archive.org/details/howwebwaswonmic00andr/page/92/mode/2up" rel="noopener noreferrer" target="_blank">Gates himself later said</a>, “That was so bad, I thought [then Microsoft CEO] Ballmer was going to retch.” For those who don’t have an extra hour to spend, here is a summary:
</p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="bd84c728a6e6a04209fc700d7ec0c1bf" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/TNRqzPCusfY?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
<small class="image-media media-caption" placeholder="Add Photo Caption...">To get a taste of the show, watch this news segment from channel 4.</small>
<small class="image-media media-photo-credit" placeholder="Add Photo Credit...">
	Courtesy of Microsoft Corporate Archives
	</small>
</p><p>
	Despite all the effort to generate fanfare, Windows for Workgroups was
	<a href="https://books.google.com/books?id=Xxmz8KfZs-IC&pg=PA39#v=onepage&q&f=false" rel="noopener noreferrer" target="_blank"> not a hit</a>. While Windows 3.1 was dominant, Microsoft had built a program that didn’t seem to capture the burgeoning interest in collaborative work in a real way. Among other things, it didn’t initially support the <a href="https://www.techtarget.com/searchnetworking/definition/TCP-IP#:~:text=TCP%2FIP%20stands%20for%20Transmission,network%20devices%20on%20the%20internet." target="_blank">TCP/IP</a> networking protocol, despite the fact that it was the networking technology that was winning the market and enabled the rise of the Internet.<br/>
</p><p>
	In its original version, Windows for Workgroups carried such a negative reputation in Microsoft’s own headquarters that the company nicknamed it
	<a href="https://devblogs.microsoft.com/oldnewthing/20190625-00/?p=102616" rel="noopener noreferrer" target="_blank"> Windows for Warehouses</a>, referring to the company’s largely unsold inventory, according to Microsoft’s own expert on company lore, Raymond Chen.
</p><p>
	Unsuccessful as it was, the fact that it existed in the first place hinted at Microsoft’s general acknowledgement that perhaps this networking thing was going to catch on with its users.
</p><p>
	Launched in late 1992, a few months after Windows 3.1 itself, the product was Microsoft’s
	<a href="https://books.google.com/books?id=F6anv43MXVcC&lpg=PP1&pg=PA255#v=onepage&q&f=false" rel="noopener noreferrer" target="_blank"> first attempt at integrated networking</a> in a Windows package. The software enabled file-sharing across servers, printer sharing, and email—table stakes in the modern day but at the time a big deal.
</p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="1873d1119dc76e3138ba24dc0d1c0e66" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/KyY-zXJ_1y0?rel=0&start=8" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
<small class="image-media media-caption" placeholder="Add Photo Caption...">This video presents a very accurate view of what it was like to use Windows in 1994.</small>
</p><p>
	Unfortunately, it was a big deal that came a few years late. Microsoft itself was
	<a href="https://books.google.com/books?id=ZhwEAAAAMBAJ&pg=PA47#v=onepage&q&f=false" rel="noopener noreferrer" target="_blank"> so lukewarm</a> on the product that the company had to update it to Windows for Workgroups 3.11 just a year later, whose marquee feature wasn’t improved network support but <a href="https://books.google.com/books?id=nDgEAAAAMBAJ&pg=PA66" target="_blank">increased disk speed</a>. Confusingly, the company had just released Windows NT by this point, a program that better matched the needs of enterprise customers.<br/>
</p><p>
	The work group terminology Microsoft introduced with Windows for Workgroups stuck around, though, and it is actually
	<a href="https://www.tomshardware.com/news/how-to-set-up-join-workgroup-windows-10,36843.html" rel="noopener noreferrer" target="_blank"> used in Windows</a> to this day.
</p><p>
	In 2024, group-oriented software feels like the default paradigm, with single-user apps being the anomaly. Over time, groupware became so pervasive that people no longer think of it as groupware, though there are plenty of big, hefty, groupware-like tools out there, like
	<a href="https://www.salesforce.com/" rel="noopener noreferrer" target="_blank">Salesforce</a>. Now, it’s just software. But no one should forget the long history of collaboration software or its ongoing value. It’s what got most of us through the pandemic, even if we never used the word “groupware” to describe it. <span class="ieee-end-mark"></span>
</p>]]></description><pubDate>Wed, 24 Jul 2024 15:00:03 +0000</pubDate><guid>https://spectrum.ieee.org/groupware</guid><category>Collaboration software</category><category>Groupware</category><category>Lotus</category><category>Microsoft</category><category>Tedium</category><category>Bill gates</category><category>Doug engelbart</category><dc:creator>Ernie Smith</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-photo-illustration-showing-3-people-working-on-1980s-era-computers-with-lines-and-arrows-connecting-them.jpg?id=52963507&amp;width=980"></media:content></item><item><title>Quantum Leap: Sydney’s Leading Role in the Next Tech Wave</title><link>https://spectrum.ieee.org/quantum-technology-sydney</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-group-of-people-in-a-research-lab-stand-around-a-quantum-device-consisting-of-metal-chambers-pipes-and-wires.jpg?id=52517557&width=1200&height=800&coordinates=0%2C0%2C0%2C0"/><br/><br/><p><em>This is a sponsored article brought to you by </em><a href="https://hello.besydney.com.au/l/315211/2024-06-25/n2jn2" rel="noopener noreferrer" target="_blank">BESydney</a><em>.</em></p><p>Australia plays a crucial role in global scientific endeavours, with a significant contribution recognized and valued worldwide. Despite comprising only 0.3 percent of the world’s population, it has contributed over 4 percent of the world’s published research. </p><p>Renowned for collaboration, Australian scientists work across disciplines and with international counterparts to achieve impactful outcomes. Notably excelling in medical sciences, engineering, and biological sciences, Australia also has globally recognized expertise in astronomy, physics and computer science. </p><p>As the country’s innovation hub and leveraging its robust scientific infrastructure, world-class universities and vibrant ecosystem, Sydney is making its mark on this burgeoning industry. </p><p>The city’s commitment to quantum research and development is evidenced by its groundbreaking advancements and substantial government support, positioning it at the forefront of the quantum revolution. </p><p>Sydney’s blend of academic excellence, industry collaboration and strategic government initiatives is creating a fertile ground for cutting-edge quantum advancements. </p><h2>Sydney’s quantum ecosystem</h2><p>Sydney’s quantum industry is bolstered by the Sydney Quantum Academy (SQA), a collaboration between four top-tier universities: University of NSW Sydney (UNSW Sydney), the University of Sydney (USYD), University of Technology Sydney (UTS), and Macquarie University. SQA integrates over 100 experts, fostering a dynamic quantum research and development environment. </p><p>With strong government backing Sydney is poised for significant growth in quantum technology, with a projected A$2.2 billion industry value and 8,700 jobs by 2030. The SQA’s mission is to cultivate a quantum-literate workforce, support industry partnerships and accelerate the development of quantum technology. </p><p>Professor Hugh Durrant-Whyte, NSW Chief Scientist and Engineer, emphasizes Sydney’s unique position: “We’ve invested in quantum for 20 years, and we have some of the best people at the Quantum Academy in Sydney. This investment and talent pool make Sydney an ideal place for pioneering quantum research and attracting global talent.” </p><h2>Key institutions and innovations</h2><p>UNSW’s Centre of Excellence for Quantum Computation and Communication Technology is at the heart of Sydney’s quantum advancements. Led by Scientia Professor Michelle Simmons AO, the founder and CEO of Silicon Quantum Computing, this centre is pioneering efforts to develop the world’s first practical supercomputer. This team is at the vanguard of precision atomic electronics, pioneering the fabrication of devices in silicon that are pivotal for both conventional and quantum computing applications and they have created the narrowest conducting wires and the smallest precision transistors. </p><p class="pull-quote">“We can now not only put atoms in place but can connect complete circuitry with atomic precision.” <strong>—Michelle Simmons, Silicon Quantum Computing</strong></p><p>Simmons was named 2018 Australian of the Year and won the 2023 Prime Minister’s Prize for Science for her work in creating the new field of atomic electronics. She is an Australian Research Council Laureate Fellow, a Fellow of the Royal Society of London, the American Academy of Arts and Science, the American Association of the Advancement of Science, the UK Institute of Physics, the Australian Academy of Technology and Engineering and the Australian Academy of Science.</p><p>In response to her 2023 accolade, Simmons said: “Twenty years ago, the ability to manipulate individual atoms and put them where we want in a device architecture was unimaginable. We can now not only put atoms in place but can connect complete circuitry with atomic precision—a capability that was developed entirely in Australia.” </p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="Standing in a modern research lab with glass walls and wooden lab benches, a man grasps a cylindrical object attached to a robot arm's gripper while a woman operates a control touch-interface tablet." class="rm-shortcode" data-rm-shortcode-id="ed4aab9fc27b2659427118b7aa0a526a" data-rm-shortcode-name="rebelmouse-image" id="ced8d" loading="lazy" src="https://spectrum.ieee.org/media-library/standing-in-a-modern-research-lab-with-glass-walls-and-wooden-lab-benches-a-man-grasps-a-cylindrical-object-attached-to-a-robot.jpg?id=52540229&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">The Design Futures Lab at UNSW in Sydney, Australia, is a hands-on teaching and research lab that aims to inspire exploration, innovation, and research into fabrication, emerging technologies, and design theories.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">UNSW</small></p><h2>Government and industry support</h2><p>In April 2024, the Australian Centre for Quantum Growth program, part of the National Quantum Strategy, provided a substantial four-year grant to support the quantum industry’s expansion in Australia. Managed by the University of Sydney, the initiative aims to establish a central hub that fosters industry growth, collaboration, and research coordination. </p><p>This centre will serve as a primary resource for the quantum sector, enhancing Australia’s global competitiveness by promoting industry-led solutions and advancing technology adoption both domestically and internationally. Additionally, the centre will emphasise ethical practices and security in the development and application of quantum technologies. </p><p>Additionally, Sydney hosts several leading quantum startups, such as Silicon Quantum Computing, Quantum Brilliance, Diraq and Q-CTRL, which focus on improving the performance and stability of quantum systems. </p><h2>Educational excellence </h2><p>Sydney’s universities are globally recognized for their contributions to quantum research. They nurture future quantum leaders, and their academic prowess attracts top talent and fosters a culture of innovation and collaboration.</p><p class="pull-quote">Sydney hosts several leading quantum startups, such as Silicon Quantum Computing, Quantum Brilliance, Diraq, and Q-CTRL, which focus on improving the performance and stability of quantum systems.</p><p>The UNSW Sydney is, one of Sydney’s universities, ranked among the world’s top 20 universities, and boasts the largest concentration of academics working in AI and quantum technologies in Australia.</p><p>UNSW Sydney Professor Toby Walsh is Laureate Fellow and Scientia Professor of Artificial Intelligence at the Department of Computer Science and Engineering at the University of New South Wales. He explains the significance of this academic strength: “Our students and researchers are at the cutting edge of quantum science. The collaborative efforts within Sydney’s academic institutions are creating a powerhouse of innovation that is driving the global quantum agenda.” </p><p>Sydney’s strategic investments and collaborative efforts in quantum technology have propelled the city to the forefront of this transformative field. With its unique and vibrant ecosystem, a blend of world-leading institutions, globally respected talent and strong government and industry support, Sydney is well-positioned to lead the global quantum revolution for the benefit of all. For more information on <strong>Sydney’s science and engineering industries</strong> visit <a href="https://hello.besydney.com.au/l/315211/2024-06-25/n2jn2" rel="noopener noreferrer" target="_blank">besydney.com.au.</a></p>]]></description><pubDate>Wed, 24 Jul 2024 00:00:02 +0000</pubDate><guid>https://spectrum.ieee.org/quantum-technology-sydney</guid><category>Quantum computing</category><category>Startups</category><category>Australia</category><category>Innovation</category><category>Type:sponsored</category><dc:creator>BESydney</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-group-of-people-in-a-research-lab-stand-around-a-quantum-device-consisting-of-metal-chambers-pipes-and-wires.jpg?id=52517557&amp;width=980"></media:content></item><item><title>Elephant Robotics’ Mercury Humanoid Robot Empowers Embodied AI Research</title><link>https://spectrum.ieee.org/elephant-robotics-mercury</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/two-grouped-photos-showing-a-winking-robot-standing-in-a-room-and-another-smiling-robot-in-a-kitchen-area.jpg?id=52857053&width=1200&height=800&coordinates=150%2C0%2C150%2C0"/><br/><br/><p><em>
	This is a sponsored article brought to you by <a href="https://www.elephantrobotics.com/en/" target="_blank">Elephant Robotics</a>.</em></p><p><a href="https://www.elephantrobotics.com/en/" rel="noopener noreferrer" target="_blank">Elephant Robotics</a> has gone through years of research and development to accelerate its mission of bringing robots to millions of homes and a vision of “Enjoy Robots World”. From the collaborative industrial robots P-series and C-series, which have been on the drawing board since its establishment in 2016, to the lightweight desktop 6 DOF collaborative robot <a href="https://shop.elephantrobotics.com/collections/mycobot-280" rel="noopener noreferrer" target="_blank">myCobot 280</a> in 2020, to the dual-armed, semi-humanoid robot <a href="https://shop.elephantrobotics.com/collections/mybuddy/products/mybuddy-280" rel="noopener noreferrer" target="_blank">myBuddy</a>, which was launched in 2022, Elephant Robotics is launching 3-5 robots per year, and this year’s full-body humanoid robot, the <a href="https://www.elephantrobotics.com/en/mercury-humanoid-robot/" rel="noopener noreferrer" target="_blank">Mercury series</a>, promises to reshape the landscape of non-human workers, introducing intelligent robots like Mercury into research and education and even everyday home environments.</p><h2>A Commitment to Practical Robotics</h2><p>
<a href="https://shop.elephantrobotics.com/products/mercury-humanoid-robot-series?_pos=1&_psq=mercury&_ss=e&_v=1.0&variant=47556966875448" rel="noopener noreferrer" target="_blank">Elephant Robotics</a> proudly introduces the Mercury Series, a suite of humanoid robots that not only push the boundaries of innovation but also embody a deep commitment to practical applications. Designed with the future of robotics in mind, the Mercury Series is poised to become the go-to choice for researchers and industry professionals seeking reliable, scalable, and robust solutions.
</p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="20a19f4fa2a474864196ce49dc15f94c" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/Ru24sDmK8yI?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
<small class="image-media media-caption" placeholder="Add Photo Caption..."><u><br/></u></small>
<small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Elephant Robotics</small></p><h2>The Genesis of Mercury Series: Bridging Vision With Practicality</h2><p>
	From the outset, the Mercury Series has been envisioned as more than just a collection of advanced prototypes. It is a testament to Elephant Robotics’ dedication to creating humanoid robots that are not only groundbreaking in their capabilities but also practical for mass production and consistent, reliable use in real-world applications.
</p><h2>Mercury X1: Wheeled Humanoid Robot</h2><p>
<a href="https://www.elephantrobotics.com/en/mercury-x1-en/" rel="noopener noreferrer" target="_blank">The Mercury X1</a> is a versatile wheeled humanoid robot that combines advanced functionalities with mobility. Equipped with dual NVIDIA Jetson controllers, lidar, ultrasonic sensors, and an 8-hour battery life, the X1 is perfect for a wide range of applications, from exploratory studies to commercial tasks requiring mobility and adaptability.
</p><h2>Mercury B1: Dual-Arm Semi-Humanoid Robot</h2><p>
<a href="https://www.elephantrobotics.com/en/mercury-b1-en/" rel="noopener noreferrer" target="_blank">The Mercury B1</a> is a semi-humanoid robot tailored for sophisticated research. It features 17 degrees of freedom, dual robotic arms, a 9-inch touchscreen, a NVIDIA Xavier control chip, and an integrated 3D camera. The B1 excels in machine vision and VR-assisted teleoperation, and its AI voice interaction and LLM integration mark significant advancements in human-robot communication.
</p><p>
	These two advanced models exemplify Elephant Robotics’ commitment to practical robotics. The wheeled humanoid robot Mercury X1 integrates advanced technology with a state-of-the-art mobile platform, ensuring not only versatility but also the feasibility of large-scale production and deployment.
</p><h2>Embracing the Power of Reliable Embodied AI</h2><p>
The Mercury Series is engineered as the ideal hardware platform for embodied AI research, providing robust support for sophisticated AI algorithms and real-world applications. Elephant Robotics demonstrates its commitment to innovation through the Mercury series’ compatibility with NVIDIA’s ISSACSIM, a state-of-the-art simulation platform that facilitates sim2real learning, bridging the gap between virtual environments and physical robot interaction.
</p><p>
	The Mercury Series is perfectly suited for the study and experimentation of mainstream large language models in embodied AI. Its advanced capabilities allow seamless integration with the latest AI research. This provides a reliable and scalable platform for exploring the frontiers of machine learning and robotics.
</p><p>
Furthermore, the Mercury Series is complemented by the <a href="https://shop.elephantrobotics.com/collections/myarm-mc/products/myarm-c650" target="_blank">myArm C650</a>, a teleoperation robotic arm that enables rapid acquisition of physical data. This feature supports secondary learning and adaptation, allowing for immediate feedback and iterative improvements in real-time. These features, combined with the Mercury Series’ reliability and practicality, make it the preferred hardware platform for researchers and institutions looking to advance the field of embodied AI.
</p><p>
The Mercury Series is supported by a rich software ecosystem, compatible with major programming languages, and integrates seamlessly with industry-standard simulation software. This comprehensive development environment is enhanced by a range of auxiliary hardware, all designed with mass production practicality in mind.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A set of images showing a robot in a variety of situations." class="rm-shortcode" data-rm-shortcode-id="0f090fc8addd438aa868f01e910606e3" data-rm-shortcode-name="rebelmouse-image" id="654a8" loading="lazy" src="https://spectrum.ieee.org/media-library/a-set-of-images-showing-a-robot-in-a-variety-of-situations.jpg?id=52857217&width=980"/>
<small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Elephant Robotics</small></p><h2>Drive to Innovate: Mass Production and Global Benchmarks</h2><p>
The “Power Spring” harmonic drive modules, a hallmark of the Elephant Robotics’ commitment to innovation for mass production, have been meticulously engineered to offer an unparalleled torque-to-weight ratio. These components are a testament to the company’s foresight in addressing the practicalities of large-scale manufacturing. The incorporation of carbon fiber in the design of these modules not only optimizes agility and power but also ensures that the robots are well-prepared for the rigors of the production line and real-world applications. The Mercury Series, with its spirit of innovation, is making a significant global impact, setting a new benchmark for what practical robotics can achieve.
</p><p>
Elephant Robotics is consistently delivering mass-produced robots to a range of renowned institutions and industry leaders, thereby redefining the industry standards for reliability and scalability. The company’s dedication to providing more than mere prototypes is evident in the active role its robots play in various sectors, transforming industries that are in search of dependable and efficient robotic solutions.
</p><h2>Conclusion: The Mercury Series—A Beacon for the Future of Practical Robotics</h2><p>
The Mercury Series represents more than a product; it is a beacon for the future of practical robotics. <a href="https://shop.elephantrobotics.com/" rel="noopener noreferrer" target="_blank">Elephant Robotics’</a> dedication to affordability, accessibility, and technological advancement ensures that the Mercury Series is not just a research tool but a platform for real-world impact.
</p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="175347c43067d1bd4f7f1530d6dbcb91" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/gKJXL0IXeUs?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Mercury Usecases | Explore the Capabilities of the Wheeled Humanoid Robot and Discover Its Precision</small>
<small class="image-media media-photo-credit" placeholder="Add Photo Credit...">
<a href="https://youtu.be/gKJXL0IXeUs" target="_blank">youtu.be</a>
</small>
</p><p>
<strong>Elephant Robotics:</strong> <a href="https://www.elephantrobotics.com/en/" rel="noopener noreferrer" target="_blank">https://www.elephantrobotics.com/en/</a>
</p><p>
<strong>Mercury Robot Series: </strong><u><a href="https://www.elephantrobotics.com/en/mercury-humanoid-robot/" rel="noopener noreferrer" target="_blank">https://www.elephantrobotics.com/en/mercury-humanoid-robot/</a></u>
</p>]]></description><pubDate>Tue, 23 Jul 2024 22:00:03 +0000</pubDate><guid>https://spectrum.ieee.org/elephant-robotics-mercury</guid><category>Elephant robotics</category><category>Ai</category><category>Humanoid robots</category><category>Mercury series</category><dc:creator>Elephant Robotics</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/two-grouped-photos-showing-a-winking-robot-standing-in-a-room-and-another-smiling-robot-in-a-kitchen-area.jpg?id=52857053&amp;width=980"></media:content></item><item><title>How Olympics Officials Try to Catch “Motor Doping”</title><link>https://spectrum.ieee.org/motor-doping-cycling</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/closeup-of-person-holding-a-large-ipad-up-to-the-chain-of-a-bike.jpg?id=52956705&width=1200&height=800&coordinates=0%2C104%2C0%2C105"/><br/><br/><p>A French cycling official confronts a rider suspected of doping and ends up jumping onto the hood of a van making a high-speed getaway. This isn’t a tragicomedy starring <a href="https://en.wikipedia.org/wiki/G%C3%A9rard_Depardieu" target="_blank">Gérard Depardieu</a>, sending up the sport’s well-earned reputation for cheating. This scenario played out in May at the <a href="https://www.cyclingweekly.com/news/motor-doping-suspect-runs-down-race-organiser-while-escaping-inspection" target="_blank">Routes de l’Oise cycling competition</a> near Paris, and the van was believed to contain evidence of a distinctly 21st-century cheat: a hidden electric motor.<strong></strong></p><p>Cyclists call it “motor doping.” At the Paris Olympics opening on Friday, officials will be deploying electromagnetic scanners and X-ray imaging to combat it, as cyclists race for gold in and around the French capital. The officials’ prey can be quite small: Cycling experts say just 20 or 30 watts of extra power is enough to tilt the field and clinch a race.</p><p>Motor doping has been confirmed only once in professional cycling, way back in 2016. And the sport’s governing body, the <a href="https://www.uci.org/" target="_blank"><u>Union Cycliste Internationale</u></a> (UCI), has since introduced increasingly sophisticated motor-detection methods. But illicit motors remain a scourge at high-profile amateur events like the Routes de l’Oise. Some top professionals, past and present, continue to raise an alarm.</p><p class="pull-quote">“It’s 10 years now that we’re speaking about this…. If you want to settle this issue you have to invest.” <strong>—Jean-Christophe Péraud, former Union Cycliste Internationale official</strong></p><p>Riders and experts reached by <em>IEEE Spectrum</em> say it’s unlikely that technological doping still exists at the professional level. “I’m confident it’s not happening any more. I think as soon as we began to speak about it, it stopped. Because at a high level it’s too dangerous for a team and an athlete,” says <a href="https://www.procyclingstats.com/rider/jean-christophe-peraud" target="_blank"><u>Jean-Christophe Péraud</u></a>, an <a href="https://olympics.com/en/athletes/jean-christophe-peraud" target="_blank"><u>Olympic silver medalist</u></a> who <strong></strong>was UCI’s first <a href="https://www.uci.org/pressrelease/the-uci-presents-a-robust-action-plan-to-combat-technological-fraud/6omY2RZitUwewjShNVCAYH" target="_blank">Manager of Equipment and the Fight against Technological Fraud</a>. </p><p>But trust is limited. Cycling is still recovering from the scandals surrounding U.S. Olympian Lance Armstrong, whose extensive use of transfusions and drugs to boost blood-oxygen levels fueled <a href="https://www.theguardian.com/sport/2015/mar/09/lance-armstrong-uci-colluded-circ-report-cycling" target="_blank"><u>allegations of collusion by UCI officials</u></a> and threats to <a href="https://www.reuters.com/article/idUSBRE90E0ZX/" target="_blank"><u>boot cycling out of the Olympics</u></a>. </p><p>Many—including Péraud—say more vigilance is needed. The solution may be next-generation detection tech: onboard scanners that provide continuous assurance that human muscle alone is powering the sport’s dramatic sprints and climbs.</p><h2>How Officials Have Hunted for Motor Doping in Cycling</h2><p>Rumors of hidden motors first <a href="https://www.france24.com/en/20100602-fabio-cancellara-youtube-video-motorized-bike-rubbish-cycling-cheating" target="_blank"><u>swirled into the mainstream in 2010</u></a> after a Swiss cyclist clinched several European events with stunning accelerations. At the time the UCI lacked means of detecting concealed motors, and its technical director promised to “speed up” work on a “quick and efficient way” to do so. </p><p>The UCI began with <a href="https://road.cc/content/news/188441-mechanical-doping-uci-tested-and-rejected-thermal-imaging-detect-motors" target="_blank"><u>infrared cameras</u></a>, but they are useless for pre- and post-race checks when a hidden motor is cold. Not until 2015, amidst <a href="https://www.espn.com/sports/endurance/story/_/id/13272428/endurance-sports-velonews-tour-de-france-leader-chris-froome-facing-more-accusations" target="_blank"><u>further motor doping rumors</u></a> and <a href="https://road.cc/content/news/186575-hidden-motors-used-strade-bianche-claims-french-tv-video#:~:text=Jean-Pierre%20Vedry,%20the%20former,no%20reply,%20no%20checks.%E2%80%9D" target="_blank"><u>allegations of UCI inaction</u></a>, did the organization begin beta testing a better tool: an iPad-based “magnetometric tablet” scanner. </p><p>According to the UCI, an adapter plugged into one of these tablet scanners creates an ambient magnetic field. Then, a magnetometer and custom software register disruptions to the field that may indicate the presence of metal or magnets in and around a bike’s carbon-fiber frame.</p><p>UCI’s tablets delivered in their debut appearance, at the 2016 Cyclocross World Championships held that year in Belgium. Scans of bikes at the rugged event—a blend of road and mountain biking—<a href="https://www.cxmagazine.com/motor-mechanical-doping-femke-van-den-driessche-suspected-2016-cyclocross-world-championships-update" target="_blank"><u>flagged a bike</u></a> bearing the name of local favorite Femke Van den Driessche. Closer inspection revealed a motor and battery lodged within the hollow frame element that angles down from a bike’s saddle to its pedals, and wires connecting the seat tube’s hidden hardware to a push-button switch under the handlebars. </p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="person in biking gear pushing bike up a hill on muddy terrain" class="rm-shortcode" data-rm-shortcode-id="7595488c2c38efa50c0515ed1fc5e689" data-rm-shortcode-name="rebelmouse-image" id="57041" loading="lazy" src="https://spectrum.ieee.org/media-library/person-in-biking-gear-pushing-bike-up-a-hill-on-muddy-terrain.jpg?id=52956711&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">In 2016, a concealed motor was found in a bike bearing Belgian cyclist Femke Van Den Driessche’s name at the world cyclo-cross championships. (Van Den Driessche is shown here with a different bike.)</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">AFP/Getty Images</small></p><p>Van den Driessche, banned from competition for six years, withdrew from racing while maintaining her innocence. (Giovambattista Lera, the amateur cyclist implicated earlier this year in France, <u><a href="https://road.cc/content/news/cyclist-accused-motor-doping-denies-wrong-doing-308633" target="_blank">also denies using electric assistance</a></u> in competition.)</p><p>The motor in Van den Driessche’s bike engaged with the bike’s crankshaft and added 200 W of power. The equipment’s Austrian manufacturer, <a href="https://web.archive.org/web/20171015031645/http://www.vivax-assist.com/en/" target="_blank"><u>Vivax Drive</u></a>, is now defunct. But anyone with cash to spare can experience 200 W of extra push via a racer equipped by Monaco-based HPS-Bike, such as the HPS-equipped <a href="https://www.lotuscars.com/en-GB/type-136" target="_blank">Lotus Type 136 racing bike</a> from U.K. sports car producer Lotus Group, which starts at £15,199 (US $19,715).<strong></strong></p><p>HPS founder & CEO <a href="https://www.linkedin.com/in/harry-gibbings-ab8065201/?originalSubdomain=mc" target="_blank"><u>Harry Gibbings</u></a> says the company seeks to empower weekend riders who don’t want to struggle up steep hills or who need an extra boost here and there to keep up with the pack. Gibbings says the technology is not available for retrofits, and is thus off limits to would-be cheats. Still, <a href="https://www.ride-hps.com/watt-assist/" target="_blank"><u>the HPS Watt Assist system</u></a> shows the outer bounds of what’s possible in discreet high-performance electric assist. </p><p>The 30-millimeter-diameter, 300-gram motor, is manufactured by Swiss motor maker <a href="https://www.maxongroup.com/en" target="_blank"><u>Maxon Group</u></a>, and Gibbings says it uses essentially the same power-dense brushless design that’s propelling NASA’s Perseverance rover on Mars. HPS builds the motor into a bike’s downtube, the frame element angling up from a bike’s crank toward its handlebars. </p><p>Notwithstanding persistent media speculation about electric motors built into rear hubs or solid wheels, Gibbings says only a motor placed in a frame’s tubes can add power without jeopardizing the look, feel, and performance of a racing bike. </p><h2>UCI’s New Techniques to Spot Cheating in Cycling</h2><p>Professional cycling got its most sophisticated detection systems in 2018, after criticism of UCI motor-doping policies <a href="https://www.bbc.com/sport/cycling/41347950.amp" target="_blank"><u>helped fuel a change of leadership</u></a>. Incoming President David Lappartient <a href="http://www.apple.com" target="_blank"><u>appointed Péraud to push detection</u></a> to new levels, and five months later UCI announced its first X-ray equipment at a press conference in Geneva. </p><p>Unlike the tablet scanners, which yield many false positives and require dismantling of suspect bikes, X-ray imaging is definitive. The <a href="https://www.youtube.com/watch?v=-iRwwquk7v0&t=3s" target="_blank"><u>detector</u></a> is built into a shielded container and driven to events.</p><p>UCI told the cycling press that its X-ray cabinet would “remove any suspicion regarding race results.” And it says it maintains a high level of testing, with close to <a href="https://www.uci.org/pressrelease/the-uci-unveils-its-programme-to-combat-doping-and-technological-fraud-for/5j3GqEVkRlbPZaa3HfwVag#:~:text=At%20last%20year's%20Tour%20de,of%20technological%20fraud%20were%20detected." target="_blank"><u>1,000 motor-doping checks at last year’s Tour de France</u></a>. </p><p>UCI declined to speak with <em>IEEE Spectrum</em> about its motor-detection program, including plans for the Paris Olympics. But it appears to have stepped up vigilance. Lappartient recently acknowledged that UCI’s controls are “<a href="https://www.theguardian.com/sport/article/2024/jun/27/uci-to-pay-whistleblowers-for-motor-doping-tip-offs-at-tour-de-france?CMP=share_btn_url" target="_blank"><u>not 100 percent secure</u></a>” and announced a reward for whistleblowers who deliver evidence of motor fraud. In May, UCI once again <a href="https://www.uci.org/pressrelease/uci-appoints-nicholas-raudenski-as-head-of-the-fight-against-technological/2pu13dvS3ykOj9YOiCg4Aj" target="_blank"><u>appointed a motor-doping czar</u></a>—a first since <a href="https://www.insidethegames.biz/articles/1096111/peraud-loses-uci-role" target="_blank">Péraud departed</a> amidst budget cuts in 2020. Among other duties, former U.S. Department of Homeland Security criminal investigator Nicholas Raudenski is tasked with “development of new methods to detect technological fraud.” </p><p class="pull-quote">Unlike the tablet scanners, X-ray imaging is definitive.</p><p>Péraud is convinced that only real-time monitoring of bikes throughout major races can prove that motor fraud is in the past, since big races provide ample opportunities to sneak in an additional bike and thus evade UCI’s current tools.</p><p>UCI has already laid the groundwork for such live monitoring, partnering with France’s <a href="https://www.cea.fr/english/Pages/Welcome.aspx" target="_blank">Alternative Energies and Atomic Energy Commission</a> (Commissariat à l’énergie atomique et aux énergies alternatives, or CEA) to capitalize on the national lab’s deep magnetometry expertise. UCI disclosed some details at its 2018 Geneva press conference, where a CEA official <a href="https://www.uci.org/article/the-uci-presents-a-robust-action-plan-to-combat-technological-fraud-185708/1RaUU9mAQ4qkyXKwN0rXp0" target="_blank"><u>presented its concept</u></a>: an embedded, high-resolution magnetometer to detect a hidden motor’s electromagnetic signature and wirelessly alert officials via receivers on race support vehicles. </p><p>As of June 2018, CEA researchers in Grenoble had <a href="https://www.minatec.org/en/cycling-detecting-hidden-electric-motors/" target="_blank"><u>identified an appropriate magnetometer</u></a> and were evaluating the electromagnetic noise that could challenge the system—“from rotating wheels and pedals to passing motorcycles and cars.” </p><p>Mounting detectors on every bike would not be cheap, but Péraud says he is convinced that cycling needs it: “It’s 10 years now that we’re speaking about this…. If you want to settle this issue you have to invest.” </p>]]></description><pubDate>Tue, 23 Jul 2024 12:01:02 +0000</pubDate><guid>https://spectrum.ieee.org/motor-doping-cycling</guid><category>Bicycles</category><category>Motors</category><category>Olympic games</category><category>Paris</category><dc:creator>Peter Fairley</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/closeup-of-person-holding-a-large-ipad-up-to-the-chain-of-a-bike.jpg?id=52956705&amp;width=980"></media:content></item><item><title>Andrew Ng: Unbiggen AI</title><link>https://spectrum.ieee.org/andrew-ng-data-centric-ai</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/andrew-ng-listens-during-the-power-of-data-sooner-than-you-think-global-technology-conference-in-brooklyn-new-york-on-wednes.jpg?id=29206806&width=1200&height=800&coordinates=0%2C0%2C0%2C210"/><br/><br/><p><strong><a href="https://en.wikipedia.org/wiki/Andrew_Ng" rel="noopener noreferrer" target="_blank">Andrew Ng</a> has serious street cred</strong> in artificial intelligence. He pioneered the use of graphics processing units (GPUs) to train deep learning models in the late 2000s with his students at <a href="https://stanfordmlgroup.github.io/" rel="noopener noreferrer" target="_blank">Stanford University</a>, cofounded <a href="https://research.google/teams/brain/" rel="noopener noreferrer" target="_blank">Google Brain</a> in 2011, and then served for three years as chief scientist for <a href="https://ir.baidu.com/" rel="noopener noreferrer" target="_blank">Baidu</a>, where he helped build the Chinese tech giant’s AI group. So when he says he has identified the next big shift in artificial intelligence, people listen. And that’s what he told <em>IEEE Spectrum</em> in an exclusive Q&A.</p><hr/><p>
	Ng’s current efforts are focused on his company
	<a href="https://landing.ai/about/" rel="noopener noreferrer" target="_blank">Landing AI</a>, which built a platform called LandingLens to help manufacturers improve visual inspection with computer vision. <a name="top"></a>He has also become something of an evangelist for what he calls the <a href="https://www.youtube.com/watch?v=06-AZXmwHjo" target="_blank">data-centric AI movement</a>, which he says can yield “small data” solutions to big issues in AI, including model efficiency, accuracy, and bias.
</p><p>
	Andrew Ng on...
</p><ul>
<li><a href="#big">What’s next for really big models</a></li>
<li><a href="#career">The career advice he didn’t listen to</a></li>
<li><a href="#defining">Defining the data-centric AI movement</a></li>
<li><a href="#synthetic">Synthetic data</a></li>
<li><a href="#work">Why Landing AI asks its customers to do the work</a></li>
</ul><p>
<a name="big"></a><strong>The great advances in deep learning over the past decade or so have been powered by ever-bigger models crunching ever-bigger amounts of data. Some people argue that that’s an <a href="https://spectrum.ieee.org/deep-learning-computational-cost" target="_self">unsustainable trajectory</a>. Do you agree that it can’t go on that way?</strong>
</p><p>
<strong>Andrew Ng: </strong>This is a big question. We’ve seen foundation models in NLP [natural language processing]. I’m excited about NLP models getting even bigger, and also about the potential of building foundation models in computer vision. I think there’s lots of signal to still be exploited in video: We have not been able to build foundation models yet for video because of compute bandwidth and the cost of processing video, as opposed to tokenized text. So I think that this engine of scaling up deep learning algorithms, which has been running for something like 15 years now, still has steam in it. Having said that, it only applies to certain problems, and there’s a set of other problems that need small data solutions.
</p><p>
<strong>When you say you want a foundation model for computer vision, what do you mean by that?</strong>
</p><p>
<strong>Ng:</strong> This is a term coined by <a href="https://cs.stanford.edu/~pliang/" rel="noopener noreferrer" target="_blank">Percy Liang</a> and <a href="https://crfm.stanford.edu/" rel="noopener noreferrer" target="_blank">some of my friends at Stanford</a> to refer to very large models, trained on very large data sets, that can be tuned for specific applications. For example, <a href="https://spectrum.ieee.org/open-ais-powerful-text-generating-tool-is-ready-for-business" target="_self">GPT-3</a> is an example of a foundation model [for NLP]. Foundation models offer a lot of promise as a new paradigm in developing machine learning applications, but also challenges in terms of making sure that they’re reasonably fair and free from bias, especially if many of us will be building on top of them.
</p><p>
<strong>What needs to happen for someone to build a foundation model for video?</strong>
</p><p>
<strong>Ng:</strong> I think there is a scalability problem. The compute power needed to process the large volume of images for video is significant, and I think that’s why foundation models have arisen first in NLP. Many researchers are working on this, and I think we’re seeing early signs of such models being developed in computer vision. But I’m confident that if a semiconductor maker gave us 10 times more processor power, we could easily find 10 times more video to build such models for vision.
</p><p>
	Having said that, a lot of what’s happened over the past decade is that deep learning has happened in consumer-facing companies that have large user bases, sometimes billions of users, and therefore very large data sets. While that paradigm of machine learning has driven a lot of economic value in consumer software, I find that that recipe of scale doesn’t work for other industries.
</p><p>
<a href="#top">Back to top</a><a name="career"></a>
</p><p>
<strong>It’s funny to hear you say that, because your early work was at a consumer-facing company with millions of users.</strong>
</p><p>
<strong>Ng: </strong>Over a decade ago, when I proposed starting the <a href="https://research.google/teams/brain/" rel="noopener noreferrer" target="_blank">Google Brain</a> project to use Google’s compute infrastructure to build very large neural networks, it was a controversial step. One very senior person pulled me aside and warned me that starting Google Brain would be bad for my career. I think he felt that the action couldn’t just be in scaling up, and that I should instead focus on architecture innovation.
</p><p class="pull-quote">
	“In many industries where giant data sets simply don’t exist, I think the focus has to shift from big data to good data. Having 50 thoughtfully engineered examples can be sufficient to explain to the neural network what you want it to learn.”<br/>
	—Andrew Ng, CEO & Founder, Landing AI
</p><p>
	I remember when my students and I published the first
	<a href="https://nips.cc/" rel="noopener noreferrer" target="_blank">NeurIPS</a> workshop paper advocating using <a href="https://developer.nvidia.com/cuda-zone" rel="noopener noreferrer" target="_blank">CUDA</a>, a platform for processing on GPUs, for deep learning—a different senior person in AI sat me down and said, “CUDA is really complicated to program. As a programming paradigm, this seems like too much work.” I did manage to convince him; the other person I did not convince.
</p><p>
<strong>I expect they’re both convinced now.</strong>
</p><p>
<strong>Ng:</strong> I think so, yes.
</p><p>
	Over the past year as I’ve been speaking to people about the data-centric AI movement, I’ve been getting flashbacks to when I was speaking to people about deep learning and scalability 10 or 15 years ago. In the past year, I’ve been getting the same mix of “there’s nothing new here” and “this seems like the wrong direction.”
</p><p>
<a href="#top">Back to top</a><a name="defining"></a>
</p><p>
<strong>How do you define data-centric AI, and why do you consider it a movement?</strong>
</p><p>
<strong>Ng:</strong> Data-centric AI is the discipline of systematically engineering the data needed to successfully build an AI system. For an AI system, you have to implement some algorithm, say a neural network, in code and then train it on your data set. The dominant paradigm over the last decade was to download the data set while you focus on improving the code. Thanks to that paradigm, over the last decade deep learning networks have improved significantly, to the point where for a lot of applications the code—the neural network architecture—is basically a solved problem. So for many practical applications, it’s now more productive to hold the neural network architecture fixed, and instead find ways to improve the data.
</p><p>
	When I started speaking about this, there were many practitioners who, completely appropriately, raised their hands and said, “Yes, we’ve been doing this for 20 years.” This is the time to take the things that some individuals have been doing intuitively and make it a systematic engineering discipline.
</p><p>
	The data-centric AI movement is much bigger than one company or group of researchers. My collaborators and I organized a
	<a href="https://neurips.cc/virtual/2021/workshop/21860" rel="noopener noreferrer" target="_blank">data-centric AI workshop at NeurIPS</a>, and I was really delighted at the number of authors and presenters that showed up.
</p><p>
<strong>You often talk about companies or institutions that have only a small amount of data to work with. How can data-centric AI help them?</strong>
</p><p>
<strong>Ng: </strong>You hear a lot about vision systems built with millions of images—I once built a face recognition system using 350 million images. Architectures built for hundreds of millions of images don’t work with only 50 images. But it turns out, if you have 50 really good examples, you can build something valuable, like a defect-inspection system. In many industries where giant data sets simply don’t exist, I think the focus has to shift from big data to good data. Having 50 thoughtfully engineered examples can be sufficient to explain to the neural network what you want it to learn.
</p><p>
<strong>When you talk about training a model with just 50 images, does that really mean you’re taking an existing model that was trained on a very large data set and fine-tuning it? Or do you mean a brand new model that’s designed to learn only from that small data set?</strong>
</p><p>
<strong>Ng: </strong>Let me describe what Landing AI does. When doing visual inspection for manufacturers, we often use our own flavor of <a href="https://developers.arcgis.com/python/guide/how-retinanet-works/" rel="noopener noreferrer" target="_blank">RetinaNet</a>. It is a pretrained model. Having said that, the pretraining is a small piece of the puzzle. What’s a bigger piece of the puzzle is providing tools that enable the manufacturer to pick the right set of images [to use for fine-tuning] and label them in a consistent way. There’s a very practical problem we’ve seen spanning vision, NLP, and speech, where even human annotators don’t agree on the appropriate label. For big data applications, the common response has been: If the data is noisy, let’s just get a lot of data and the algorithm will average over it. But if you can develop tools that flag where the data’s inconsistent and give you a very targeted way to improve the consistency of the data, that turns out to be a more efficient way to get a high-performing system.
</p><p class="pull-quote">
	“Collecting more data often helps, but if you try to collect more data for everything, that can be a very expensive activity.”<br/>
	—Andrew Ng
</p><p>
	For example, if you have 10,000 images where 30 images are of one class, and those 30 images are labeled inconsistently, one of the things we do is build tools to draw your attention to the subset of data that’s inconsistent. So you can very quickly relabel those images to be more consistent, and this leads to improvement in performance.
</p><p>
<strong>Could this focus on high-quality data help with bias in data sets? If you’re able to curate the data more before training?</strong>
</p><p>
<strong>Ng:</strong> Very much so. Many researchers have pointed out that biased data is one factor among many leading to biased systems. There have been many thoughtful efforts to engineer the data. At the NeurIPS workshop, <a href="https://www.cs.princeton.edu/~olgarus/" rel="noopener noreferrer" target="_blank">Olga Russakovsky</a> gave a really nice talk on this. At the main NeurIPS conference, I also really enjoyed <a href="https://neurips.cc/virtual/2021/invited-talk/22281" rel="noopener noreferrer" target="_blank">Mary Gray’s presentation,</a> which touched on how data-centric AI is one piece of the solution, but not the entire solution. New tools like <a href="https://www.microsoft.com/en-us/research/project/datasheets-for-datasets/" rel="noopener noreferrer" target="_blank">Datasheets for Datasets</a> also seem like an important piece of the puzzle.
</p><p>
	One of the powerful tools that data-centric AI gives us is the ability to engineer a subset of the data. Imagine training a machine-learning system and finding that its performance is okay for most of the data set, but its performance is biased for just a subset of the data. If you try to change the whole neural network architecture to improve the performance on just that subset, it’s quite difficult. But if you can engineer a subset of the data you can address the problem in a much more targeted way.
</p><p>
<strong>When you talk about engineering the data, what do you mean exactly?</strong>
</p><p>
<strong>Ng: </strong>In AI, data cleaning is important, but the way the data has been cleaned has often been in very manual ways. In computer vision, someone may visualize images through a <a href="https://jupyter.org/" rel="noopener noreferrer" target="_blank">Jupyter notebook</a> and maybe spot the problem, and maybe fix it. But I’m excited about tools that allow you to have a very large data set, tools that draw your attention quickly and efficiently to the subset of data where, say, the labels are noisy. Or to quickly bring your attention to the one class among 100 classes where it would benefit you to collect more data. Collecting more data often helps, but if you try to collect more data for everything, that can be a very expensive activity.
</p><p>
	For example, I once figured out that a speech-recognition system was performing poorly when there was car noise in the background. Knowing that allowed me to collect more data with car noise in the background, rather than trying to collect more data for everything, which would have been expensive and slow.
</p><p>
<a href="#top">Back to top</a><a name="synthetic"></a>
</p><p>
<strong>What about using synthetic data, is that often a good solution?</strong>
</p><p>
<strong>Ng: </strong>I think synthetic data is an important tool in the tool chest of data-centric AI. At the NeurIPS workshop, <a href="https://tensorlab.cms.caltech.edu/users/anima/" rel="noopener noreferrer" target="_blank">Anima Anandkumar</a> gave a great talk that touched on synthetic data. I think there are important uses of synthetic data that go beyond just being a preprocessing step for increasing the data set for a learning algorithm. I’d love to see more tools to let developers use synthetic data generation as part of the closed loop of iterative machine learning development.
</p><p>
<strong>Do you mean that synthetic data would allow you to try the model on more data sets?</strong>
</p><p>
<strong>Ng: </strong>Not really. Here’s an example. Let’s say you’re trying to detect defects in a smartphone casing. There are many different types of defects on smartphones. It could be a scratch, a dent, pit marks, discoloration of the material, other types of blemishes. If you train the model and then find through error analysis that it’s doing well overall but it’s performing poorly on pit marks, then synthetic data generation allows you to address the problem in a more targeted way. You could generate more data just for the pit-mark category.
</p><p class="pull-quote">
	“In the consumer software Internet, we could train a handful of machine-learning models to serve a billion users. In manufacturing, you might have 10,000 manufacturers building 10,000 custom AI models.”<br/>
	—Andrew Ng
</p><p>
	Synthetic data generation is a very powerful tool, but there are many simpler tools that I will often try first. Such as data augmentation, improving labeling consistency, or just asking a factory to collect more data.
</p><p>
<a href="#top">Back to top</a><a name="work"></a>
</p><p>
<strong>To make these issues more concrete, can you walk me through an example? When a company approaches <a href="https://landing.ai/" rel="noopener noreferrer" target="_blank">Landing AI</a> and says it has a problem with visual inspection, how do you onboard them and work toward deployment?</strong>
</p><p>
<strong>Ng: </strong>When a customer approaches us we usually have a conversation about their inspection problem and look at a few images to verify that the problem is feasible with computer vision. Assuming it is, we ask them to upload the data to the <a href="https://landing.ai/platform/" rel="noopener noreferrer" target="_blank">LandingLens</a> platform. We often advise them on the methodology of data-centric AI and help them label the data.
</p><p>
	One of the foci of Landing AI is to empower manufacturing companies to do the machine learning work themselves. A lot of our work is making sure the software is fast and easy to use. Through the iterative process of machine learning development, we advise customers on things like how to train models on the platform, when and how to improve the labeling of data so the performance of the model improves. Our training and software supports them all the way through deploying the trained model to an edge device in the factory.
</p><p>
<strong>How do you deal with changing needs? If products change or lighting conditions change in the factory, can the model keep up?</strong>
</p><p>
<strong>Ng:</strong> It varies by manufacturer. There is data drift in many contexts. But there are some manufacturers that have been running the same manufacturing line for 20 years now with few changes, so they don’t expect changes in the next five years. Those stable environments make things easier. For other manufacturers, we provide tools to flag when there’s a significant data-drift issue. I find it really important to empower manufacturing customers to correct data, retrain, and update the model. Because if something changes and it’s 3 a.m. in the United States, I want them to be able to adapt their learning algorithm right away to maintain operations.
</p><p>
	In the consumer software Internet, we could train a handful of machine-learning models to serve a billion users. In manufacturing, you might have 10,000 manufacturers building 10,000 custom AI models. The challenge is, how do you do that without Landing AI having to hire 10,000 machine learning specialists?
</p><p>
<strong>So you’re saying that to make it scale, you have to empower customers to do a lot of the training and other work.</strong>
</p><p>
<strong>Ng: </strong>Yes, exactly! This is an industry-wide problem in AI, not just in manufacturing. Look at health care. Every hospital has its own slightly different format for electronic health records. How can every hospital train its own custom AI model? Expecting every hospital’s IT personnel to invent new neural-network architectures is unrealistic. The only way out of this dilemma is to build tools that empower the customers to build their own models by giving them tools to engineer the data and express their domain knowledge. That’s what Landing AI is executing in computer vision, and the field of AI needs other teams to execute this in other domains.
</p><p>
<strong>Is there anything else you think it’s important for people to understand about the work you’re doing or the data-centric AI movement?</strong>
</p><p>
<strong>Ng: </strong>In the last decade, the biggest shift in AI was a shift to deep learning. I think it’s quite possible that in this decade the biggest shift will be to data-centric AI. With the maturity of today’s neural network architectures, I think for a lot of the practical applications the bottleneck will be whether we can efficiently get the data we need to develop systems that work well. The data-centric AI movement has tremendous energy and momentum across the whole community. I hope more researchers and developers will jump in and work on it.
</p><p>
<a href="#top">Back to top</a>
</p><p><em>This article appears in the April 2022 print issue as “Andrew Ng, AI Minimalist</em><em>.”</em></p>]]></description><pubDate>Wed, 09 Feb 2022 15:31:12 +0000</pubDate><guid>https://spectrum.ieee.org/andrew-ng-data-centric-ai</guid><category>Deep learning</category><category>Artificial intelligence</category><category>Andrew ng</category><category>Type:cover</category><dc:creator>Eliza Strickland</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/andrew-ng-listens-during-the-power-of-data-sooner-than-you-think-global-technology-conference-in-brooklyn-new-york-on-wednes.jpg?id=29206806&amp;width=980"></media:content></item><item><title>How AI Will Change Chip Design</title><link>https://spectrum.ieee.org/ai-chip-design-matlab</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/layered-rendering-of-colorful-semiconductor-wafers-with-a-bright-white-light-sitting-on-one.jpg?id=29285079&width=1200&height=800&coordinates=0%2C0%2C0%2C0"/><br/><br/><p>The end of <a href="https://spectrum.ieee.org/on-beyond-moores-law-4-new-laws-of-computing" target="_self">Moore’s Law</a> is looming. Engineers and designers can do only so much to <a href="https://spectrum.ieee.org/ibm-introduces-the-worlds-first-2nm-node-chip" target="_self">miniaturize transistors</a> and <a href="https://spectrum.ieee.org/cerebras-giant-ai-chip-now-has-a-trillions-more-transistors" target="_self">pack as many of them as possible into chips</a>. So they’re turning to other approaches to chip design, incorporating technologies like AI into the process.</p><p>Samsung, for instance, is <a href="https://spectrum.ieee.org/processing-in-dram-accelerates-ai" target="_self">adding AI to its memory chips</a> to enable processing in memory, thereby saving energy and speeding up machine learning. Speaking of speed, Google’s TPU V4 AI chip has <a href="https://spectrum.ieee.org/heres-how-googles-tpu-v4-ai-chip-stacked-up-in-training-tests" target="_self">doubled its processing power</a> compared with that of  its previous version.</p><p>But AI holds still more promise and potential for the semiconductor industry. To better understand how AI is set to revolutionize chip design, we spoke with <a href="https://www.linkedin.com/in/heather-gorr-phd" rel="noopener noreferrer" target="_blank">Heather Gorr</a>, senior product manager for <a href="https://www.mathworks.com/" rel="noopener noreferrer" target="_blank">MathWorks</a>’ MATLAB platform.</p><p><strong>How is AI currently being used to design the next generation of chips?</strong></p><p><strong>Heather Gorr:</strong> AI is such an important technology because it’s involved in most parts of the cycle, including the design and manufacturing process. There’s a lot of important applications here, even in the general process engineering where we want to optimize things. I think defect detection is a big one at all phases of the process, especially in manufacturing. But even thinking ahead in the design process, [AI now plays a significant role] when you’re designing the light and the sensors and all the different components. There’s a lot of anomaly detection and fault mitigation that you really want to consider.</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="Portrait of a woman with blonde-red hair smiling at the camera" class="rm-shortcode rm-resized-image" data-rm-shortcode-id="1f18a02ccaf51f5c766af2ebc4af18e1" data-rm-shortcode-name="rebelmouse-image" id="2dc00" loading="lazy" src="https://spectrum.ieee.org/media-library/portrait-of-a-woman-with-blonde-red-hair-smiling-at-the-camera.jpg?id=29288554&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..." style="max-width: 100%;">Heather Gorr</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..." style="max-width: 100%;">MathWorks</small></p><p>Then, thinking about the logistical modeling that you see in any industry, there is always planned downtime that you want to mitigate; but you also end up having unplanned downtime. So, looking back at that historical data of when you’ve had those moments where maybe it took a bit longer than expected to manufacture something, you can take a look at all of that data and use AI to try to identify the proximate cause or to see  something that might jump out even in the processing and design phases. We think of AI oftentimes as a predictive tool, or as a robot doing something, but a lot of times you get a lot of insight from the data through AI.</p><p><strong>What are the benefits of using AI for chip design?</strong></p><p><strong>Gorr:</strong> Historically, we’ve seen a lot of physics-based modeling, which is a very intensive process. We want to do a <a href="https://en.wikipedia.org/wiki/Model_order_reduction" rel="noopener noreferrer" target="_blank">reduced order model</a>, where instead of solving such a computationally expensive and extensive model, we can do something a little cheaper. You could create a surrogate model, so to speak, of that physics-based model, use the data, and then do your <a href="https://institutefordiseasemodeling.github.io/idmtools/parameter-sweeps.html" rel="noopener noreferrer" target="_blank">parameter sweeps</a>, your optimizations, your <a href="https://www.ibm.com/cloud/learn/monte-carlo-simulation" rel="noopener noreferrer" target="_blank">Monte Carlo simulations</a> using the surrogate model. That takes a lot less time computationally than solving the physics-based equations directly. So, we’re seeing that benefit in many ways, including the efficiency and economy that are the results of iterating quickly on the experiments and the simulations that will really help in the design.</p><p><strong>So it’s like having a digital twin in a sense?</strong></p><p><strong>Gorr:</strong> Exactly. That’s pretty much what people are doing, where you have the physical system model and the experimental data. Then, in conjunction, you have this other model that you could tweak and tune and try different parameters and experiments that let sweep through all of those different situations and come up with a better design in the end.</p><p><strong>So, it’s going to be more efficient and, as you said, cheaper?</strong></p><p><strong>Gorr:</strong> Yeah, definitely. Especially in the experimentation and design phases, where you’re trying different things. That’s obviously going to yield dramatic cost savings if you’re actually manufacturing and producing [the chips]. You want to simulate, test, experiment as much as possible without making something using the actual process engineering.</p><p><strong>We’ve talked about the benefits. How about the drawbacks?</strong></p><p><strong>Gorr: </strong>The [AI-based experimental models] tend to not be as accurate as physics-based models. Of course, that’s why you do many simulations and parameter sweeps. But that’s also the benefit of having that digital twin, where you can keep that in mind—it’s not going to be as accurate as that precise model that we’ve developed over the years.</p><p>Both chip design and manufacturing are system intensive; you have to consider every little part. And that can be really challenging. It’s a case where you might have models to predict something and different parts of it, but you still need to bring it all together.</p><p>One of the other things to think about too is that you need the data to build the models. You have to incorporate data from all sorts of different sensors and different sorts of teams, and so that heightens the challenge.</p><p><strong>How can engineers use AI to better prepare and extract insights from hardware or sensor data?</strong></p><p><strong>Gorr: </strong>We always think about using AI to predict something or do some robot task, but you can use AI to come up with patterns and pick out things you might not have noticed before on your own. People will use AI when they have high-frequency data coming from many different sensors, and a lot of times it’s useful to explore the frequency domain and things like data synchronization or resampling. Those can be really challenging if you’re not sure where to start.</p><p>One of the things I would say is, use the tools that are available. There’s a vast community of people working on these things, and you can find lots of examples [of applications and techniques] on <a href="https://github.com/" rel="noopener noreferrer" target="_blank">GitHub</a> or <a href="https://www.mathworks.com/matlabcentral/" rel="noopener noreferrer" target="_blank">MATLAB Central</a>, where people have shared nice examples, even little apps they’ve created. I think many of us are buried in data and just not sure what to do with it, so definitely take advantage of what’s already out there in the community. You can explore and see what makes sense to you, and bring in that balance of domain knowledge and the insight you get from the tools and AI.</p><p><strong>What should engineers and designers consider wh</strong><strong>en using AI for chip design?</strong></p><p><strong>Gorr:</strong> Think through what problems you’re trying to solve or what insights you might hope to find, and try to be clear about that. Consider all of the different components, and document and test each of those different parts. Consider all of the people involved, and explain and hand off in a way that is sensible for the whole team.</p><p><strong>How do you think AI will affect chip designers’ jobs?</strong></p><p><strong>Gorr:</strong> It’s going to free up a lot of human capital for more advanced tasks. We can use AI to reduce waste, to optimize the materials, to optimize the design, but then you still have that human involved whenever it comes to decision-making. I think it’s a great example of people and technology working hand in hand. It’s also an industry where all people involved—even on the manufacturing floor—need to have some level of understanding of what’s happening, so this is a great industry for advancing AI because of how we test things and how we think about them before we put them on the chip.</p><p><strong>How do you envision the future of AI and chip design?</strong></p><p><strong>Gorr</strong><strong>:</strong> It’s very much dependent on that human element—involving people in the process and having that interpretable model. We can do many things with the mathematical minutiae of modeling, but it comes down to how people are using it, how everybody in the process is understanding and applying it. Communication and involvement of people of all skill levels in the process are going to be really important. We’re going to see less of those superprecise predictions and more transparency of information, sharing, and that digital twin—not only using AI but also using our human knowledge and all of the work that many people have done over the years.</p>]]></description><pubDate>Tue, 08 Feb 2022 14:00:01 +0000</pubDate><guid>https://spectrum.ieee.org/ai-chip-design-matlab</guid><category>Chip fabrication</category><category>Matlab</category><category>Moore’s law</category><category>Chip design</category><category>Ai</category><category>Digital twins</category><dc:creator>Rina Diane Caballar</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/layered-rendering-of-colorful-semiconductor-wafers-with-a-bright-white-light-sitting-on-one.jpg?id=29285079&amp;width=980"></media:content></item><item><title>Atomically Thin Materials Significantly Shrink Qubits</title><link>https://spectrum.ieee.org/2d-hbn-qubit</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-golden-square-package-holds-a-small-processor-sitting-on-top-is-a-metal-square-with-mit-etched-into-it.jpg?id=29281587&width=1200&height=800&coordinates=0%2C0%2C0%2C0"/><br/><br/><p>Quantum computing is a devilishly complex technology, with many technical hurdles impacting its development. Of these challenges two critical issues stand out: miniaturization and qubit quality.</p><p>IBM has adopted the superconducting qubit road map of <a href="https://spectrum.ieee.org/ibms-envisons-the-road-to-quantum-computing-like-an-apollo-mission" target="_self">reaching a 1,121-qubit processor by 2023</a>, leading to the expectation that 1,000 qubits with today’s qubit form factor is feasible. However, current approaches will require very large chips (50 millimeters on a side, or larger) at the scale of small wafers, or the use of chiplets on multichip modules. While this approach will work, the aim is to attain a better path toward scalability.</p><p>Now researchers at <a href="https://www.nature.com/articles/s41563-021-01187-w" rel="noopener noreferrer" target="_blank">MIT have been able to both reduce the size of the qubits</a> and done so in a way that reduces the interference that occurs between neighboring qubits. The MIT researchers have increased the number of superconducting qubits that can be added onto a device by a factor of 100.</p><p>“We are addressing both qubit miniaturization and quality,” said <a href="https://equs.mit.edu/william-d-oliver/" rel="noopener noreferrer" target="_blank">William Oliver</a>, the director for the <a href="https://cqe.mit.edu/" target="_blank">Center for Quantum Engineering</a> at MIT. “Unlike conventional transistor scaling, where only the number really matters, for qubits, large numbers are not sufficient, they must also be high-performance. Sacrificing performance for qubit number is not a useful trade in quantum computing. They must go hand in hand.”</p><p>The key to this big increase in qubit density and reduction of interference comes down to the use of two-dimensional materials, in particular the 2D insulator hexagonal boron nitride (hBN). The MIT researchers demonstrated that a few atomic monolayers of hBN can be stacked to form the insulator in the capacitors of a superconducting qubit.</p><p>Just like other capacitors, the capacitors in these superconducting circuits take the form of a sandwich in which an insulator material is sandwiched between two metal plates. The big difference for these capacitors is that the superconducting circuits can operate only at extremely low temperatures—less than 0.02 degrees above absolute zero (-273.15 °C).</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="Golden dilution refrigerator hanging vertically" class="rm-shortcode rm-resized-image" data-rm-shortcode-id="694399af8a1c345e51a695ff73909eda" data-rm-shortcode-name="rebelmouse-image" id="6c615" loading="lazy" src="https://spectrum.ieee.org/media-library/golden-dilution-refrigerator-hanging-vertically.jpg?id=29281593&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..." style="max-width: 100%;">Superconducting qubits are measured at temperatures as low as 20 millikelvin in a dilution refrigerator.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..." style="max-width: 100%;">Nathan Fiske/MIT</small></p><p>In that environment, insulating materials that are available for the job, such as PE-CVD silicon oxide or silicon nitride, have quite a few defects that are too lossy for quantum computing applications. To get around these material shortcomings, most superconducting circuits use what are called coplanar capacitors. In these capacitors, the plates are positioned laterally to one another, rather than on top of one another.</p><p>As a result, the intrinsic silicon substrate below the plates and to a smaller degree the vacuum above the plates serve as the capacitor dielectric. Intrinsic silicon is chemically pure and therefore has few defects, and the large size dilutes the electric field at the plate interfaces, all of which leads to a low-loss capacitor. The lateral size of each plate in this open-face design ends up being quite large (typically 100 by 100 micrometers) in order to achieve the required capacitance.</p><p>In an effort to move away from the large lateral configuration, the MIT researchers embarked on a search for an insulator that has very few defects and is compatible with superconducting capacitor plates.</p><p>“We chose to study hBN because it is the most widely used insulator in 2D material research due to its cleanliness and chemical inertness,” said colead author <a href="https://equs.mit.edu/joel-wang/" rel="noopener noreferrer" target="_blank">Joel Wang</a>, a research scientist in the Engineering Quantum Systems group of the MIT Research Laboratory for Electronics. </p><p>On either side of the hBN, the MIT researchers used the 2D superconducting material, niobium diselenide. One of the trickiest aspects of fabricating the capacitors was working with the niobium diselenide, which oxidizes in seconds when exposed to air, according to Wang. This necessitates that the assembly of the capacitor occur in a glove box filled with argon gas.</p><p>While this would seemingly complicate the scaling up of the production of these capacitors, Wang doesn’t regard this as a limiting factor.</p><p>“What determines the quality factor of the capacitor are the two interfaces between the two materials,” said Wang. “Once the sandwich is made, the two interfaces are “sealed” and we don’t see any noticeable degradation over time when exposed to the atmosphere.”</p><p>This lack of degradation is because around 90 percent of the electric field is contained within the sandwich structure, so the oxidation of the outer surface of the niobium diselenide does not play a significant role anymore. This ultimately makes the capacitor footprint much smaller, and it accounts for the reduction in cross talk between the neighboring qubits.</p><p>“The main challenge for scaling up the fabrication will be the wafer-scale growth of hBN and 2D superconductors like [niobium diselenide], and how one can do wafer-scale stacking of these films,” added Wang.</p><p>Wang believes that this research has shown 2D hBN to be a good insulator candidate for superconducting qubits. He says that the groundwork the MIT team has done will serve as a road map for using other hybrid 2D materials to build superconducting circuits.</p>]]></description><pubDate>Mon, 07 Feb 2022 16:12:05 +0000</pubDate><guid>https://spectrum.ieee.org/2d-hbn-qubit</guid><category>Quantum computing</category><category>2d materials</category><category>Ibm</category><category>Qubits</category><category>Hexagonal boron nitride</category><category>Superconducting qubits</category><category>Mit</category><dc:creator>Dexter Johnson</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-golden-square-package-holds-a-small-processor-sitting-on-top-is-a-metal-square-with-mit-etched-into-it.jpg?id=29281587&amp;width=980"></media:content></item></channel></rss>
